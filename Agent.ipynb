{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "hGY8IrgUDTFV"
      },
      "outputs": [],
      "source": [
        "from collections import deque\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "import torch"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# !pip install mido\n",
        "\n",
        "!pip install pytorch_lightning"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nRQJ__Jog8dH",
        "outputId": "0d9eedb1-37b1-4481-da13-536ed5950ae7"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pytorch_lightning\n",
            "  Downloading pytorch_lightning-2.5.2-py3-none-any.whl.metadata (21 kB)\n",
            "Requirement already satisfied: torch>=2.1.0 in /usr/local/lib/python3.11/dist-packages (from pytorch_lightning) (2.6.0+cu124)\n",
            "Requirement already satisfied: tqdm>=4.57.0 in /usr/local/lib/python3.11/dist-packages (from pytorch_lightning) (4.67.1)\n",
            "Requirement already satisfied: PyYAML>=5.4 in /usr/local/lib/python3.11/dist-packages (from pytorch_lightning) (6.0.2)\n",
            "Requirement already satisfied: fsspec>=2022.5.0 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]>=2022.5.0->pytorch_lightning) (2025.3.2)\n",
            "Collecting torchmetrics>=0.7.0 (from pytorch_lightning)\n",
            "  Downloading torchmetrics-1.7.3-py3-none-any.whl.metadata (21 kB)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from pytorch_lightning) (24.2)\n",
            "Requirement already satisfied: typing-extensions>=4.4.0 in /usr/local/lib/python3.11/dist-packages (from pytorch_lightning) (4.14.0)\n",
            "Collecting lightning-utilities>=0.10.0 (from pytorch_lightning)\n",
            "  Downloading lightning_utilities-0.14.3-py3-none-any.whl.metadata (5.6 kB)\n",
            "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]>=2022.5.0->pytorch_lightning) (3.11.15)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from lightning-utilities>=0.10.0->pytorch_lightning) (75.2.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch>=2.1.0->pytorch_lightning) (3.18.0)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=2.1.0->pytorch_lightning) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=2.1.0->pytorch_lightning) (3.1.6)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch>=2.1.0->pytorch_lightning)\n",
            "  Downloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.4.127 (from torch>=2.1.0->pytorch_lightning)\n",
            "  Downloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.4.127 (from torch>=2.1.0->pytorch_lightning)\n",
            "  Downloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cudnn-cu12==9.1.0.70 (from torch>=2.1.0->pytorch_lightning)\n",
            "  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cublas-cu12==12.4.5.8 (from torch>=2.1.0->pytorch_lightning)\n",
            "  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cufft-cu12==11.2.1.3 (from torch>=2.1.0->pytorch_lightning)\n",
            "  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-curand-cu12==10.3.5.147 (from torch>=2.1.0->pytorch_lightning)\n",
            "  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cusolver-cu12==11.6.1.9 (from torch>=2.1.0->pytorch_lightning)\n",
            "  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cusparse-cu12==12.3.1.170 (from torch>=2.1.0->pytorch_lightning)\n",
            "  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch>=2.1.0->pytorch_lightning) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>=2.1.0->pytorch_lightning) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.1.0->pytorch_lightning) (12.4.127)\n",
            "Collecting nvidia-nvjitlink-cu12==12.4.127 (from torch>=2.1.0->pytorch_lightning)\n",
            "  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch>=2.1.0->pytorch_lightning) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch>=2.1.0->pytorch_lightning) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch>=2.1.0->pytorch_lightning) (1.3.0)\n",
            "Requirement already satisfied: numpy>1.20.0 in /usr/local/lib/python3.11/dist-packages (from torchmetrics>=0.7.0->pytorch_lightning) (2.0.2)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch_lightning) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch_lightning) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch_lightning) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch_lightning) (1.7.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch_lightning) (6.6.3)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch_lightning) (0.3.2)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch_lightning) (1.20.1)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=2.1.0->pytorch_lightning) (3.0.2)\n",
            "Requirement already satisfied: idna>=2.0 in /usr/local/lib/python3.11/dist-packages (from yarl<2.0,>=1.17.0->aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch_lightning) (3.10)\n",
            "Downloading pytorch_lightning-2.5.2-py3-none-any.whl (825 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m825.4/825.4 kB\u001b[0m \u001b[31m22.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading lightning_utilities-0.14.3-py3-none-any.whl (28 kB)\n",
            "Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m1.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (13.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m115.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (24.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m90.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (883 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m59.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m5.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m13.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m7.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m5.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m37.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading torchmetrics-1.7.3-py3-none-any.whl (962 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m962.6/962.6 kB\u001b[0m \u001b[31m55.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: nvidia-nvjitlink-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, lightning-utilities, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12, torchmetrics, pytorch_lightning\n",
            "  Attempting uninstall: nvidia-nvjitlink-cu12\n",
            "    Found existing installation: nvidia-nvjitlink-cu12 12.5.82\n",
            "    Uninstalling nvidia-nvjitlink-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-nvjitlink-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-curand-cu12\n",
            "    Found existing installation: nvidia-curand-cu12 10.3.6.82\n",
            "    Uninstalling nvidia-curand-cu12-10.3.6.82:\n",
            "      Successfully uninstalled nvidia-curand-cu12-10.3.6.82\n",
            "  Attempting uninstall: nvidia-cufft-cu12\n",
            "    Found existing installation: nvidia-cufft-cu12 11.2.3.61\n",
            "    Uninstalling nvidia-cufft-cu12-11.2.3.61:\n",
            "      Successfully uninstalled nvidia-cufft-cu12-11.2.3.61\n",
            "  Attempting uninstall: nvidia-cuda-runtime-cu12\n",
            "    Found existing installation: nvidia-cuda-runtime-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-runtime-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-runtime-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n",
            "    Found existing installation: nvidia-cuda-nvrtc-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-nvrtc-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-cupti-cu12\n",
            "    Found existing installation: nvidia-cuda-cupti-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-cupti-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-cupti-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cublas-cu12\n",
            "    Found existing installation: nvidia-cublas-cu12 12.5.3.2\n",
            "    Uninstalling nvidia-cublas-cu12-12.5.3.2:\n",
            "      Successfully uninstalled nvidia-cublas-cu12-12.5.3.2\n",
            "  Attempting uninstall: nvidia-cusparse-cu12\n",
            "    Found existing installation: nvidia-cusparse-cu12 12.5.1.3\n",
            "    Uninstalling nvidia-cusparse-cu12-12.5.1.3:\n",
            "      Successfully uninstalled nvidia-cusparse-cu12-12.5.1.3\n",
            "  Attempting uninstall: nvidia-cudnn-cu12\n",
            "    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\n",
            "    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\n",
            "      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\n",
            "  Attempting uninstall: nvidia-cusolver-cu12\n",
            "    Found existing installation: nvidia-cusolver-cu12 11.6.3.83\n",
            "    Uninstalling nvidia-cusolver-cu12-11.6.3.83:\n",
            "      Successfully uninstalled nvidia-cusolver-cu12-11.6.3.83\n",
            "Successfully installed lightning-utilities-0.14.3 nvidia-cublas-cu12-12.4.5.8 nvidia-cuda-cupti-cu12-12.4.127 nvidia-cuda-nvrtc-cu12-12.4.127 nvidia-cuda-runtime-cu12-12.4.127 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-nvjitlink-cu12-12.4.127 pytorch_lightning-2.5.2 torchmetrics-1.7.3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test_midis = ['monteverdi_libri_dei_madrigali_1_5_(c)icking-archive.mid',\n",
        "              'monteverdi_libri_dei_madrigali_1_10_(c)icking-archive.mid',\n",
        "              'monteverdi_libri_dei_madrigali_4_12_(c)icking-archive.mid',\n",
        "              'monteverdi_libri_dei_madrigali_4_13_(c)icking-archive.mid']"
      ],
      "metadata": {
        "id": "IGZzKlXyhFfL"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from mido import MidiFile\n",
        "\n",
        "input_folder = 'path/to/your/midi/folder'\n",
        "output_folder = 'path/to/your/output/folder'\n",
        "\n",
        "# Make sure the output folder exists\n",
        "os.makedirs(output_folder, exist_ok=True)\n",
        "\n",
        "def Midi_File_Input(mids):\n",
        "    token = []\n",
        "    for i, track in enumerate(mids.tracks):\n",
        "        for msg in track:\n",
        "            if msg.type == 'note_on':\n",
        "                token.append(f\"<{msg.type}_{msg.channel}_{msg.note}_{msg.velocity}>\")\n",
        "            elif msg.type == 'note_off':\n",
        "                token.append(f\"<{msg.type}_{msg.channel}_{msg.note}_{msg.velocity}>\")\n",
        "            # elif msg.type == 'track_name':\n",
        "            #     token.append(f\"<{msg.type}_{msg.name}>\")\n",
        "            elif msg.type == 'control_change':\n",
        "                token.append(f\"<{msg.type}_{msg.channel}_{msg.control}_{msg.value}>\")\n",
        "            elif msg.type == 'program_change':\n",
        "                token.append(f\"<{msg.type}_{msg.program}_>\")\n",
        "            elif msg.type == 'key_signature':\n",
        "                token.append(f\"<{msg.type}_{msg.key}>\")\n",
        "    return token\n",
        "\n",
        "token_data = {}\n",
        "all_tokens = []  # Flat list for training\n",
        "\n",
        "\n",
        "# Process each MIDI file\n",
        "for filename in test_midis:\n",
        "    midi_path = '/content/drive/MyDrive/Final Project Folder/Midi Files/' + filename\n",
        "\n",
        "    midi = MidiFile(midi_path)\n",
        "\n",
        "    tokens = Midi_File_Input(midi)\n",
        "\n",
        "    # Add song_start and song_end markers\n",
        "    song_tokens = ['<song_start>'] + tokens + ['<song_end>']\n",
        "\n",
        "    # Store in dictionary (per-song)\n",
        "    token_data[filename] = song_tokens\n",
        "\n",
        "    # Append to flat list (for generative model training)\n",
        "    all_tokens.extend(song_tokens)\n"
      ],
      "metadata": {
        "id": "LQO2Rqa-edQc"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from collections import Counter\n",
        "\n",
        "# Flatten all tokens across all songs\n",
        "all_tokens_flat = [token for tokens in token_data.values() for token in tokens]\n",
        "\n",
        "# Count and sort tokens (optional for ordering)\n",
        "token_freq = Counter(all_tokens_flat)\n",
        "\n",
        "# Assign token ID\n",
        "vocab = {token: idx for idx, token in enumerate(sorted(token_freq))}\n",
        "\n",
        "# Optionally store reverse map too:\n",
        "inv_vocab = {idx: token for token, idx in vocab.items()}\n",
        "\n",
        "token_ids_data = {}\n",
        "\n",
        "for filename, tokens in token_data.items():\n",
        "    token_ids = [vocab[token] for token in tokens]\n",
        "    token_ids_data[filename] = token_ids\n",
        "\n",
        "vocab[\"<pad>\"] = 279"
      ],
      "metadata": {
        "id": "rVOUC75henYk"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vocab"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W71FfMJxhI9A",
        "outputId": "1456afa8-4bb4-430a-c999-2e0a04b60f9f"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'<control_change_0_100_0>': 0,\n",
              " '<control_change_0_101_0>': 1,\n",
              " '<control_change_0_10_104>': 2,\n",
              " '<control_change_0_10_24>': 3,\n",
              " '<control_change_0_10_44>': 4,\n",
              " '<control_change_0_10_64>': 5,\n",
              " '<control_change_0_10_84>': 6,\n",
              " '<control_change_0_12_3>': 7,\n",
              " '<control_change_0_38_0>': 8,\n",
              " '<control_change_0_6_12>': 9,\n",
              " '<control_change_0_7_100>': 10,\n",
              " '<control_change_0_7_101>': 11,\n",
              " '<control_change_0_7_102>': 12,\n",
              " '<control_change_0_7_103>': 13,\n",
              " '<control_change_0_7_104>': 14,\n",
              " '<control_change_0_7_105>': 15,\n",
              " '<control_change_0_7_106>': 16,\n",
              " '<control_change_0_7_107>': 17,\n",
              " '<control_change_0_7_108>': 18,\n",
              " '<control_change_0_7_109>': 19,\n",
              " '<control_change_0_7_110>': 20,\n",
              " '<control_change_0_7_111>': 21,\n",
              " '<control_change_0_7_112>': 22,\n",
              " '<control_change_0_7_113>': 23,\n",
              " '<control_change_0_7_114>': 24,\n",
              " '<control_change_0_7_115>': 25,\n",
              " '<control_change_0_7_116>': 26,\n",
              " '<control_change_0_7_117>': 27,\n",
              " '<control_change_0_7_118>': 28,\n",
              " '<control_change_0_7_83>': 29,\n",
              " '<control_change_0_7_84>': 30,\n",
              " '<control_change_0_7_85>': 31,\n",
              " '<control_change_0_7_86>': 32,\n",
              " '<control_change_0_7_87>': 33,\n",
              " '<control_change_0_7_88>': 34,\n",
              " '<control_change_0_7_89>': 35,\n",
              " '<control_change_0_7_90>': 36,\n",
              " '<control_change_0_7_91>': 37,\n",
              " '<control_change_0_7_92>': 38,\n",
              " '<control_change_0_7_93>': 39,\n",
              " '<control_change_0_7_94>': 40,\n",
              " '<control_change_0_7_95>': 41,\n",
              " '<control_change_0_7_96>': 42,\n",
              " '<control_change_0_7_97>': 43,\n",
              " '<control_change_0_7_98>': 44,\n",
              " '<control_change_0_7_99>': 45,\n",
              " '<control_change_0_91_76>': 46,\n",
              " '<key_signature_Am>': 47,\n",
              " '<key_signature_Dm>': 48,\n",
              " '<note_off_0_43_0>': 49,\n",
              " '<note_off_0_45_0>': 50,\n",
              " '<note_off_0_46_0>': 51,\n",
              " '<note_off_0_47_0>': 52,\n",
              " '<note_off_0_48_0>': 53,\n",
              " '<note_off_0_49_0>': 54,\n",
              " '<note_off_0_50_0>': 55,\n",
              " '<note_off_0_51_0>': 56,\n",
              " '<note_off_0_52_0>': 57,\n",
              " '<note_off_0_53_0>': 58,\n",
              " '<note_off_0_54_0>': 59,\n",
              " '<note_off_0_55_0>': 60,\n",
              " '<note_off_0_56_0>': 61,\n",
              " '<note_off_0_57_0>': 62,\n",
              " '<note_off_0_58_0>': 63,\n",
              " '<note_off_0_59_0>': 64,\n",
              " '<note_off_0_60_0>': 65,\n",
              " '<note_off_0_61_0>': 66,\n",
              " '<note_off_0_62_0>': 67,\n",
              " '<note_off_0_63_0>': 68,\n",
              " '<note_off_0_64_0>': 69,\n",
              " '<note_off_0_65_0>': 70,\n",
              " '<note_off_0_66_0>': 71,\n",
              " '<note_off_0_67_0>': 72,\n",
              " '<note_off_0_68_0>': 73,\n",
              " '<note_off_0_69_0>': 74,\n",
              " '<note_off_0_70_0>': 75,\n",
              " '<note_off_0_71_0>': 76,\n",
              " '<note_off_0_72_0>': 77,\n",
              " '<note_off_0_73_0>': 78,\n",
              " '<note_off_0_74_0>': 79,\n",
              " '<note_off_0_75_0>': 80,\n",
              " '<note_off_0_76_0>': 81,\n",
              " '<note_off_0_77_0>': 82,\n",
              " '<note_off_0_78_0>': 83,\n",
              " '<note_off_0_79_0>': 84,\n",
              " '<note_off_0_81_0>': 85,\n",
              " '<note_on_0_43_125>': 86,\n",
              " '<note_on_0_45_113>': 87,\n",
              " '<note_on_0_45_120>': 88,\n",
              " '<note_on_0_45_125>': 89,\n",
              " '<note_on_0_46_127>': 90,\n",
              " '<note_on_0_47_125>': 91,\n",
              " '<note_on_0_47_127>': 92,\n",
              " '<note_on_0_48_124>': 93,\n",
              " '<note_on_0_48_125>': 94,\n",
              " '<note_on_0_48_126>': 95,\n",
              " '<note_on_0_48_127>': 96,\n",
              " '<note_on_0_49_125>': 97,\n",
              " '<note_on_0_50_120>': 98,\n",
              " '<note_on_0_50_124>': 99,\n",
              " '<note_on_0_50_125>': 100,\n",
              " '<note_on_0_50_126>': 101,\n",
              " '<note_on_0_50_127>': 102,\n",
              " '<note_on_0_51_126>': 103,\n",
              " '<note_on_0_51_127>': 104,\n",
              " '<note_on_0_52_119>': 105,\n",
              " '<note_on_0_52_120>': 106,\n",
              " '<note_on_0_52_124>': 107,\n",
              " '<note_on_0_52_125>': 108,\n",
              " '<note_on_0_52_126>': 109,\n",
              " '<note_on_0_52_127>': 110,\n",
              " '<note_on_0_53_114>': 111,\n",
              " '<note_on_0_53_124>': 112,\n",
              " '<note_on_0_53_125>': 113,\n",
              " '<note_on_0_53_126>': 114,\n",
              " '<note_on_0_53_127>': 115,\n",
              " '<note_on_0_54_120>': 116,\n",
              " '<note_on_0_54_124>': 117,\n",
              " '<note_on_0_54_125>': 118,\n",
              " '<note_on_0_54_127>': 119,\n",
              " '<note_on_0_55_113>': 120,\n",
              " '<note_on_0_55_119>': 121,\n",
              " '<note_on_0_55_120>': 122,\n",
              " '<note_on_0_55_124>': 123,\n",
              " '<note_on_0_55_125>': 124,\n",
              " '<note_on_0_55_126>': 125,\n",
              " '<note_on_0_55_127>': 126,\n",
              " '<note_on_0_56_125>': 127,\n",
              " '<note_on_0_57_113>': 128,\n",
              " '<note_on_0_57_116>': 129,\n",
              " '<note_on_0_57_118>': 130,\n",
              " '<note_on_0_57_120>': 131,\n",
              " '<note_on_0_57_124>': 132,\n",
              " '<note_on_0_57_125>': 133,\n",
              " '<note_on_0_57_126>': 134,\n",
              " '<note_on_0_57_127>': 135,\n",
              " '<note_on_0_58_113>': 136,\n",
              " '<note_on_0_58_114>': 137,\n",
              " '<note_on_0_58_125>': 138,\n",
              " '<note_on_0_58_126>': 139,\n",
              " '<note_on_0_58_127>': 140,\n",
              " '<note_on_0_59_113>': 141,\n",
              " '<note_on_0_59_116>': 142,\n",
              " '<note_on_0_59_120>': 143,\n",
              " '<note_on_0_59_124>': 144,\n",
              " '<note_on_0_59_125>': 145,\n",
              " '<note_on_0_59_126>': 146,\n",
              " '<note_on_0_59_127>': 147,\n",
              " '<note_on_0_60_113>': 148,\n",
              " '<note_on_0_60_114>': 149,\n",
              " '<note_on_0_60_116>': 150,\n",
              " '<note_on_0_60_118>': 151,\n",
              " '<note_on_0_60_120>': 152,\n",
              " '<note_on_0_60_124>': 153,\n",
              " '<note_on_0_60_125>': 154,\n",
              " '<note_on_0_60_126>': 155,\n",
              " '<note_on_0_60_127>': 156,\n",
              " '<note_on_0_61_120>': 157,\n",
              " '<note_on_0_61_121>': 158,\n",
              " '<note_on_0_61_125>': 159,\n",
              " '<note_on_0_61_126>': 160,\n",
              " '<note_on_0_61_127>': 161,\n",
              " '<note_on_0_62_113>': 162,\n",
              " '<note_on_0_62_116>': 163,\n",
              " '<note_on_0_62_120>': 164,\n",
              " '<note_on_0_62_125>': 165,\n",
              " '<note_on_0_62_126>': 166,\n",
              " '<note_on_0_62_127>': 167,\n",
              " '<note_on_0_63_120>': 168,\n",
              " '<note_on_0_63_125>': 169,\n",
              " '<note_on_0_63_126>': 170,\n",
              " '<note_on_0_63_127>': 171,\n",
              " '<note_on_0_64_113>': 172,\n",
              " '<note_on_0_64_116>': 173,\n",
              " '<note_on_0_64_118>': 174,\n",
              " '<note_on_0_64_120>': 175,\n",
              " '<note_on_0_64_121>': 176,\n",
              " '<note_on_0_64_124>': 177,\n",
              " '<note_on_0_64_125>': 178,\n",
              " '<note_on_0_64_126>': 179,\n",
              " '<note_on_0_64_127>': 180,\n",
              " '<note_on_0_65_113>': 181,\n",
              " '<note_on_0_65_114>': 182,\n",
              " '<note_on_0_65_116>': 183,\n",
              " '<note_on_0_65_118>': 184,\n",
              " '<note_on_0_65_120>': 185,\n",
              " '<note_on_0_65_125>': 186,\n",
              " '<note_on_0_65_126>': 187,\n",
              " '<note_on_0_65_127>': 188,\n",
              " '<note_on_0_66_120>': 189,\n",
              " '<note_on_0_66_121>': 190,\n",
              " '<note_on_0_66_124>': 191,\n",
              " '<note_on_0_66_125>': 192,\n",
              " '<note_on_0_66_126>': 193,\n",
              " '<note_on_0_66_127>': 194,\n",
              " '<note_on_0_67_113>': 195,\n",
              " '<note_on_0_67_114>': 196,\n",
              " '<note_on_0_67_116>': 197,\n",
              " '<note_on_0_67_119>': 198,\n",
              " '<note_on_0_67_120>': 199,\n",
              " '<note_on_0_67_124>': 200,\n",
              " '<note_on_0_67_125>': 201,\n",
              " '<note_on_0_67_126>': 202,\n",
              " '<note_on_0_67_127>': 203,\n",
              " '<note_on_0_68_120>': 204,\n",
              " '<note_on_0_68_125>': 205,\n",
              " '<note_on_0_68_126>': 206,\n",
              " '<note_on_0_68_127>': 207,\n",
              " '<note_on_0_69_113>': 208,\n",
              " '<note_on_0_69_116>': 209,\n",
              " '<note_on_0_69_118>': 210,\n",
              " '<note_on_0_69_119>': 211,\n",
              " '<note_on_0_69_120>': 212,\n",
              " '<note_on_0_69_124>': 213,\n",
              " '<note_on_0_69_125>': 214,\n",
              " '<note_on_0_69_126>': 215,\n",
              " '<note_on_0_69_127>': 216,\n",
              " '<note_on_0_70_113>': 217,\n",
              " '<note_on_0_70_114>': 218,\n",
              " '<note_on_0_70_125>': 219,\n",
              " '<note_on_0_70_126>': 220,\n",
              " '<note_on_0_70_127>': 221,\n",
              " '<note_on_0_71_116>': 222,\n",
              " '<note_on_0_71_120>': 223,\n",
              " '<note_on_0_71_124>': 224,\n",
              " '<note_on_0_71_125>': 225,\n",
              " '<note_on_0_71_126>': 226,\n",
              " '<note_on_0_71_127>': 227,\n",
              " '<note_on_0_72_113>': 228,\n",
              " '<note_on_0_72_114>': 229,\n",
              " '<note_on_0_72_116>': 230,\n",
              " '<note_on_0_72_119>': 231,\n",
              " '<note_on_0_72_120>': 232,\n",
              " '<note_on_0_72_124>': 233,\n",
              " '<note_on_0_72_125>': 234,\n",
              " '<note_on_0_72_126>': 235,\n",
              " '<note_on_0_72_127>': 236,\n",
              " '<note_on_0_73_125>': 237,\n",
              " '<note_on_0_73_126>': 238,\n",
              " '<note_on_0_73_127>': 239,\n",
              " '<note_on_0_74_113>': 240,\n",
              " '<note_on_0_74_114>': 241,\n",
              " '<note_on_0_74_116>': 242,\n",
              " '<note_on_0_74_118>': 243,\n",
              " '<note_on_0_74_120>': 244,\n",
              " '<note_on_0_74_124>': 245,\n",
              " '<note_on_0_74_125>': 246,\n",
              " '<note_on_0_74_126>': 247,\n",
              " '<note_on_0_74_127>': 248,\n",
              " '<note_on_0_75_120>': 249,\n",
              " '<note_on_0_75_121>': 250,\n",
              " '<note_on_0_75_125>': 251,\n",
              " '<note_on_0_75_126>': 252,\n",
              " '<note_on_0_75_127>': 253,\n",
              " '<note_on_0_76_113>': 254,\n",
              " '<note_on_0_76_114>': 255,\n",
              " '<note_on_0_76_120>': 256,\n",
              " '<note_on_0_76_121>': 257,\n",
              " '<note_on_0_76_124>': 258,\n",
              " '<note_on_0_76_125>': 259,\n",
              " '<note_on_0_76_126>': 260,\n",
              " '<note_on_0_76_127>': 261,\n",
              " '<note_on_0_77_113>': 262,\n",
              " '<note_on_0_77_114>': 263,\n",
              " '<note_on_0_77_125>': 264,\n",
              " '<note_on_0_77_126>': 265,\n",
              " '<note_on_0_77_127>': 266,\n",
              " '<note_on_0_78_120>': 267,\n",
              " '<note_on_0_78_125>': 268,\n",
              " '<note_on_0_78_127>': 269,\n",
              " '<note_on_0_79_120>': 270,\n",
              " '<note_on_0_79_125>': 271,\n",
              " '<note_on_0_79_126>': 272,\n",
              " '<note_on_0_79_127>': 273,\n",
              " '<note_on_0_81_125>': 274,\n",
              " '<program_change_52_>': 275,\n",
              " '<song_end>': 276,\n",
              " '<song_start>': 277,\n",
              " '<pad>': 279}"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "from torch.optim import Adam\n",
        "from torch.utils.data import TensorDataset, DataLoader\n",
        "\n",
        "#from pytorch_lightning import\n",
        "from pytorch_lightning import Trainer, LightningModule\n",
        "\n",
        "\n",
        "class DecodeOTrans(LightningModule):\n",
        "  def __init__(self, d_model, max_length, num_tokens = 100):\n",
        "\n",
        "    #Here we specify the number of tokens available in the vocabulary,\n",
        "    #Number of word embeddings per token\n",
        "    #Max token length\n",
        "    super().__init__()\n",
        "\n",
        "    self.we = nn.Embedding(num_embeddings = num_tokens, embedding_dim = d_model) #We creating a word embedding value\n",
        "    #Embeddings needs to know how many tokens are in the vocabulary and understand the dimension size to represent the embedding\n",
        "\n",
        "    self.pe = PositionalEncoding(d_model = d_model, max_length = max_length)\n",
        "\n",
        "    #Then we create a positional encoding object using the class we created earlier\n",
        "\n",
        "    self.self_attention = Attention(d_model = d_model)\n",
        "\n",
        "    self.fc_layer = nn.Linear(in_features = d_model, out_features = num_tokens) #This is our fully connected layer also know as our Dense Layer of neurons (RNN neural networks)\n",
        "\n",
        "    self.loss = nn.CrossEntropyLoss()\n",
        "\n",
        "  def encode_state(self, token_ids):\n",
        "\n",
        "    word_embeddings = self.we(token_ids)  # [batch_size, seq_len, d_model]\n",
        "    positional_encoding = self.pe(word_embeddings)  # add positional encoding\n",
        "\n",
        "    seq_len = token_ids.size(1)\n",
        "    #mask = torch.tril(torch.ones((seq_len, seq_len), device=token_ids.device)) == 0\n",
        "\n",
        "    mask = torch.tril(torch.ones((seq_len, seq_len), device=token_ids.device)).unsqueeze(0) == 0\n",
        "\n",
        "    self_attention_values = self.self_attention(\n",
        "        positional_encoding,\n",
        "        positional_encoding,\n",
        "        positional_encoding,\n",
        "        mask=mask\n",
        "    )\n",
        "\n",
        "    contextual_embeddings = positional_encoding + self_attention_values\n",
        "\n",
        "    # Option A: use last token's hidden state\n",
        "    state_vector = contextual_embeddings[:, -1, :]\n",
        "\n",
        "    # OR Option B: use mean pooling\n",
        "    # state_vector = contextual_embeddings.mean(dim=1)\n",
        "\n",
        "    return state_vector  # [batch_size, d_model]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 464
        },
        "id": "w2lmwLKuKUxV",
        "outputId": "b8205380-09eb-4819-a66d-d59a92aedb03"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ModuleNotFoundError",
          "evalue": "No module named 'pytorch_lightning'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-15-4048832150.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;31m#from pytorch_lightning import\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mpytorch_lightning\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mTrainer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mLightningModule\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'pytorch_lightning'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ],
          "errorDetails": {
            "actions": [
              {
                "action": "open_url",
                "actionText": "Open Examples",
                "url": "/notebooks/snippets/importing_libraries.ipynb"
              }
            ]
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Agent RL + Trans (For Music GPT Adapted)"
      ],
      "metadata": {
        "id": "4rt2jBfFqUMU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class MusicDQNAgent:\n",
        "    def __init__(self, d_model, max_length, num_tokens, num_actions):\n",
        "\n",
        "      self.encoder = TransformerStateEncoder(d_model, max_length, num_tokens)\n",
        "      #This is our transformer based encoder we created which takes the tokens and outputs it as dense vectors with positional + attentional mechanism converted into a simple numerical format\n",
        "      self.q_net = QNet(d_model, 128, num_actions)\n",
        "      # This is the Q neural network this predicts possible actions from the inputted state.\n",
        "\n",
        "      self.gamma = 0.99\n",
        "\n",
        "      #Hyperparameter tells the model to reward future or immediate rewards.\n",
        "      self.epsilon = 0.1\n",
        "\n",
        "      #This is the exploration rate and tells the model to explore so choose randomly to further explore the entire environment and not get stuck at local minima.\n",
        "      #self.memory = deque(maxlen=10000)\n",
        "\n",
        "      #This stores tuples of (state, action, reward, next_state, done).\n",
        "      self.optimizer = torch.optim.Adam(list(self.encoder.parameters()) + list(self.q_net.parameters()), lr=1e-4)\n",
        "\n",
        "      #This is our optimiser Adam here we're balancing the predicted and Q values.\n",
        "\n",
        "      #Parameters\n",
        "\n",
        "        #d_model: The dimensionality of each token vector the larger the dimensionality the more detail which is included in the tokens descriptions\n",
        "        #max_length: maximum sequence length expected by the transformer\n",
        "        #num_tokens: Total number of unique tokens in my vocabulary\n",
        "        #num_actions: The number of possible actions the agent can choose from\n",
        "    def get_state(self, token_sequence):\n",
        "        # token_sequence: [1, seq_len]\n",
        "      with torch.no_grad():\n",
        "          #This tells our encoder to not train anything since we only want the embeddings\n",
        "          return self.encoder(token_sequence)  # [1, d_model]\n",
        "\n",
        "\n",
        "    #Here we are defining how we are going to grab the state we are in\n",
        "\n",
        "    def get_action(self, state_vector):\n",
        "      if random.random() < self.epsilon:\n",
        "        #This generates a float between 0 and 1 if the epison is larger then it will got with epsilon\n",
        "        #Therefore the larger the episoln the less exploration there is\n",
        "          return random.randint(0, self.q_net.linear2.out_features - 1)\n",
        "            #If we are exploring then return a random token where num_tokens -1 would be the next one.\n",
        "      q_values = self.q_net(state_vector)\n",
        "        #If we are not exploring then use the current state to predict the next state using the Q Net.\n",
        "      return torch.argmax(q_values).item()\n",
        "\n",
        "    #     #Here we choose the token with the highest indexed value\n",
        "    # def remember(self, state, action, reward, next_state, done):\n",
        "\n",
        "    #   self.memory.append((state, action, reward, next_state, done))\n",
        "\n",
        "    # def train_short_memory(self, state, action, reward, next_state, done):\n",
        "    #     self.train_step(state, action, reward, next_state, done)\n",
        "\n",
        "    # def train_long_memory(self, batch_size=64):\n",
        "    #     if len(self.memory) < batch_size:\n",
        "    #         return\n",
        "\n",
        "    #     mini_batch = random.sample(self.memory, batch_size)\n",
        "\n",
        "    #     for state, action, reward, next_state, done in mini_batch:\n",
        "    #         self.train_step(state, action, reward, next_state, done)\n",
        "\n",
        "    def compute_reward(predicted_token, target_token):\n",
        "    return 1.0 if predicted_token == target_token else -0.1\n",
        "\n",
        "    def train_step(self, state, action, reward, next_state, done):\n",
        "\n",
        "      self.q_net.train()\n",
        "\n",
        "        #This puts the Q neural network into training mode\n",
        "\n",
        "      q_values = self.q_net(state)\n",
        "\n",
        "        #So forward passes through the Q network\n",
        "      target = q_values.clone().detach()\n",
        "\n",
        "        #Create a copy of the Q-values tensor.\n",
        "\n",
        "        #Detach() removes it from the computation graph — this prevents gradients from flowing through the target.\n",
        "\n",
        "      with torch.no_grad():\n",
        "          next_q = self.q_net(next_state)\n",
        "          q_target = reward + self.gamma * torch.max(next_q) * (1 - int(done))\n",
        "\n",
        "        #We're computing the target Q-value using the Bellman equation:\n",
        "\n",
        "        target[0, action] = q_target\n",
        "\n",
        "        #We now replace only the Q-value for the action we actually took with the target value.\n",
        "\n",
        "        #This tells the network:\n",
        "\n",
        "        #\"You predicted X for action A, but the real value should have been Y — update your weights accordingly.\"\n",
        "\n",
        "        loss = F.mse_loss(q_values, target)\n",
        "        self.optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        self.optimizer.step()\n",
        "\n",
        "\n",
        "        #Mean Squared Error between:\n",
        "\n",
        "        #q_values: what the network predicted\n",
        "\n",
        "        #target: what it should have predicted (Bellman update)\n",
        "\n",
        "        #Then we run standard backprop and gradient descent.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "99u4RQh0qcvl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "agent = MusicDQNAgent(d_model=128, max_length=20, num_tokens=len(vocab), num_actions=len(vocab))\n",
        "num_episodes = 500\n",
        "\n",
        "for episode in range(num_episodes):\n",
        "    current_seq = [vocab[\"<start>\"]]\n",
        "    done = False\n",
        "\n",
        "    while not done and len(current_seq) < 20:\n",
        "        state_tensor = torch.tensor([current_seq], dtype=torch.long)\n",
        "        state_vector = agent.get_state(state_tensor)\n",
        "\n",
        "        action = agent.get_action(state_vector)\n",
        "\n",
        "        next_seq = current_seq + [action]\n",
        "        next_state_tensor = torch.tensor([next_seq], dtype=torch.long)\n",
        "        next_state_vector = agent.get_state(next_state_tensor)\n",
        "\n",
        "        # Dummy reward for now (customize later)\n",
        "        reward = 1.0 if action == target_sequence[len(current_seq)] else -0.1 if len(current_seq) < len(target_sequence) else -0.5\n",
        "\n",
        "        done = action == vocab[\"<end>\"] or len(next_seq) >= 20\n",
        "\n",
        "        #Directly train with this single experience\n",
        "        agent.train_step(state_vector, action, reward, next_state_vector, done)\n",
        "\n",
        "        current_seq = next_seq\n"
      ],
      "metadata": {
        "id": "3Kx0hrExduYQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "# Assume 0 corresponds to <start> in your vocabulary\n",
        "start_token_id = 0\n",
        "\n",
        "# Begin with just the start token\n",
        "generated_sequence = [start_token_id]\n",
        "\n",
        "max_length = 20  # Set how long you want the output to be\n",
        "\n",
        "for _ in range(max_length):\n",
        "    # Convert to tensor: [1, seq_len]\n",
        "    token_tensor = torch.tensor([generated_sequence], dtype=torch.long)\n",
        "\n",
        "    # Get encoded state from transformer encoder\n",
        "    state_vector = agent.get_state(token_tensor)\n",
        "\n",
        "    # Choose next token (action)\n",
        "    next_token = agent.get_action(state_vector)\n",
        "\n",
        "    # Optionally stop if end token is produced (e.g., <end> = 1)\n",
        "    if next_token == 1:  # Assuming 1 is <end>\n",
        "        break\n",
        "\n",
        "    # Add predicted token to sequence\n",
        "    generated_sequence.append(next_token)\n",
        "\n",
        "# Decode tokens back to symbols if needed\n",
        "print(\"Generated token sequence:\", generated_sequence)\n"
      ],
      "metadata": {
        "id": "0_nsyGxUqT2o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Agent (Snakes Game)"
      ],
      "metadata": {
        "id": "VDQsbHLKqRrv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# class Agent:\n",
        "#   def __init__(self):\n",
        "#     self.epsilon = 0 # Controls the randomness\n",
        "#     self.gamma = 0 # discount rate\n",
        "#     self.memort = deque(maxlen = MAX_MEMORY) #If we exceed this memory then it will automatically remove elements from the left\n",
        "\n",
        "#     # TODO: model, trainer\n",
        "#     pass\n",
        "#   def get_state():\n",
        "\n",
        "#     pass\n",
        "#   def remember(self, state, action, reward, next_state, done): #It's likelu we will need to change this {done} to be something which is linked to my project like the final token or something\n",
        "#     pass\n",
        "#   def train_long_memory(self):\n",
        "#     pass\n",
        "#   def train_short_memory(self, state, action, reward, next_state, done):\n",
        "#     pass\n",
        "#   def get_action(self, state):\n",
        "#     pass\n",
        "#   def train():\n",
        "#     plot_scores = []\n",
        "#     plot_mean_scores = []\n",
        "#     total_score = 0\n",
        "#     record = 0\n",
        "#     agent = Agent()\n",
        "#     while True:\n",
        "#       #Get old state\n",
        "#       state_old = agent.get_state(game) #This is our previous state which will be used in my case for progressive training\n",
        "\n",
        "#       #This will be the previous state\n",
        "#       #so this will likely have to be adapted where we have append in a list the previous state\n",
        "\n",
        "#       final_move = agent.get_action(state_old) #This is our action\n",
        "\n",
        "#       reward, done, score = game.play_step(final_move)\n",
        "#       state_new = agent.get_state(game)\n",
        "\n",
        "#       #train short memeory\n",
        "\n",
        "#       agent.train_short_memory(state_old, final_move, reward, state_new, done)\n",
        "\n",
        "#       #remember\n",
        "\n",
        "#       agent.remember(state_old, final_move, reward, state_new, done)\n",
        "\n",
        "#       if done:\n",
        "#         #train long memory\n",
        "#         game.reset()\n",
        "#         agent.n_games += 1\n",
        "#         agent.train_long_memory()\n",
        "\n",
        "#         if score > record:\n",
        "#           record = score\n",
        "#           agent.model.save()\n",
        "\n",
        "#         print('Game', agent.n_games, 'Score', score, 'Record:', record)\n",
        "\n",
        "#     #This is currently how it's done in the Snake game we would need to adapt this for our needs\n",
        "#     #I think in our adaptation this would be the some form of validation variable.\n",
        "#     pass"
      ],
      "metadata": {
        "id": "jrxxBMl1Dy_s"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}