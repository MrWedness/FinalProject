{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "hGY8IrgUDTFV"
      },
      "outputs": [],
      "source": [
        "from collections import deque\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "import torch\n",
        "\n",
        "import random\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "from torch.optim import Adam\n",
        "from torch.utils.data import TensorDataset, DataLoader\n",
        "\n",
        "#import lightning as L\n",
        "from pytorch_lightning import Trainer\n",
        "\n",
        "#import qnet"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install mido\n",
        "\n",
        "!pip install pytorch_lightning\n",
        "\n",
        "# !pip install qnet"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nRQJ__Jog8dH",
        "outputId": "c42e047d-3fb8-4620-f603-0c1afed3c74c"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting mido\n",
            "  Downloading mido-1.3.3-py3-none-any.whl.metadata (6.4 kB)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from mido) (24.2)\n",
            "Downloading mido-1.3.3-py3-none-any.whl (54 kB)\n",
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/54.6 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m54.6/54.6 kB\u001b[0m \u001b[31m3.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: mido\n",
            "Successfully installed mido-1.3.3\n",
            "Collecting pytorch_lightning\n",
            "  Downloading pytorch_lightning-2.5.2-py3-none-any.whl.metadata (21 kB)\n",
            "Requirement already satisfied: torch>=2.1.0 in /usr/local/lib/python3.11/dist-packages (from pytorch_lightning) (2.6.0+cu124)\n",
            "Requirement already satisfied: tqdm>=4.57.0 in /usr/local/lib/python3.11/dist-packages (from pytorch_lightning) (4.67.1)\n",
            "Requirement already satisfied: PyYAML>=5.4 in /usr/local/lib/python3.11/dist-packages (from pytorch_lightning) (6.0.2)\n",
            "Requirement already satisfied: fsspec>=2022.5.0 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]>=2022.5.0->pytorch_lightning) (2025.3.2)\n",
            "Collecting torchmetrics>=0.7.0 (from pytorch_lightning)\n",
            "  Downloading torchmetrics-1.7.4-py3-none-any.whl.metadata (21 kB)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from pytorch_lightning) (24.2)\n",
            "Requirement already satisfied: typing-extensions>=4.4.0 in /usr/local/lib/python3.11/dist-packages (from pytorch_lightning) (4.14.1)\n",
            "Collecting lightning-utilities>=0.10.0 (from pytorch_lightning)\n",
            "  Downloading lightning_utilities-0.14.3-py3-none-any.whl.metadata (5.6 kB)\n",
            "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]>=2022.5.0->pytorch_lightning) (3.11.15)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from lightning-utilities>=0.10.0->pytorch_lightning) (75.2.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch>=2.1.0->pytorch_lightning) (3.18.0)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=2.1.0->pytorch_lightning) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=2.1.0->pytorch_lightning) (3.1.6)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch>=2.1.0->pytorch_lightning)\n",
            "  Downloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.4.127 (from torch>=2.1.0->pytorch_lightning)\n",
            "  Downloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.4.127 (from torch>=2.1.0->pytorch_lightning)\n",
            "  Downloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cudnn-cu12==9.1.0.70 (from torch>=2.1.0->pytorch_lightning)\n",
            "  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cublas-cu12==12.4.5.8 (from torch>=2.1.0->pytorch_lightning)\n",
            "  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cufft-cu12==11.2.1.3 (from torch>=2.1.0->pytorch_lightning)\n",
            "  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-curand-cu12==10.3.5.147 (from torch>=2.1.0->pytorch_lightning)\n",
            "  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cusolver-cu12==11.6.1.9 (from torch>=2.1.0->pytorch_lightning)\n",
            "  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cusparse-cu12==12.3.1.170 (from torch>=2.1.0->pytorch_lightning)\n",
            "  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch>=2.1.0->pytorch_lightning) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>=2.1.0->pytorch_lightning) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.1.0->pytorch_lightning) (12.4.127)\n",
            "Collecting nvidia-nvjitlink-cu12==12.4.127 (from torch>=2.1.0->pytorch_lightning)\n",
            "  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch>=2.1.0->pytorch_lightning) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch>=2.1.0->pytorch_lightning) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch>=2.1.0->pytorch_lightning) (1.3.0)\n",
            "Requirement already satisfied: numpy>1.20.0 in /usr/local/lib/python3.11/dist-packages (from torchmetrics>=0.7.0->pytorch_lightning) (2.0.2)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch_lightning) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch_lightning) (1.4.0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch_lightning) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch_lightning) (1.7.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch_lightning) (6.6.3)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch_lightning) (0.3.2)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch_lightning) (1.20.1)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=2.1.0->pytorch_lightning) (3.0.2)\n",
            "Requirement already satisfied: idna>=2.0 in /usr/local/lib/python3.11/dist-packages (from yarl<2.0,>=1.17.0->aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch_lightning) (3.10)\n",
            "Downloading pytorch_lightning-2.5.2-py3-none-any.whl (825 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m825.4/825.4 kB\u001b[0m \u001b[31m21.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading lightning_utilities-0.14.3-py3-none-any.whl (28 kB)\n",
            "Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m3.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (13.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m114.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (24.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m94.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (883 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m56.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m5.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m12.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m7.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m5.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m108.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading torchmetrics-1.7.4-py3-none-any.whl (963 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m963.5/963.5 kB\u001b[0m \u001b[31m61.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: nvidia-nvjitlink-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, lightning-utilities, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12, torchmetrics, pytorch_lightning\n",
            "  Attempting uninstall: nvidia-nvjitlink-cu12\n",
            "    Found existing installation: nvidia-nvjitlink-cu12 12.5.82\n",
            "    Uninstalling nvidia-nvjitlink-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-nvjitlink-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-curand-cu12\n",
            "    Found existing installation: nvidia-curand-cu12 10.3.6.82\n",
            "    Uninstalling nvidia-curand-cu12-10.3.6.82:\n",
            "      Successfully uninstalled nvidia-curand-cu12-10.3.6.82\n",
            "  Attempting uninstall: nvidia-cufft-cu12\n",
            "    Found existing installation: nvidia-cufft-cu12 11.2.3.61\n",
            "    Uninstalling nvidia-cufft-cu12-11.2.3.61:\n",
            "      Successfully uninstalled nvidia-cufft-cu12-11.2.3.61\n",
            "  Attempting uninstall: nvidia-cuda-runtime-cu12\n",
            "    Found existing installation: nvidia-cuda-runtime-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-runtime-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-runtime-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n",
            "    Found existing installation: nvidia-cuda-nvrtc-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-nvrtc-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-cupti-cu12\n",
            "    Found existing installation: nvidia-cuda-cupti-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-cupti-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-cupti-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cublas-cu12\n",
            "    Found existing installation: nvidia-cublas-cu12 12.5.3.2\n",
            "    Uninstalling nvidia-cublas-cu12-12.5.3.2:\n",
            "      Successfully uninstalled nvidia-cublas-cu12-12.5.3.2\n",
            "  Attempting uninstall: nvidia-cusparse-cu12\n",
            "    Found existing installation: nvidia-cusparse-cu12 12.5.1.3\n",
            "    Uninstalling nvidia-cusparse-cu12-12.5.1.3:\n",
            "      Successfully uninstalled nvidia-cusparse-cu12-12.5.1.3\n",
            "  Attempting uninstall: nvidia-cudnn-cu12\n",
            "    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\n",
            "    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\n",
            "      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\n",
            "  Attempting uninstall: nvidia-cusolver-cu12\n",
            "    Found existing installation: nvidia-cusolver-cu12 11.6.3.83\n",
            "    Uninstalling nvidia-cusolver-cu12-11.6.3.83:\n",
            "      Successfully uninstalled nvidia-cusolver-cu12-11.6.3.83\n",
            "Successfully installed lightning-utilities-0.14.3 nvidia-cublas-cu12-12.4.5.8 nvidia-cuda-cupti-cu12-12.4.127 nvidia-cuda-nvrtc-cu12-12.4.127 nvidia-cuda-runtime-cu12-12.4.127 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-nvjitlink-cu12-12.4.127 pytorch_lightning-2.5.2 torchmetrics-1.7.4\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test_midis = ['monteverdi_libri_dei_madrigali_1_5_(c)icking-archive.mid',\n",
        "              'monteverdi_libri_dei_madrigali_1_10_(c)icking-archive.mid',\n",
        "              'monteverdi_libri_dei_madrigali_4_12_(c)icking-archive.mid',\n",
        "              'monteverdi_libri_dei_madrigali_4_13_(c)icking-archive.mid']"
      ],
      "metadata": {
        "id": "IGZzKlXyhFfL"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from mido import MidiFile\n",
        "\n",
        "input_folder = 'path/to/your/midi/folder'\n",
        "output_folder = 'path/to/your/output/folder'\n",
        "\n",
        "# Make sure the output folder exists\n",
        "os.makedirs(output_folder, exist_ok=True)\n",
        "\n",
        "def Midi_File_Input(mids):\n",
        "    token = []\n",
        "    for i, track in enumerate(mids.tracks):\n",
        "        for msg in track:\n",
        "            if msg.type == 'note_on':\n",
        "                token.append(f\"<{msg.type}_{msg.channel}_{msg.note}_{msg.velocity}>\")\n",
        "            elif msg.type == 'note_off':\n",
        "                token.append(f\"<{msg.type}_{msg.channel}_{msg.note}_{msg.velocity}>\")\n",
        "            # elif msg.type == 'track_name':\n",
        "            #     token.append(f\"<{msg.type}_{msg.name}>\")\n",
        "            elif msg.type == 'control_change':\n",
        "                token.append(f\"<{msg.type}_{msg.channel}_{msg.control}_{msg.value}>\")\n",
        "            elif msg.type == 'program_change':\n",
        "                token.append(f\"<{msg.type}_{msg.program}_>\")\n",
        "            elif msg.type == 'key_signature':\n",
        "                token.append(f\"<{msg.type}_{msg.key}>\")\n",
        "    return token\n",
        "\n",
        "token_data = {}\n",
        "all_tokens = []  # Flat list for training\n",
        "\n",
        "\n",
        "# Process each MIDI file\n",
        "for filename in test_midis:\n",
        "    midi_path = '/content/drive/MyDrive/Final Project Folder/Midi Files/' + filename\n",
        "\n",
        "    midi = MidiFile(midi_path)\n",
        "\n",
        "    tokens = Midi_File_Input(midi)\n",
        "\n",
        "    # Add song_start and song_end markers\n",
        "    song_tokens = ['<song_start>'] + tokens + ['<song_end>']\n",
        "\n",
        "    # Store in dictionary (per-song)\n",
        "    token_data[filename] = song_tokens\n",
        "\n",
        "    # Append to flat list (for generative model training)\n",
        "    all_tokens.extend(song_tokens)\n"
      ],
      "metadata": {
        "id": "LQO2Rqa-edQc"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from collections import Counter\n",
        "\n",
        "# Flatten all tokens across all songs\n",
        "all_tokens_flat = [token for tokens in token_data.values() for token in tokens]\n",
        "\n",
        "# Count and sort tokens (optional for ordering)\n",
        "token_freq = Counter(all_tokens_flat)\n",
        "\n",
        "# Assign token ID\n",
        "vocab = {token: idx for idx, token in enumerate(sorted(token_freq))}\n",
        "\n",
        "# Optionally store reverse map too:\n",
        "inv_vocab = {idx: token for token, idx in vocab.items()}\n",
        "\n",
        "token_ids_data = {}\n",
        "\n",
        "for filename, tokens in token_data.items():\n",
        "    token_ids = [vocab[token] for token in tokens]\n",
        "    token_ids_data[filename] = token_ids\n",
        "\n",
        "vocab[\"<pad>\"] = 279"
      ],
      "metadata": {
        "id": "rVOUC75henYk"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W71FfMJxhI9A",
        "outputId": "495dfa3b-b37e-45ea-8864-a89d71ef1e19"
      },
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{0: '<control_change_0_100_0>',\n",
              " 1: '<control_change_0_101_0>',\n",
              " 2: '<control_change_0_10_104>',\n",
              " 3: '<control_change_0_10_24>',\n",
              " 4: '<control_change_0_10_44>',\n",
              " 5: '<control_change_0_10_64>',\n",
              " 6: '<control_change_0_10_84>',\n",
              " 7: '<control_change_0_12_3>',\n",
              " 8: '<control_change_0_38_0>',\n",
              " 9: '<control_change_0_6_12>',\n",
              " 10: '<control_change_0_7_100>',\n",
              " 11: '<control_change_0_7_101>',\n",
              " 12: '<control_change_0_7_102>',\n",
              " 13: '<control_change_0_7_103>',\n",
              " 14: '<control_change_0_7_104>',\n",
              " 15: '<control_change_0_7_105>',\n",
              " 16: '<control_change_0_7_106>',\n",
              " 17: '<control_change_0_7_107>',\n",
              " 18: '<control_change_0_7_108>',\n",
              " 19: '<control_change_0_7_109>',\n",
              " 20: '<control_change_0_7_110>',\n",
              " 21: '<control_change_0_7_111>',\n",
              " 22: '<control_change_0_7_112>',\n",
              " 23: '<control_change_0_7_113>',\n",
              " 24: '<control_change_0_7_114>',\n",
              " 25: '<control_change_0_7_115>',\n",
              " 26: '<control_change_0_7_116>',\n",
              " 27: '<control_change_0_7_117>',\n",
              " 28: '<control_change_0_7_118>',\n",
              " 29: '<control_change_0_7_83>',\n",
              " 30: '<control_change_0_7_84>',\n",
              " 31: '<control_change_0_7_85>',\n",
              " 32: '<control_change_0_7_86>',\n",
              " 33: '<control_change_0_7_87>',\n",
              " 34: '<control_change_0_7_88>',\n",
              " 35: '<control_change_0_7_89>',\n",
              " 36: '<control_change_0_7_90>',\n",
              " 37: '<control_change_0_7_91>',\n",
              " 38: '<control_change_0_7_92>',\n",
              " 39: '<control_change_0_7_93>',\n",
              " 40: '<control_change_0_7_94>',\n",
              " 41: '<control_change_0_7_95>',\n",
              " 42: '<control_change_0_7_96>',\n",
              " 43: '<control_change_0_7_97>',\n",
              " 44: '<control_change_0_7_98>',\n",
              " 45: '<control_change_0_7_99>',\n",
              " 46: '<control_change_0_91_76>',\n",
              " 47: '<key_signature_Am>',\n",
              " 48: '<key_signature_Dm>',\n",
              " 49: '<note_off_0_43_0>',\n",
              " 50: '<note_off_0_45_0>',\n",
              " 51: '<note_off_0_46_0>',\n",
              " 52: '<note_off_0_47_0>',\n",
              " 53: '<note_off_0_48_0>',\n",
              " 54: '<note_off_0_49_0>',\n",
              " 55: '<note_off_0_50_0>',\n",
              " 56: '<note_off_0_51_0>',\n",
              " 57: '<note_off_0_52_0>',\n",
              " 58: '<note_off_0_53_0>',\n",
              " 59: '<note_off_0_54_0>',\n",
              " 60: '<note_off_0_55_0>',\n",
              " 61: '<note_off_0_56_0>',\n",
              " 62: '<note_off_0_57_0>',\n",
              " 63: '<note_off_0_58_0>',\n",
              " 64: '<note_off_0_59_0>',\n",
              " 65: '<note_off_0_60_0>',\n",
              " 66: '<note_off_0_61_0>',\n",
              " 67: '<note_off_0_62_0>',\n",
              " 68: '<note_off_0_63_0>',\n",
              " 69: '<note_off_0_64_0>',\n",
              " 70: '<note_off_0_65_0>',\n",
              " 71: '<note_off_0_66_0>',\n",
              " 72: '<note_off_0_67_0>',\n",
              " 73: '<note_off_0_68_0>',\n",
              " 74: '<note_off_0_69_0>',\n",
              " 75: '<note_off_0_70_0>',\n",
              " 76: '<note_off_0_71_0>',\n",
              " 77: '<note_off_0_72_0>',\n",
              " 78: '<note_off_0_73_0>',\n",
              " 79: '<note_off_0_74_0>',\n",
              " 80: '<note_off_0_75_0>',\n",
              " 81: '<note_off_0_76_0>',\n",
              " 82: '<note_off_0_77_0>',\n",
              " 83: '<note_off_0_78_0>',\n",
              " 84: '<note_off_0_79_0>',\n",
              " 85: '<note_off_0_81_0>',\n",
              " 86: '<note_on_0_43_125>',\n",
              " 87: '<note_on_0_45_113>',\n",
              " 88: '<note_on_0_45_120>',\n",
              " 89: '<note_on_0_45_125>',\n",
              " 90: '<note_on_0_46_127>',\n",
              " 91: '<note_on_0_47_125>',\n",
              " 92: '<note_on_0_47_127>',\n",
              " 93: '<note_on_0_48_124>',\n",
              " 94: '<note_on_0_48_125>',\n",
              " 95: '<note_on_0_48_126>',\n",
              " 96: '<note_on_0_48_127>',\n",
              " 97: '<note_on_0_49_125>',\n",
              " 98: '<note_on_0_50_120>',\n",
              " 99: '<note_on_0_50_124>',\n",
              " 100: '<note_on_0_50_125>',\n",
              " 101: '<note_on_0_50_126>',\n",
              " 102: '<note_on_0_50_127>',\n",
              " 103: '<note_on_0_51_126>',\n",
              " 104: '<note_on_0_51_127>',\n",
              " 105: '<note_on_0_52_119>',\n",
              " 106: '<note_on_0_52_120>',\n",
              " 107: '<note_on_0_52_124>',\n",
              " 108: '<note_on_0_52_125>',\n",
              " 109: '<note_on_0_52_126>',\n",
              " 110: '<note_on_0_52_127>',\n",
              " 111: '<note_on_0_53_114>',\n",
              " 112: '<note_on_0_53_124>',\n",
              " 113: '<note_on_0_53_125>',\n",
              " 114: '<note_on_0_53_126>',\n",
              " 115: '<note_on_0_53_127>',\n",
              " 116: '<note_on_0_54_120>',\n",
              " 117: '<note_on_0_54_124>',\n",
              " 118: '<note_on_0_54_125>',\n",
              " 119: '<note_on_0_54_127>',\n",
              " 120: '<note_on_0_55_113>',\n",
              " 121: '<note_on_0_55_119>',\n",
              " 122: '<note_on_0_55_120>',\n",
              " 123: '<note_on_0_55_124>',\n",
              " 124: '<note_on_0_55_125>',\n",
              " 125: '<note_on_0_55_126>',\n",
              " 126: '<note_on_0_55_127>',\n",
              " 127: '<note_on_0_56_125>',\n",
              " 128: '<note_on_0_57_113>',\n",
              " 129: '<note_on_0_57_116>',\n",
              " 130: '<note_on_0_57_118>',\n",
              " 131: '<note_on_0_57_120>',\n",
              " 132: '<note_on_0_57_124>',\n",
              " 133: '<note_on_0_57_125>',\n",
              " 134: '<note_on_0_57_126>',\n",
              " 135: '<note_on_0_57_127>',\n",
              " 136: '<note_on_0_58_113>',\n",
              " 137: '<note_on_0_58_114>',\n",
              " 138: '<note_on_0_58_125>',\n",
              " 139: '<note_on_0_58_126>',\n",
              " 140: '<note_on_0_58_127>',\n",
              " 141: '<note_on_0_59_113>',\n",
              " 142: '<note_on_0_59_116>',\n",
              " 143: '<note_on_0_59_120>',\n",
              " 144: '<note_on_0_59_124>',\n",
              " 145: '<note_on_0_59_125>',\n",
              " 146: '<note_on_0_59_126>',\n",
              " 147: '<note_on_0_59_127>',\n",
              " 148: '<note_on_0_60_113>',\n",
              " 149: '<note_on_0_60_114>',\n",
              " 150: '<note_on_0_60_116>',\n",
              " 151: '<note_on_0_60_118>',\n",
              " 152: '<note_on_0_60_120>',\n",
              " 153: '<note_on_0_60_124>',\n",
              " 154: '<note_on_0_60_125>',\n",
              " 155: '<note_on_0_60_126>',\n",
              " 156: '<note_on_0_60_127>',\n",
              " 157: '<note_on_0_61_120>',\n",
              " 158: '<note_on_0_61_121>',\n",
              " 159: '<note_on_0_61_125>',\n",
              " 160: '<note_on_0_61_126>',\n",
              " 161: '<note_on_0_61_127>',\n",
              " 162: '<note_on_0_62_113>',\n",
              " 163: '<note_on_0_62_116>',\n",
              " 164: '<note_on_0_62_120>',\n",
              " 165: '<note_on_0_62_125>',\n",
              " 166: '<note_on_0_62_126>',\n",
              " 167: '<note_on_0_62_127>',\n",
              " 168: '<note_on_0_63_120>',\n",
              " 169: '<note_on_0_63_125>',\n",
              " 170: '<note_on_0_63_126>',\n",
              " 171: '<note_on_0_63_127>',\n",
              " 172: '<note_on_0_64_113>',\n",
              " 173: '<note_on_0_64_116>',\n",
              " 174: '<note_on_0_64_118>',\n",
              " 175: '<note_on_0_64_120>',\n",
              " 176: '<note_on_0_64_121>',\n",
              " 177: '<note_on_0_64_124>',\n",
              " 178: '<note_on_0_64_125>',\n",
              " 179: '<note_on_0_64_126>',\n",
              " 180: '<note_on_0_64_127>',\n",
              " 181: '<note_on_0_65_113>',\n",
              " 182: '<note_on_0_65_114>',\n",
              " 183: '<note_on_0_65_116>',\n",
              " 184: '<note_on_0_65_118>',\n",
              " 185: '<note_on_0_65_120>',\n",
              " 186: '<note_on_0_65_125>',\n",
              " 187: '<note_on_0_65_126>',\n",
              " 188: '<note_on_0_65_127>',\n",
              " 189: '<note_on_0_66_120>',\n",
              " 190: '<note_on_0_66_121>',\n",
              " 191: '<note_on_0_66_124>',\n",
              " 192: '<note_on_0_66_125>',\n",
              " 193: '<note_on_0_66_126>',\n",
              " 194: '<note_on_0_66_127>',\n",
              " 195: '<note_on_0_67_113>',\n",
              " 196: '<note_on_0_67_114>',\n",
              " 197: '<note_on_0_67_116>',\n",
              " 198: '<note_on_0_67_119>',\n",
              " 199: '<note_on_0_67_120>',\n",
              " 200: '<note_on_0_67_124>',\n",
              " 201: '<note_on_0_67_125>',\n",
              " 202: '<note_on_0_67_126>',\n",
              " 203: '<note_on_0_67_127>',\n",
              " 204: '<note_on_0_68_120>',\n",
              " 205: '<note_on_0_68_125>',\n",
              " 206: '<note_on_0_68_126>',\n",
              " 207: '<note_on_0_68_127>',\n",
              " 208: '<note_on_0_69_113>',\n",
              " 209: '<note_on_0_69_116>',\n",
              " 210: '<note_on_0_69_118>',\n",
              " 211: '<note_on_0_69_119>',\n",
              " 212: '<note_on_0_69_120>',\n",
              " 213: '<note_on_0_69_124>',\n",
              " 214: '<note_on_0_69_125>',\n",
              " 215: '<note_on_0_69_126>',\n",
              " 216: '<note_on_0_69_127>',\n",
              " 217: '<note_on_0_70_113>',\n",
              " 218: '<note_on_0_70_114>',\n",
              " 219: '<note_on_0_70_125>',\n",
              " 220: '<note_on_0_70_126>',\n",
              " 221: '<note_on_0_70_127>',\n",
              " 222: '<note_on_0_71_116>',\n",
              " 223: '<note_on_0_71_120>',\n",
              " 224: '<note_on_0_71_124>',\n",
              " 225: '<note_on_0_71_125>',\n",
              " 226: '<note_on_0_71_126>',\n",
              " 227: '<note_on_0_71_127>',\n",
              " 228: '<note_on_0_72_113>',\n",
              " 229: '<note_on_0_72_114>',\n",
              " 230: '<note_on_0_72_116>',\n",
              " 231: '<note_on_0_72_119>',\n",
              " 232: '<note_on_0_72_120>',\n",
              " 233: '<note_on_0_72_124>',\n",
              " 234: '<note_on_0_72_125>',\n",
              " 235: '<note_on_0_72_126>',\n",
              " 236: '<note_on_0_72_127>',\n",
              " 237: '<note_on_0_73_125>',\n",
              " 238: '<note_on_0_73_126>',\n",
              " 239: '<note_on_0_73_127>',\n",
              " 240: '<note_on_0_74_113>',\n",
              " 241: '<note_on_0_74_114>',\n",
              " 242: '<note_on_0_74_116>',\n",
              " 243: '<note_on_0_74_118>',\n",
              " 244: '<note_on_0_74_120>',\n",
              " 245: '<note_on_0_74_124>',\n",
              " 246: '<note_on_0_74_125>',\n",
              " 247: '<note_on_0_74_126>',\n",
              " 248: '<note_on_0_74_127>',\n",
              " 249: '<note_on_0_75_120>',\n",
              " 250: '<note_on_0_75_121>',\n",
              " 251: '<note_on_0_75_125>',\n",
              " 252: '<note_on_0_75_126>',\n",
              " 253: '<note_on_0_75_127>',\n",
              " 254: '<note_on_0_76_113>',\n",
              " 255: '<note_on_0_76_114>',\n",
              " 256: '<note_on_0_76_120>',\n",
              " 257: '<note_on_0_76_121>',\n",
              " 258: '<note_on_0_76_124>',\n",
              " 259: '<note_on_0_76_125>',\n",
              " 260: '<note_on_0_76_126>',\n",
              " 261: '<note_on_0_76_127>',\n",
              " 262: '<note_on_0_77_113>',\n",
              " 263: '<note_on_0_77_114>',\n",
              " 264: '<note_on_0_77_125>',\n",
              " 265: '<note_on_0_77_126>',\n",
              " 266: '<note_on_0_77_127>',\n",
              " 267: '<note_on_0_78_120>',\n",
              " 268: '<note_on_0_78_125>',\n",
              " 269: '<note_on_0_78_127>',\n",
              " 270: '<note_on_0_79_120>',\n",
              " 271: '<note_on_0_79_125>',\n",
              " 272: '<note_on_0_79_126>',\n",
              " 273: '<note_on_0_79_127>',\n",
              " 274: '<note_on_0_81_125>',\n",
              " 275: '<program_change_52_>',\n",
              " 276: '<song_end>',\n",
              " 277: '<song_start>'}"
            ]
          },
          "metadata": {},
          "execution_count": 46
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Positional Encoding"
      ],
      "metadata": {
        "id": "z7AzOvzPMqhZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class PositionalEncoding(nn.Module):\n",
        "  def __init__(self,d_model, max_length):\n",
        "\n",
        "    super().__init__()\n",
        "\n",
        "    pe = torch.zeros(max_length, d_model) #Here we start by creating an empty matrix which will get updated.\n",
        "\n",
        "  #d_model is the number of word embedding dimensions the larger this is the richer our description of a word\n",
        "  #The actual word embedding values are updated via back propagation\n",
        "\n",
        "    position = torch.arange(start = 0, end = max_length, step = 1).float().unsqueeze(1)\n",
        "\n",
        "    embedding_index = torch.arange(start=0, end=d_model,step=2).float()\n",
        "\n",
        "    div_term = 1/torch.tensor(10000.0)**(embedding_index/ d_model)\n",
        "\n",
        "\n",
        "    #Our Positional encoding starts with a sin and cos equation.\n",
        "  #Have a look at the literture to check why we do this but it's not too difficult to understand\n",
        "\n",
        "    pe[:, 0::2] = torch.sin(position * div_term) #This updates the first column\n",
        "    pe[:, 1::2] = torch.cos(position * div_term) # This updates the second column\n",
        "\n",
        "  #This happens in an alternating method where the first column and the secondary column gets updated\n",
        "\n",
        "    self.register_buffer('pe', pe.unsqueeze(0))\n",
        "\n",
        "  def forward (self, word_embeddings):\n",
        "\n",
        "    seq_len = word_embeddings.size(1)\n",
        "    return word_embeddings + self.pe[:, :seq_len, :]\n",
        "\n",
        "    #return word_embeddings + self.pe[:word_embeddings.size(0), :] #Finally we add the positonal encoding values to the word embedding values"
      ],
      "metadata": {
        "id": "sv0dKspRMfk5"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Attention Coding"
      ],
      "metadata": {
        "id": "96eU94ZLMohC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Attention(nn.Module):\n",
        "  def __init__(self, d_model):\n",
        "\n",
        "#Here we pass in the number of word embedding values per token This is for the sake of matrix nultiplication\n",
        "#When we do our matrix multiplication to create the queries, keys and values.\n",
        "\n",
        "    super().__init__()\n",
        "\n",
        "    self.W_q = nn.Linear(in_features = d_model, out_features = d_model, bias = False)\n",
        "    self.W_k = nn.Linear(in_features = d_model, out_features = d_model, bias = False)\n",
        "    self.W_v = nn.Linear(in_features = d_model, out_features = d_model, bias = False)\n",
        "\n",
        "    #Above we have our weight matrix which are used to calculate our Query, Keys and Values\n",
        "    #Finally, we don't include a bias term here when calculating attention which is why we set the bias to False\n",
        "\n",
        "    self.row_dim = 0\n",
        "    self.col_dim = 1 #To give us flexibility to input data sequentiallly into batches we create variables to do so\n",
        "\n",
        "  def forward(self, encodings_for_q, encodings_for_k, encodings_for_v, mask = None):\n",
        "\n",
        "      #The forward method is where we use the masked self attention values for each token to be calculated\n",
        "      #Next we give this forward method the flexbility\n",
        "      #Finally since we want to do masked self attention we can pass in a mask too.\n",
        "\n",
        "    q = self.W_q(encodings_for_q)\n",
        "    k = self.W_k(encodings_for_k)\n",
        "    v = self.W_v(encodings_for_v)\n",
        "\n",
        "    #sims = torch.matmul(q, k.transpose(dim0 = self.row_dim, dim1 = self.col_dim))\n",
        "\n",
        "    #scaled_sims = sims / torch.tensor(k.size(self.col_dim)**0.5)\n",
        "\n",
        "    sims = torch.matmul(q, k.transpose(-2, -1))  # [batch_size, seq_len, seq_len]\n",
        "\n",
        "    scaled_sims = sims / (k.size(-1) ** 0.5)\n",
        "\n",
        "    if mask is not None:\n",
        "      scaled_sims = scaled_sims.masked_fill(mask = mask, value=-1e9) #Here we include our mask values which stops the attention from looking ahead to caluclate weights.\n",
        "\n",
        "    attention_percents = F.softmax(scaled_sims, dim = self.col_dim)\n",
        "\n",
        "    attention_scores = torch.matmul(attention_percents, v)\n",
        "\n",
        "    return attention_scores"
      ],
      "metadata": {
        "id": "A4LfLUFYMnf5"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "from torch.optim import Adam\n",
        "from torch.utils.data import TensorDataset, DataLoader\n",
        "\n",
        "#from pytorch_lightning import\n",
        "from pytorch_lightning import Trainer, LightningModule\n",
        "\n",
        "\n",
        "class DecodeOTrans(LightningModule):\n",
        "  def __init__(self, d_model, max_length, num_tokens = 100):\n",
        "\n",
        "    #Here we specify the number of tokens available in the vocabulary,\n",
        "    #Number of word embeddings per token\n",
        "    #Max token length\n",
        "    super().__init__()\n",
        "\n",
        "    self.we = nn.Embedding(num_embeddings = num_tokens, embedding_dim = d_model) #We creating a word embedding value\n",
        "    #Embeddings needs to know how many tokens are in the vocabulary and understand the dimension size to represent the embedding\n",
        "\n",
        "    self.pe = PositionalEncoding(d_model = d_model, max_length = max_length)\n",
        "\n",
        "    #Then we create a positional encoding object using the class we created earlier\n",
        "\n",
        "    self.self_attention = Attention(d_model = d_model)\n",
        "\n",
        "    self.fc_layer = nn.Linear(in_features = d_model, out_features = num_tokens) #This is our fully connected layer also know as our Dense Layer of neurons (RNN neural networks)\n",
        "\n",
        "    self.loss = nn.CrossEntropyLoss()\n",
        "\n",
        "  def encode_state(self, token_ids):\n",
        "\n",
        "    word_embeddings = self.we(token_ids)  # [batch_size, seq_len, d_model]\n",
        "    positional_encoding = self.pe(word_embeddings)  # add positional encoding\n",
        "\n",
        "    seq_len = token_ids.size(1)\n",
        "    #mask = torch.tril(torch.ones((seq_len, seq_len), device=token_ids.device)) == 0\n",
        "\n",
        "    mask = torch.tril(torch.ones((seq_len, seq_len), device=token_ids.device)).unsqueeze(0) == 0\n",
        "\n",
        "    self_attention_values = self.self_attention(\n",
        "        positional_encoding,\n",
        "        positional_encoding,\n",
        "        positional_encoding,\n",
        "        mask=mask\n",
        "    )\n",
        "\n",
        "    contextual_embeddings = positional_encoding + self_attention_values\n",
        "\n",
        "    # Option A: use last token's hidden state\n",
        "    state_vector = contextual_embeddings[:, -1, :]\n",
        "\n",
        "    # OR Option B: use mean pooling\n",
        "    # state_vector = contextual_embeddings.mean(dim=1)\n",
        "\n",
        "    return state_vector  # [batch_size, d_model]\n",
        "\n",
        "  def forward(self, token_ids):\n",
        "    return self.encode_state(token_ids)\n",
        "\n",
        "#Difference between state vectors and probability logits in a sentence\n",
        "# The state vector is a rich, high-dimensional summary of the input sequence\n",
        "#  (via embeddings + position + attention),\n",
        "#  and the logits are what you get after mapping that state vector\n",
        "#  through a final linear layer into output space\n",
        "#   (like actions or vocabulary)."
      ],
      "metadata": {
        "id": "w2lmwLKuKUxV"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This is my Practice exploratory code"
      ],
      "metadata": {
        "id": "T7ZMu_EsCc2m"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "x = DecodeOTrans(d_model=128, max_length=20, num_tokens=len(vocab))\n"
      ],
      "metadata": {
        "id": "FuuuxWIr2wOp"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sample_input = torch.randint(0, len(vocab), (1, 10))\n",
        "\n",
        "#The torch.randit creates an empty matrix to place my state vector values into\n",
        "\n",
        "state_vector = x(sample_input)\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 233
        },
        "id": "BI4QyMOO44Lw",
        "outputId": "0389c60f-31ac-4bc5-af34-7fecc78328a6"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'state_vecotr' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-19-2004032759.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mstate_vector\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msample_input\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0mstate_vecotr\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'state_vecotr' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sample_input"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7wKyGzCn7moc",
        "outputId": "ade36eec-3712-4959-f588-809941926196"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[ 77, 133, 106,  35,  79,  28,  42, 139, 259,  97]])"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Above is my state vector  we get the (batch_size, d_model) where d_model is the size of the embeddings so each token gets the same number of numebrs which represent the token."
      ],
      "metadata": {
        "id": "G5iQBFNt7s1g"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "XWsuRCnQ888d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class QNet(nn.Module):\n",
        "    def __init__(self, input_dim, hidden_dim, num_actions):\n",
        "        super().__init__()\n",
        "        self.linear1 = nn.Linear(input_dim, hidden_dim)\n",
        "        self.linear2 = nn.Linear(hidden_dim, num_actions)\n",
        "\n",
        "    def forward(self, state_vector):\n",
        "        x = F.relu(self.linear1(state_vector))\n",
        "        return self.linear2(x)  # Q-values for each action\n"
      ],
      "metadata": {
        "id": "F7q3U1GeP-DM"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Agent RL + Trans (For Music GPT Adapted)"
      ],
      "metadata": {
        "id": "4rt2jBfFqUMU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class MusicDQNAgent:\n",
        "    def __init__(self, d_model, max_length, num_tokens, num_actions):\n",
        "\n",
        "      self.encoder = DecodeOTrans(d_model, max_length, num_tokens)\n",
        "      #This is our transformer based encoder we created which takes the tokens and outputs it as dense vectors with positional + attentional mechanism converted into a simple numerical format\n",
        "      self.q_net = QNet(d_model, 128, num_actions)\n",
        "      # This is the Q neural network this predicts possible actions from the inputted state.\n",
        "\n",
        "      self.gamma = 0.99\n",
        "\n",
        "      #Hyperparameter tells the model to reward future or immediate rewards.\n",
        "      self.epsilon = 0.1\n",
        "\n",
        "      #This is the exploration rate and tells the model to explore so choose randomly to further explore the entire environment and not get stuck at local minima.\n",
        "      #self.memory = deque(maxlen=10000)\n",
        "\n",
        "      #This stores tuples of (state, action, reward, next_state, done).\n",
        "      self.optimizer = torch.optim.Adam(list(self.encoder.parameters()) + list(self.q_net.parameters()), lr=1e-4)\n",
        "\n",
        "      #This is our optimiser Adam here we're balancing the predicted and Q values.\n",
        "\n",
        "      #Parameters\n",
        "\n",
        "        #d_model: The dimensionality of each token vector the larger the dimensionality the more detail which is included in the tokens descriptions\n",
        "        #max_length: maximum sequence length expected by the transformer\n",
        "        #num_tokens: Total number of unique tokens in my vocabulary\n",
        "        #num_actions: The number of possible actions the agent can choose from\n",
        "    def get_state(self, token_sequence, no_grad=True):\n",
        "\n",
        "        # token_sequence: [1, seq_len]\n",
        "      return self.encoder(token_sequence) # [1, d_model]\n",
        "\n",
        "\n",
        "    #Here we are defining how we are going to grab the state we are in\n",
        "\n",
        "    def get_action(self, state_vector):\n",
        "      if random.random() < self.epsilon:\n",
        "        #This generates a float between 0 and 1 if the epison is larger then it will got with epsilon\n",
        "        #Therefore the larger the episoln the less exploration there is\n",
        "          return random.randint(0, self.q_net.linear2.out_features - 1)\n",
        "            #If we are exploring then return a random token where num_tokens -1 would be the next one.\n",
        "      q_values = self.q_net(state_vector)\n",
        "        #If we are not exploring then use the current state to predict the next state using the Q Net.\n",
        "      return torch.argmax(q_values).item()\n",
        "\n",
        "    #     #Here we choose the token with the highest indexed value\n",
        "    # def remember(self, state, action, reward, next_state, done):\n",
        "\n",
        "    #   self.memory.append((state, action, reward, next_state, done))\n",
        "\n",
        "    # def train_short_memory(self, state, action, reward, next_state, done):\n",
        "    #     self.train_step(state, action, reward, next_state, done)\n",
        "\n",
        "    # def train_long_memory(self, batch_size=64):\n",
        "    #     if len(self.memory) < batch_size:\n",
        "    #         return\n",
        "\n",
        "    #     mini_batch = random.sample(self.memory, batch_size)\n",
        "\n",
        "    #     for state, action, reward, next_state, done in mini_batch:\n",
        "    #         self.train_step(state, action, reward, next_state, done)\n",
        "\n",
        "    def compute_reward(predicted_token, target_token):\n",
        "\n",
        "      return 1.0 if predicted_token == target_token else -0.1\n",
        "\n",
        "    def train_step(self, state, action, reward, next_state, done):\n",
        "\n",
        "      self.q_net.train()\n",
        "\n",
        "        #This puts the Q neural network into training mode\n",
        "\n",
        "      q_values = self.q_net(state)\n",
        "\n",
        "        #So forward passes through the Q network\n",
        "      target = q_values.clone().detach()\n",
        "\n",
        "        #Create a copy of the Q-values tensor.\n",
        "\n",
        "        #Detach() removes it from the computation graph — this prevents gradients from flowing through the target.\n",
        "\n",
        "      next_q = self.q_net(next_state)\n",
        "      q_target = reward + self.gamma * torch.max(next_q) * (1 - int(done))\n",
        "\n",
        "        #We're computing the target Q-value using the Bellman equation:\n",
        "\n",
        "      target[0, action] = q_target\n",
        "\n",
        "        #We now replace only the Q-value for the action we actually took with the target value.\n",
        "\n",
        "        #This tells the network:\n",
        "\n",
        "        #\"You predicted X for action A, but the real value should have been Y — update your weights accordingly.\"\n",
        "\n",
        "      loss = F.mse_loss(q_values, target)\n",
        "      self.optimizer.zero_grad()\n",
        "      loss.backward()\n",
        "      self.optimizer.step()\n",
        "\n",
        "        #Mean Squared Error between:\n",
        "\n",
        "        #q_values: what the network predicted\n",
        "\n",
        "        #target: what it should have predicted (Bellman update)\n",
        "\n",
        "        #Then we run standard backprop and gradient descent."
      ],
      "metadata": {
        "id": "99u4RQh0qcvl"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for index in test_midis:\n",
        "  print(index)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WNgpyFRRaTFn",
        "outputId": "f438b7f6-781c-43b9-916f-65285704ef78"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "monteverdi_libri_dei_madrigali_1_5_(c)icking-archive.mid\n",
            "monteverdi_libri_dei_madrigali_1_10_(c)icking-archive.mid\n",
            "monteverdi_libri_dei_madrigali_4_12_(c)icking-archive.mid\n",
            "monteverdi_libri_dei_madrigali_4_13_(c)icking-archive.mid\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "input_sequences = []\n",
        "\n",
        "\n",
        "for i in test_midis:\n",
        "  print(token_ids_data[i])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Uy3ylVT_ecXE",
        "outputId": "7313f979-34f2-40e9-cb81-3286eb385ff0"
      },
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[277, 48, 275, 28, 28, 28, 28, 28, 2, 6, 5, 4, 3, 46, 7, 46, 7, 46, 7, 46, 7, 46, 7, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 9, 9, 9, 9, 9, 8, 8, 8, 8, 8, 13, 13, 13, 13, 248, 221, 203, 126, 14, 14, 14, 14, 16, 16, 16, 16, 18, 18, 18, 18, 20, 20, 20, 20, 22, 22, 22, 22, 24, 24, 24, 24, 26, 26, 26, 26, 27, 27, 27, 27, 28, 28, 28, 28, 27, 27, 27, 27, 26, 26, 26, 26, 79, 75, 72, 60, 26, 26, 26, 26, 248, 216, 194, 167, 27, 27, 27, 27, 28, 28, 28, 28, 27, 27, 27, 27, 26, 26, 26, 26, 79, 74, 71, 67, 272, 220, 202, 170, 84, 75, 72, 68, 253, 236, 203, 156, 80, 77, 72, 65, 247, 247, 202, 146, 79, 79, 72, 64, 26, 26, 26, 26, 253, 236, 203, 156, 27, 27, 27, 27, 28, 28, 28, 28, 27, 27, 27, 27, 26, 26, 26, 26, 80, 77, 72, 65, 26, 26, 26, 26, 248, 216, 194, 167, 27, 27, 27, 27, 28, 28, 28, 28, 27, 27, 27, 27, 26, 26, 26, 26, 79, 74, 71, 67, 266, 248, 221, 140, 82, 79, 75, 63, 263, 241, 218, 137, 82, 79, 75, 63, 262, 240, 217, 136, 82, 79, 75, 63, 266, 248, 221, 140, 82, 79, 75, 63, 252, 235, 202, 155, 80, 77, 72, 65, 26, 26, 26, 26, 248, 216, 194, 167, 27, 27, 27, 27, 28, 28, 28, 28, 27, 27, 27, 27, 26, 26, 26, 26, 79, 74, 71, 67, 26, 26, 26, 248, 227, 203, 126, 27, 27, 27, 79, 28, 28, 28, 26, 272, 27, 27, 27, 24, 20, 16, 11, 26, 26, 42, 27, 37, 33, 30, 76, 72, 60, 28, 26, 26, 219, 201, 27, 26, 27, 27, 28, 84, 13, 265, 82, 28, 28, 259, 27, 27, 81, 13, 248, 248, 26, 26, 14, 15, 16, 18, 19, 21, 22, 79, 75, 72, 24, 260, 215, 187, 25, 26, 27, 74, 70, 28, 201, 178, 27, 81, 72, 69, 264, 188, 167, 26, 79, 70, 67, 259, 235, 202, 179, 82, 77, 246, 219, 81, 75, 72, 69, 232, 216, 216, 188, 79, 77, 74, 74, 70, 247, 220, 187, 166, 79, 75, 70, 67, 261, 236, 203, 156, 81, 77, 72, 65, 265, 247, 220, 139, 82, 79, 75, 63, 26, 26, 266, 236, 214, 188, 27, 27, 77, 220, 219, 74, 28, 28, 27, 27, 75, 26, 236, 214, 27, 75, 26, 26, 201, 74, 28, 25, 212, 27, 72, 26, 82, 77, 74, 70, 26, 248, 248, 221, 140, 27, 79, 75, 63, 28, 220, 202, 166, 126, 27, 24, 20, 16, 11, 42, 37, 33, 30, 79, 75, 72, 67, 60, 25, 26, 26, 26, 216, 188, 167, 167, 27, 27, 27, 26, 28, 28, 28, 25, 27, 27, 27, 24, 24, 24, 20, 20, 20, 15, 15, 15, 10, 10, 10, 41, 41, 41, 37, 37, 37, 33, 33, 33, 30, 30, 30, 28, 74, 70, 67, 67, 28, 28, 28, 25, 25, 26, 220, 203, 167, 126, 75, 72, 67, 60, 221, 203, 167, 126, 75, 72, 67, 60, 215, 216, 167, 119, 74, 74, 67, 59, 25, 25, 25, 26, 221, 203, 167, 126, 27, 26, 26, 26, 28, 25, 25, 25, 27, 26, 75, 72, 67, 60, 216, 180, 161, 135, 74, 69, 66, 62, 235, 216, 188, 115, 77, 74, 70, 58, 236, 216, 188, 115, 77, 74, 70, 58, 235, 216, 188, 115, 74, 216, 77, 70, 58, 228, 182, 111, 220, 167, 126, 77, 70, 58, 74, 202, 75, 72, 67, 60, 25, 25, 26, 216, 187, 161, 135, 27, 26, 26, 28, 176, 25, 25, 27, 70, 26, 74, 69, 66, 62, 25, 25, 25, 26, 13, 214, 193, 166, 101, 27, 26, 26, 26, 28, 25, 25, 25, 27, 26, 25, 24, 13, 248, 23, 21, 20, 14, 18, 15, 16, 16, 14, 17, 12, 19, 11, 20, 45, 21, 43, 23, 24, 41, 25, 39, 26, 37, 27, 36, 34, 33, 28, 32, 27, 31, 30, 74, 71, 67, 55, 25, 25, 25, 202, 166, 125, 26, 26, 28, 12, 27, 26, 26, 25, 25, 79, 12, 26, 227, 248, 13, 28, 15, 27, 16, 19, 27, 21, 23, 24, 25, 26, 28, 25, 27, 26, 26, 76, 79, 67, 60, 25, 25, 25, 26, 236, 235, 180, 110, 25, 27, 26, 26, 26, 28, 25, 25, 27, 27, 26, 77, 72, 69, 57, 25, 25, 25, 26, 216, 216, 156, 115, 28, 27, 27, 26, 26, 26, 25, 25, 25, 28, 27, 26, 74, 65, 58, 25, 25, 25, 194, 166, 101, 25, 26, 26, 26, 25, 26, 27, 27, 74, 77, 71, 25, 25, 26, 167, 221, 203, 28, 27, 27, 26, 28, 28, 25, 27, 27, 26, 26, 25, 26, 25, 26, 67, 75, 24, 25, 26, 216, 215, 23, 22, 21, 20, 25, 27, 19, 18, 17, 28, 16, 27, 15, 14, 13, 12, 11, 26, 26, 10, 25, 45, 44, 43, 42, 74, 72, 67, 25, 25, 26, 248, 194, 167, 41, 40, 39, 38, 37, 36, 27, 35, 34, 26, 26, 25, 25, 33, 32, 31, 28, 27, 74, 71, 67, 55, 25, 25, 25, 202, 220, 170, 26, 26, 25, 26, 26, 25, 25, 24, 23, 28, 79, 22, 26, 236, 215, 21, 75, 20, 18, 17, 15, 13, 11, 10, 27, 44, 42, 40, 202, 38, 74, 37, 35, 34, 33, 28, 32, 27, 31, 30, 72, 68, 25, 25, 190, 166, 72, 26, 26, 25, 26, 27, 28, 77, 71, 25, 26, 221, 203, 27, 28, 28, 27, 27, 26, 26, 25, 75, 26, 12, 214, 27, 26, 28, 27, 72, 25, 12, 248, 188, 25, 13, 14, 15, 16, 18, 19, 26, 21, 22, 23, 24, 25, 26, 26, 25, 25, 74, 67, 25, 26, 12, 203, 147, 27, 26, 28, 25, 27, 26, 72, 79, 70, 64, 12, 25, 25, 25, 26, 216, 236, 180, 156, 135, 13, 15, 16, 19, 21, 23, 24, 27, 25, 26, 26, 26, 25, 25, 25, 26, 28, 25, 27, 69, 65, 62, 25, 25, 25, 166, 166, 119, 26, 26, 25, 26, 26, 25, 25, 74, 77, 59, 25, 25, 26, 203, 221, 126, 27, 26, 26, 25, 25, 28, 27, 75, 67, 67, 60, 25, 25, 25, 25, 215, 216, 161, 134, 26, 26, 26, 25, 25, 26, 26, 27, 27, 72, 74, 66, 25, 25, 26, 188, 135, 167, 27, 26, 26, 28, 28, 28, 25, 25, 27, 27, 27, 26, 70, 62, 67, 25, 25, 26, 178, 134, 160, 27, 26, 26, 26, 26, 28, 25, 25, 27, 25, 25, 26, 69, 74, 62, 66, 62, 25, 25, 25, 25, 26, 194, 216, 135, 167, 101, 27, 26, 26, 26, 28, 25, 25, 25, 27, 26, 26, 25, 24, 23, 71, 74, 62, 67, 22, 25, 25, 25, 26, 248, 194, 216, 167, 21, 20, 18, 17, 15, 27, 13, 11, 10, 44, 42, 26, 26, 26, 28, 40, 25, 25, 25, 27, 38, 37, 35, 34, 33, 26, 32, 31, 30, 79, 71, 74, 67, 55, 25, 25, 25, 26, 248, 203, 221, 126, 27, 26, 26, 26, 28, 25, 25, 25, 27, 26, 28, 79, 72, 75, 60, 266, 216, 236, 115, 82, 74, 77, 58, 25, 25, 25, 26, 265, 221, 248, 140, 27, 26, 26, 26, 28, 25, 25, 25, 27, 26, 82, 75, 79, 63, 252, 203, 236, 156, 80, 72, 77, 65, 25, 25, 25, 26, 248, 194, 216, 167, 27, 26, 26, 26, 28, 25, 25, 25, 27, 26, 79, 71, 74, 67, 25, 25, 25, 26, 248, 194, 216, 167, 27, 26, 26, 26, 28, 25, 25, 25, 27, 22, 24, 19, 20, 14, 15, 45, 10, 41, 41, 36, 37, 33, 33, 30, 30, 79, 71, 74, 67, 25, 25, 216, 188, 28, 25, 28, 26, 26, 236, 25, 25, 13, 74, 70, 203, 171, 77, 72, 68, 25, 25, 13, 265, 221, 188, 167, 14, 16, 18, 20, 22, 24, 26, 27, 75, 26, 26, 28, 216, 25, 25, 27, 26, 82, 74, 70, 67, 25, 25, 252, 221, 203, 203, 80, 72, 26, 26, 248, 221, 25, 25, 79, 75, 75, 72, 25, 26, 235, 236, 202, 171, 27, 77, 68, 25, 26, 25, 28, 248, 167, 25, 27, 26, 77, 220, 193, 72, 26, 26, 179, 25, 25, 71, 75, 25, 26, 216, 190, 69, 27, 26, 28, 25, 27, 26, 74, 79, 71, 67, 25, 25, 25, 26, 13, 227, 248, 203, 126, 27, 26, 26, 26, 28, 25, 25, 25, 27, 26, 76, 79, 72, 60, 13, 25, 25, 25, 26, 234, 215, 187, 155, 113, 14, 15, 16, 17, 19, 20, 21, 27, 23, 24, 25, 26, 27, 28, 26, 26, 26, 28, 27, 25, 25, 25, 27, 26, 26, 77, 74, 70, 65, 58, 248, 216, 216, 188, 102, 79, 74, 74, 70, 55, 260, 239, 216, 180, 134, 81, 78, 74, 69, 62, 259, 238, 215, 179, 133, 81, 78, 74, 69, 62, 260, 239, 216, 180, 134, 81, 78, 74, 69, 62, 26, 25, 25, 25, 26, 266, 248, 216, 167, 102, 27, 27, 28, 26, 26, 26, 28, 27, 25, 25, 25, 27, 26, 26, 82, 79, 74, 67, 55, 261, 239, 216, 180, 135, 81, 78, 74, 69, 62, 247, 248, 216, 188, 101, 79, 79, 74, 70, 55, 261, 203, 236, 180, 96, 81, 72, 77, 69, 53, 265, 216, 236, 156, 114, 82, 74, 77, 65, 58, 274, 215, 235, 187, 113, 85, 74, 77, 70, 58, 272, 248, 221, 167, 125, 84, 79, 75, 67, 60, 26, 25, 25, 26, 261, 239, 216, 215, 135, 202, 27, 27, 74, 187, 72, 28, 26, 26, 28, 176, 27, 25, 25, 27, 70, 26, 26, 81, 78, 74, 69, 62, 26, 266, 248, 216, 167, 102, 27, 82, 79, 74, 67, 28, 265, 248, 216, 167, 27, 24, 20, 16, 11, 42, 37, 33, 30, 82, 79, 74, 67, 55, 266, 248, 216, 167, 82, 79, 74, 67, 264, 247, 215, 166, 28, 82, 79, 74, 67, 266, 248, 216, 167, 82, 79, 74, 67, 247, 221, 221, 203, 79, 75, 75, 72, 25, 25, 25, 26, 266, 236, 216, 188, 27, 26, 26, 26, 28, 25, 25, 25, 27, 26, 82, 77, 74, 70, 266, 248, 221, 167, 82, 79, 75, 67, 25, 25, 25, 26, 272, 221, 203, 171, 27, 26, 26, 26, 28, 25, 25, 25, 27, 26, 84, 75, 72, 68, 265, 248, 221, 167, 82, 79, 75, 67, 25, 25, 25, 26, 266, 236, 216, 188, 27, 26, 26, 26, 28, 25, 25, 25, 27, 26, 82, 77, 74, 70, 25, 25, 25, 26, 264, 247, 220, 139, 27, 26, 26, 26, 28, 25, 25, 25, 27, 24, 26, 23, 25, 22, 24, 21, 23, 20, 21, 18, 20, 17, 18, 15, 16, 13, 14, 11, 12, 10, 11, 44, 45, 42, 43, 40, 41, 38, 39, 37, 37, 35, 36, 34, 34, 33, 33, 32, 32, 31, 31, 30, 30, 82, 79, 75, 63, 25, 25, 220, 202, 28, 13, 26, 26, 25, 25, 28, 13, 248, 14, 15, 16, 18, 19, 21, 22, 75, 72, 24, 216, 167, 25, 26, 27, 28, 27, 74, 67, 25, 25, 216, 188, 26, 79, 26, 26, 215, 25, 25, 74, 74, 70, 25, 25, 26, 236, 203, 171, 27, 28, 27, 26, 26, 26, 25, 25, 77, 26, 219, 27, 72, 68, 194, 167, 28, 27, 71, 67, 25, 25, 203, 171, 26, 75, 26, 202, 25, 26, 25, 72, 68, 25, 26, 221, 166, 27, 28, 12, 27, 26, 26, 25, 75, 72, 216, 194, 74, 71, 67, 12, 25, 25, 26, 265, 248, 216, 167, 13, 15, 16, 18, 21, 27, 22, 24, 25, 26, 26, 26, 28, 25, 25, 25, 27, 26, 82, 79, 74, 67, 260, 236, 216, 135, 81, 77, 74, 62, 25, 25, 25, 26, 248, 221, 188, 140, 27, 26, 26, 28, 25, 25, 27, 26, 26, 25, 79, 75, 63, 25, 25, 26, 236, 215, 115, 27, 26, 28, 25, 27, 26, 26, 25, 77, 70, 58, 25, 26, 221, 167, 126, 27, 74, 67, 26, 25, 28, 203, 180, 25, 27, 26, 75, 69, 60, 26, 25, 26, 214, 160, 134, 25, 27, 72, 187, 166, 66, 26, 28, 25, 27, 179, 160, 70, 67, 166, 146, 26, 69, 66, 12, 176, 158, 67, 64, 74, 62, 69, 66, 25, 216, 188, 167, 167, 74, 70, 67, 12, 25, 26, 25, 26, 247, 221, 167, 126, 25, 13, 15, 16, 19, 27, 21, 23, 24, 25, 67, 26, 26, 26, 28, 203, 25, 25, 25, 27, 26, 79, 75, 72, 67, 60, 265, 248, 188, 167, 102, 82, 79, 70, 67, 55, 25, 25, 25, 25, 26, 253, 221, 203, 126, 104, 27, 26, 26, 26, 26, 28, 25, 25, 25, 25, 27, 26, 80, 75, 72, 60, 56, 25, 25, 25, 25, 26, 246, 221, 188, 167, 90, 27, 26, 25, 26, 26, 26, 28, 25, 25, 25, 27, 70, 25, 203, 26, 26, 25, 75, 72, 67, 51, 25, 25, 25, 25, 26, 234, 203, 202, 171, 96, 79, 27, 26, 26, 26, 25, 25, 25, 26, 28, 25, 27, 72, 68, 53, 25, 25, 25, 247, 166, 101, 26, 26, 26, 26, 27, 27, 27, 77, 26, 221, 193, 72, 27, 28, 28, 28, 28, 179, 27, 27, 27, 27, 71, 26, 75, 25, 26, 214, 190, 69, 27, 26, 26, 26, 26, 28, 25, 27, 25, 25, 25, 26, 74, 79, 71, 67, 55, 25, 227, 248, 203, 167, 126, 76, 79, 72, 67, 26, 247, 248, 221, 167, 25, 22, 19, 14, 45, 41, 36, 33, 30, 79, 79, 75, 67, 60, 266, 248, 216, 167, 28, 82, 79, 74, 67, 264, 247, 215, 166, 82, 79, 74, 67, 266, 248, 216, 167, 82, 79, 74, 67, 247, 221, 221, 203, 79, 75, 75, 72, 25, 25, 25, 26, 266, 236, 216, 188, 27, 26, 26, 26, 28, 25, 25, 25, 27, 26, 82, 77, 74, 70, 266, 248, 221, 167, 82, 79, 75, 67, 25, 25, 25, 26, 272, 221, 203, 171, 27, 26, 26, 26, 28, 25, 25, 25, 27, 26, 84, 75, 72, 68, 265, 248, 221, 167, 82, 79, 75, 67, 25, 25, 25, 26, 264, 236, 216, 188, 27, 26, 26, 26, 25, 25, 25, 28, 27, 77, 74, 70, 25, 25, 25, 13, 248, 221, 140, 26, 26, 26, 26, 25, 25, 25, 22, 19, 14, 10, 41, 36, 33, 30, 79, 75, 63, 82, 13, 25, 25, 26, 248, 202, 139, 124, 14, 15, 16, 17, 27, 19, 20, 21, 23, 28, 28, 24, 13, 27, 25, 26, 27, 26, 28, 26, 26, 27, 25, 25, 79, 13, 26, 248, 167, 26, 14, 15, 16, 18, 19, 21, 27, 22, 72, 63, 60, 24, 188, 135, 101, 25, 26, 27, 28, 28, 27, 27, 70, 62, 55, 26, 25, 25, 188, 135, 115, 26, 26, 27, 79, 67, 28, 26, 26, 235, 134, 27, 25, 25, 26, 77, 70, 62, 62, 58, 26, 25, 26, 25, 26, 236, 203, 156, 126, 104, 27, 27, 27, 26, 25, 22, 19, 14, 10, 28, 26, 28, 28, 41, 27, 25, 27, 27, 36, 33, 30, 72, 26, 26, 26, 28, 26, 77, 65, 60, 56, 26, 247, 187, 139, 140, 101, 27, 79, 70, 63, 55, 26, 25, 28, 26, 140, 126, 104, 27, 28, 27, 26, 27, 26, 63, 28, 28, 202, 125, 27, 27, 26, 26, 26, 25, 72, 63, 60, 56, 26, 26, 26, 26, 248, 167, 140, 100, 27, 27, 27, 27, 28, 28, 28, 27, 27, 27, 26, 26, 26, 28, 27, 26, 25, 79, 67, 63, 60, 24, 26, 248, 248, 135, 119, 23, 21, 20, 18, 16, 14, 12, 11, 27, 45, 43, 79, 62, 59, 25, 26, 26, 41, 265, 215, 167, 39, 37, 36, 34, 28, 33, 27, 27, 27, 32, 31, 30, 55, 26, 28, 28, 25, 27, 27, 26, 26, 26, 82, 79, 74, 67, 260, 235, 215, 135, 28, 81, 77, 74, 62, 25, 26, 26, 26, 248, 221, 188, 140, 27, 27, 27, 26, 28, 28, 25, 27, 27, 28, 26, 26, 27, 79, 75, 63, 25, 26, 26, 236, 214, 115, 27, 26, 27, 26, 28, 25, 27, 26, 28, 27, 77, 70, 58, 25, 26, 221, 167, 126, 26, 27, 74, 67, 26, 26, 28, 202, 179, 25, 27, 27, 26, 75, 69, 60, 28, 25, 26, 214, 159, 134, 27, 26, 27, 72, 186, 165, 66, 26, 28, 25, 27, 178, 159, 70, 67, 165, 145, 26, 69, 66, 12, 175, 157, 67, 64, 74, 62, 69, 66, 26, 216, 188, 167, 167, 27, 74, 70, 67, 12, 25, 28, 26, 26, 247, 220, 167, 126, 27, 13, 15, 16, 19, 26, 27, 27, 21, 23, 24, 25, 67, 26, 26, 28, 28, 203, 25, 25, 27, 27, 26, 26, 79, 75, 72, 67, 60, 247, 215, 187, 167, 102, 79, 74, 70, 67, 55, 25, 25, 26, 26, 26, 219, 203, 171, 126, 104, 27, 27, 27, 26, 26, 28, 28, 25, 25, 27, 27, 26, 26, 28, 27, 72, 68, 60, 56, 25, 25, 26, 26, 188, 165, 140, 90, 26, 27, 27, 214, 75, 26, 26, 28, 28, 25, 25, 27, 27, 26, 199, 74, 26, 26, 27, 28, 27, 70, 63, 51, 25, 25, 26, 26, 180, 154, 126, 96, 67, 27, 27, 26, 26, 26, 28, 25, 25, 27, 28, 26, 27, 72, 69, 60, 53, 25, 25, 26, 26, 194, 214, 166, 101, 27, 27, 26, 28, 28, 27, 27, 26, 26, 26, 26, 27, 27, 71, 65, 26, 26, 26, 203, 201, 140, 74, 27, 27, 27, 28, 28, 28, 28, 27, 27, 27, 27, 28, 26, 26, 27, 72, 63, 26, 26, 214, 133, 26, 27, 27, 26, 26, 28, 28, 27, 27, 26, 189, 25, 25, 72, 27, 26, 26, 28, 27, 26, 74, 71, 62, 67, 55, 25, 25, 26, 26, 26, 219, 201, 124, 166, 125, 27, 27, 27, 26, 26, 28, 28, 28, 25, 25, 27, 27, 27, 26, 26, 26, 24, 24, 23, 23, 25, 25, 25, 24, 24, 24, 22, 22, 21, 21, 23, 23, 23, 20, 20, 22, 22, 22, 19, 19, 21, 21, 21, 18, 18, 19, 19, 19, 17, 17, 18, 18, 18, 16, 16, 17, 17, 17, 14, 14, 16, 16, 16, 13, 13, 14, 14, 14, 12, 12, 13, 13, 13, 11, 11, 12, 12, 12, 45, 45, 10, 10, 10, 44, 44, 45, 45, 45, 43, 43, 44, 44, 44, 42, 42, 42, 42, 42, 40, 40, 41, 41, 41, 39, 39, 40, 40, 40, 38, 38, 39, 39, 39, 37, 37, 37, 37, 37, 36, 36, 36, 36, 36, 35, 35, 35, 35, 35, 34, 34, 34, 34, 34, 33, 33, 33, 33, 33, 32, 32, 32, 32, 32, 31, 31, 31, 31, 31, 30, 30, 30, 30, 30, 75, 72, 60, 67, 60, 276]\n",
            "[277, 48, 275, 28, 28, 28, 28, 28, 2, 6, 5, 4, 3, 46, 7, 46, 7, 46, 7, 46, 7, 46, 7, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 9, 9, 9, 9, 9, 8, 8, 8, 8, 8, 26, 26, 26, 247, 226, 202, 79, 76, 72, 248, 227, 203, 79, 76, 72, 246, 225, 201, 79, 76, 72, 248, 227, 203, 79, 76, 72, 252, 235, 155, 80, 77, 65, 26, 26, 26, 236, 216, 188, 27, 27, 27, 28, 28, 28, 27, 27, 27, 26, 26, 26, 77, 74, 70, 220, 247, 139, 75, 79, 63, 219, 246, 138, 75, 79, 63, 202, 235, 155, 72, 77, 65, 26, 26, 26, 194, 216, 165, 27, 27, 27, 28, 28, 27, 27, 26, 26, 28, 27, 71, 74, 26, 26, 12, 194, 216, 27, 27, 26, 28, 28, 27, 27, 24, 20, 16, 11, 26, 42, 37, 33, 30, 71, 74, 67, 12, 26, 26, 219, 165, 125, 13, 14, 15, 16, 27, 27, 17, 18, 28, 19, 13, 20, 21, 22, 23, 28, 28, 24, 27, 27, 25, 26, 26, 25, 24, 27, 13, 248, 23, 21, 14, 20, 28, 16, 18, 27, 18, 16, 20, 14, 22, 12, 24, 11, 26, 45, 26, 27, 43, 28, 41, 27, 39, 37, 36, 34, 33, 26, 32, 31, 30, 26, 79, 75, 67, 26, 26, 236, 214, 25, 27, 27, 28, 28, 12, 27, 26, 28, 27, 77, 60, 25, 12, 26, 248, 167, 119, 13, 14, 15, 16, 18, 19, 27, 26, 21, 26, 22, 25, 23, 24, 25, 26, 28, 25, 27, 74, 59, 25, 26, 221, 125, 27, 26, 28, 27, 26, 26, 27, 79, 75, 67, 25, 26, 26, 236, 203, 171, 27, 27, 26, 28, 28, 25, 27, 27, 24, 20, 16, 11, 28, 42, 27, 37, 33, 30, 72, 68, 25, 166, 26, 26, 28, 26, 12, 27, 77, 12, 26, 221, 167, 25, 13, 28, 15, 27, 16, 19, 27, 21, 23, 24, 25, 26, 28, 25, 27, 26, 26, 75, 67, 60, 25, 25, 26, 214, 188, 115, 25, 27, 26, 26, 25, 25, 28, 27, 70, 67, 58, 25, 25, 25, 203, 156, 104, 26, 26, 26, 201, 25, 25, 74, 26, 25, 72, 56, 25, 25, 26, 192, 216, 101, 72, 27, 26, 28, 25, 27, 26, 26, 25, 74, 65, 25, 25, 26, 201, 167, 140, 71, 27, 26, 26, 25, 25, 28, 27, 67, 63, 55, 25, 25, 25, 247, 134, 101, 26, 26, 26, 26, 25, 25, 25, 26, 189, 72, 27, 28, 27, 26, 71, 79, 62, 55, 25, 25, 25, 26, 13, 203, 226, 125, 166, 27, 26, 28, 27, 26, 27, 26, 26, 25, 27, 72, 13, 26, 273, 126, 28, 14, 27, 15, 16, 18, 19, 21, 27, 22, 24, 28, 25, 27, 26, 27, 28, 28, 27, 27, 26, 25, 26, 60, 24, 25, 203, 23, 22, 21, 20, 25, 19, 18, 26, 26, 17, 26, 16, 25, 15, 14, 13, 12, 11, 10, 45, 44, 43, 42, 84, 76, 72, 60, 266, 248, 216, 167, 41, 40, 39, 38, 37, 36, 35, 34, 82, 79, 74, 67, 265, 248, 221, 139, 33, 32, 31, 82, 79, 75, 67, 63, 253, 236, 203, 156, 80, 72, 65, 252, 203, 155, 77, 220, 28, 80, 75, 72, 65, 26, 25, 25, 26, 248, 216, 194, 167, 27, 27, 28, 26, 26, 28, 27, 25, 25, 27, 24, 22, 22, 24, 20, 19, 19, 20, 16, 14, 14, 16, 11, 10, 10, 11, 42, 41, 41, 42, 37, 36, 36, 37, 33, 33, 33, 33, 30, 30, 30, 30, 79, 74, 71, 67, 28, 28, 28, 28, 26, 26, 26, 26, 247, 226, 202, 125, 79, 76, 72, 60, 253, 236, 203, 156, 80, 77, 72, 65, 247, 247, 202, 146, 79, 79, 72, 64, 26, 26, 26, 26, 253, 236, 203, 156, 27, 27, 27, 27, 28, 28, 28, 28, 27, 27, 27, 27, 26, 26, 26, 26, 80, 77, 72, 65, 26, 26, 26, 26, 25, 248, 221, 203, 126, 27, 27, 27, 27, 28, 27, 24, 20, 16, 11, 28, 28, 28, 42, 27, 27, 27, 37, 33, 30, 60, 126, 26, 26, 26, 60, 135, 28, 79, 75, 72, 62, 25, 248, 266, 167, 140, 79, 82, 67, 26, 235, 260, 179, 25, 77, 81, 69, 63, 25, 26, 26, 26, 221, 248, 188, 140, 27, 27, 27, 26, 28, 28, 28, 25, 27, 27, 27, 26, 26, 26, 75, 79, 70, 63, 25, 216, 236, 180, 156, 74, 77, 69, 26, 202, 220, 202, 25, 72, 75, 72, 65, 25, 26, 26, 246, 216, 192, 166, 27, 27, 28, 201, 27, 71, 26, 26, 28, 25, 27, 74, 26, 216, 27, 192, 72, 178, 26, 71, 28, 189, 27, 69, 26, 79, 74, 67, 71, 25, 26, 26, 26, 227, 203, 203, 125, 27, 27, 72, 28, 28, 215, 27, 27, 24, 20, 15, 26, 10, 26, 41, 25, 37, 33, 30, 74, 76, 72, 26, 221, 248, 140, 27, 79, 63, 28, 28, 26, 235, 134, 27, 26, 75, 77, 62, 60, 26, 26, 26, 248, 221, 221, 203, 126, 27, 27, 27, 79, 72, 28, 28, 28, 260, 203, 27, 27, 27, 26, 26, 26, 81, 75, 75, 72, 60, 25, 26, 26, 266, 236, 216, 156, 115, 27, 27, 74, 58, 26, 28, 28, 202, 109, 25, 27, 27, 26, 26, 82, 77, 72, 65, 57, 26, 26, 26, 266, 246, 214, 188, 100, 27, 27, 27, 82, 70, 272, 167, 28, 28, 28, 27, 27, 27, 84, 67, 25, 26, 269, 167, 27, 26, 26, 26, 26, 28, 25, 27, 26, 83, 79, 74, 67, 55, 26, 25, 26, 26, 26, 271, 227, 203, 167, 124, 27, 27, 27, 27, 26, 28, 28, 25, 27, 27, 26, 26, 28, 28, 27, 27, 26, 26, 25, 25, 76, 72, 67, 24, 24, 25, 26, 26, 248, 221, 203, 23, 23, 21, 21, 20, 20, 18, 18, 16, 16, 14, 14, 12, 12, 11, 11, 27, 27, 45, 45, 43, 43, 41, 41, 39, 39, 37, 37, 36, 36, 34, 34, 26, 28, 33, 33, 28, 25, 27, 32, 32, 27, 31, 31, 30, 30, 84, 60, 26, 26, 75, 72, 215, 188, 28, 28, 79, 74, 70, 26, 236, 203, 180, 27, 72, 69, 25, 26, 28, 215, 188, 27, 27, 26, 77, 26, 28, 221, 25, 27, 26, 75, 74, 70, 26, 235, 202, 171, 27, 72, 68, 25, 26, 28, 12, 194, 167, 27, 26, 27, 26, 77, 220, 27, 28, 27, 75, 12, 26, 13, 216, 101, 28, 13, 27, 14, 15, 16, 27, 17, 18, 26, 20, 21, 28, 22, 27, 23, 24, 25, 26, 26, 25, 26, 74, 71, 26, 26, 13, 248, 266, 216, 14, 15, 27, 16, 25, 18, 19, 27, 21, 22, 28, 24, 27, 25, 26, 27, 28, 28, 26, 27, 27, 74, 67, 55, 25, 25, 26, 201, 140, 104, 26, 26, 27, 26, 26, 25, 25, 28, 27, 79, 82, 63, 56, 25, 25, 26, 26, 236, 253, 156, 104, 27, 27, 26, 26, 25, 26, 28, 28, 25, 27, 27, 72, 56, 25, 26, 188, 114, 27, 26, 26, 28, 27, 26, 26, 27, 77, 80, 70, 65, 25, 26, 26, 26, 221, 248, 186, 167, 27, 27, 27, 26, 28, 28, 28, 25, 27, 27, 27, 28, 26, 26, 27, 75, 79, 67, 25, 26, 26, 214, 234, 187, 26, 27, 27, 26, 169, 70, 26, 28, 28, 165, 25, 27, 27, 68, 26, 152, 25, 67, 27, 26, 26, 28, 27, 26, 74, 77, 65, 70, 58, 25, 25, 26, 26, 26, 221, 246, 138, 187, 90, 27, 27, 27, 26, 26, 28, 25, 27, 22, 24, 19, 20, 14, 15, 27, 45, 10, 28, 28, 41, 41, 27, 27, 36, 37, 33, 33, 26, 26, 30, 30, 25, 25, 75, 51, 24, 24, 23, 23, 21, 21, 20, 20, 28, 18, 18, 27, 16, 16, 28, 28, 26, 26, 14, 14, 12, 12, 11, 11, 26, 45, 45, 25, 43, 43, 41, 41, 24, 247, 139, 39, 39, 23, 37, 37, 22, 36, 36, 21, 34, 34, 20, 33, 33, 19, 32, 32, 18, 31, 31, 17, 30, 30, 15, 14, 79, 63, 79, 63, 13, 236, 135, 12, 10, 45, 44, 43, 28, 26, 41, 40, 39, 38, 37, 77, 62, 36, 247, 166, 139, 35, 34, 33, 32, 31, 28, 67, 70, 180, 79, 69, 63, 234, 186, 133, 77, 70, 62, 28, 220, 201, 125, 75, 60, 26, 26, 216, 167, 192, 27, 27, 72, 178, 71, 28, 28, 26, 189, 27, 27, 69, 24, 24, 20, 20, 15, 15, 10, 10, 41, 41, 37, 37, 33, 33, 30, 30, 74, 71, 67, 26, 203, 28, 27, 26, 28, 28, 26, 220, 247, 27, 24, 20, 16, 11, 42, 37, 33, 30, 72, 75, 79, 216, 236, 74, 77, 220, 247, 166, 28, 67, 13, 180, 75, 79, 69, 214, 234, 186, 74, 77, 70, 26, 202, 220, 201, 72, 75, 13, 26, 26, 248, 216, 167, 14, 16, 18, 20, 192, 22, 27, 27, 72, 24, 26, 178, 27, 71, 28, 28, 28, 189, 166, 27, 27, 27, 69, 24, 24, 20, 20, 15, 15, 26, 10, 10, 41, 41, 37, 37, 33, 33, 30, 30, 79, 74, 67, 71, 67, 180, 203, 156, 69, 186, 70, 72, 65, 28, 28, 201, 155, 170, 26, 65, 68, 26, 13, 140, 167, 192, 27, 72, 178, 71, 63, 28, 215, 189, 166, 27, 69, 26, 74, 71, 67, 67, 26, 26, 26, 13, 248, 203, 203, 167, 126, 14, 15, 27, 27, 27, 16, 18, 19, 21, 22, 72, 28, 28, 28, 24, 220, 27, 27, 27, 25, 24, 24, 26, 20, 20, 27, 16, 16, 11, 11, 28, 42, 26, 42, 27, 37, 37, 33, 33, 30, 30, 75, 72, 67, 60, 26, 216, 167, 28, 28, 26, 13, 27, 26, 74, 28, 13, 215, 215, 101, 27, 24, 14, 20, 16, 15, 18, 10, 20, 41, 22, 37, 24, 33, 26, 30, 27, 79, 74, 67, 55, 28, 261, 236, 96, 27, 28, 81, 26, 25, 264, 82, 77, 74, 53, 26, 271, 220, 202, 126, 103, 27, 75, 72, 60, 56, 28, 26, 26, 248, 221, 167, 100, 27, 26, 27, 27, 79, 67, 28, 26, 268, 246, 167, 27, 84, 26, 28, 259, 27, 83, 27, 75, 67, 25, 26, 26, 267, 216, 167, 81, 27, 27, 28, 27, 26, 26, 28, 28, 25, 27, 27, 26, 26, 26, 83, 79, 74, 67, 55, 26, 25, 26, 26, 26, 271, 227, 203, 167, 124, 27, 27, 27, 27, 26, 28, 28, 25, 27, 27, 26, 26, 28, 28, 27, 27, 26, 26, 25, 25, 76, 72, 67, 24, 24, 25, 26, 26, 248, 221, 203, 23, 23, 21, 21, 20, 20, 18, 18, 16, 16, 14, 14, 12, 12, 11, 11, 27, 27, 45, 45, 43, 43, 41, 41, 39, 39, 37, 37, 36, 36, 34, 34, 26, 28, 33, 33, 28, 25, 27, 32, 32, 27, 31, 31, 30, 30, 84, 60, 26, 26, 75, 72, 215, 188, 28, 28, 79, 74, 70, 26, 236, 203, 180, 27, 72, 69, 25, 26, 28, 215, 188, 27, 27, 26, 77, 26, 28, 221, 25, 27, 26, 75, 74, 70, 26, 235, 202, 171, 27, 72, 68, 25, 26, 28, 12, 194, 167, 27, 26, 27, 26, 77, 220, 27, 28, 27, 75, 12, 26, 13, 216, 101, 28, 13, 27, 14, 15, 16, 27, 17, 18, 26, 20, 21, 28, 22, 27, 23, 24, 25, 26, 26, 25, 26, 74, 71, 26, 26, 13, 248, 266, 216, 14, 15, 27, 16, 25, 18, 19, 27, 21, 22, 28, 24, 27, 25, 26, 27, 28, 28, 26, 27, 27, 74, 67, 55, 25, 25, 26, 201, 140, 104, 26, 26, 27, 26, 26, 25, 25, 28, 27, 79, 82, 63, 56, 25, 25, 26, 26, 236, 253, 156, 104, 27, 27, 26, 26, 25, 26, 28, 28, 25, 27, 27, 72, 56, 25, 26, 188, 114, 27, 26, 26, 28, 27, 26, 26, 27, 77, 80, 70, 65, 25, 26, 26, 26, 221, 248, 186, 167, 27, 27, 27, 26, 28, 28, 28, 25, 27, 27, 27, 28, 26, 26, 27, 75, 79, 67, 25, 26, 26, 214, 234, 187, 26, 27, 27, 26, 169, 70, 26, 28, 28, 165, 25, 27, 27, 68, 26, 152, 25, 67, 27, 26, 26, 28, 27, 26, 74, 77, 65, 70, 58, 25, 25, 26, 26, 26, 221, 246, 138, 187, 90, 27, 27, 27, 26, 26, 28, 25, 27, 22, 24, 19, 20, 14, 15, 27, 45, 10, 28, 28, 41, 41, 27, 27, 36, 37, 33, 33, 26, 26, 30, 30, 25, 25, 75, 51, 24, 24, 23, 23, 21, 21, 20, 20, 28, 18, 18, 27, 16, 16, 28, 28, 26, 26, 14, 14, 12, 12, 11, 11, 26, 45, 45, 25, 43, 43, 41, 41, 24, 247, 139, 39, 39, 23, 37, 37, 22, 36, 36, 21, 34, 34, 20, 33, 33, 19, 32, 32, 18, 31, 31, 17, 30, 30, 15, 14, 79, 63, 79, 63, 13, 236, 135, 12, 10, 45, 44, 43, 28, 26, 41, 40, 39, 38, 37, 77, 62, 36, 247, 166, 139, 35, 34, 33, 32, 31, 28, 67, 70, 180, 79, 69, 63, 234, 186, 133, 77, 70, 62, 28, 220, 201, 125, 75, 60, 26, 26, 216, 167, 192, 27, 27, 72, 178, 71, 28, 28, 26, 189, 27, 27, 69, 24, 24, 20, 20, 15, 15, 10, 10, 41, 41, 37, 37, 33, 33, 30, 30, 74, 71, 67, 26, 203, 28, 27, 26, 28, 28, 26, 220, 247, 27, 24, 20, 16, 11, 42, 37, 33, 30, 72, 75, 79, 216, 236, 74, 77, 220, 247, 166, 28, 67, 13, 180, 75, 79, 69, 214, 234, 186, 74, 77, 70, 26, 202, 220, 201, 72, 75, 13, 26, 26, 248, 216, 167, 14, 16, 18, 20, 192, 22, 27, 27, 72, 24, 26, 178, 27, 71, 28, 28, 28, 189, 166, 27, 27, 27, 69, 24, 24, 20, 20, 15, 15, 26, 10, 10, 41, 41, 37, 37, 33, 33, 30, 30, 79, 74, 67, 71, 67, 180, 203, 156, 69, 186, 70, 72, 65, 28, 28, 201, 155, 170, 26, 65, 68, 26, 13, 140, 167, 192, 27, 72, 178, 71, 63, 28, 215, 189, 166, 27, 69, 26, 74, 71, 67, 67, 26, 26, 26, 13, 248, 203, 203, 167, 126, 14, 15, 27, 27, 27, 16, 18, 19, 21, 22, 72, 28, 28, 28, 24, 220, 27, 27, 27, 25, 24, 24, 26, 20, 20, 27, 16, 16, 11, 11, 28, 42, 26, 42, 27, 37, 37, 33, 33, 30, 30, 75, 72, 67, 60, 26, 216, 167, 28, 28, 26, 13, 27, 26, 74, 28, 13, 215, 215, 101, 27, 24, 14, 20, 16, 15, 18, 10, 20, 41, 22, 37, 24, 33, 26, 30, 27, 79, 74, 67, 55, 28, 261, 236, 96, 27, 28, 81, 26, 26, 264, 82, 77, 74, 53, 26, 271, 220, 202, 125, 103, 27, 75, 72, 60, 56, 28, 26, 26, 248, 221, 167, 100, 27, 26, 27, 27, 79, 67, 28, 26, 268, 246, 166, 27, 84, 26, 28, 259, 27, 83, 27, 75, 67, 26, 26, 26, 267, 216, 167, 81, 27, 27, 27, 28, 27, 26, 28, 28, 28, 27, 27, 27, 26, 26, 26, 26, 83, 79, 74, 67, 55, 26, 26, 26, 26, 26, 271, 225, 201, 165, 124, 27, 27, 27, 27, 27, 28, 28, 28, 28, 28, 27, 27, 27, 27, 27, 26, 26, 26, 26, 26, 25, 25, 25, 25, 25, 24, 24, 24, 24, 24, 23, 23, 23, 23, 23, 21, 21, 21, 21, 21, 20, 20, 20, 20, 20, 18, 18, 18, 18, 18, 16, 16, 16, 16, 16, 14, 14, 14, 14, 14, 12, 12, 12, 12, 12, 11, 11, 11, 11, 11, 45, 45, 45, 45, 45, 43, 43, 43, 43, 43, 41, 41, 41, 41, 41, 39, 39, 39, 39, 39, 37, 37, 37, 37, 37, 36, 36, 36, 36, 36, 34, 34, 34, 34, 34, 33, 33, 33, 33, 33, 32, 32, 32, 32, 32, 31, 31, 31, 31, 31, 30, 30, 30, 30, 30, 84, 76, 72, 67, 60, 276]\n",
            "[277, 47, 275, 28, 28, 28, 28, 28, 2, 6, 5, 4, 3, 46, 7, 46, 7, 46, 7, 46, 7, 46, 7, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 9, 9, 9, 9, 9, 8, 8, 8, 8, 8, 26, 26, 26, 236, 216, 210, 77, 74, 74, 248, 227, 209, 79, 76, 74, 259, 234, 214, 81, 77, 74, 259, 234, 214, 81, 77, 74, 259, 234, 214, 81, 77, 74, 261, 236, 216, 81, 77, 74, 259, 234, 214, 81, 77, 74, 234, 214, 214, 77, 74, 74, 246, 225, 201, 259, 234, 178, 79, 76, 72, 234, 214, 186, 81, 77, 69, 246, 225, 165, 77, 74, 70, 225, 201, 178, 79, 76, 67, 234, 214, 154, 76, 72, 69, 214, 192, 165, 77, 74, 65, 225, 201, 145, 74, 71, 67, 201, 178, 154, 76, 72, 64, 214, 192, 72, 69, 225, 201, 145, 74, 71, 65, 232, 212, 131, 76, 72, 64, 77, 74, 62, 248, 227, 126, 79, 76, 60, 259, 234, 154, 81, 77, 65, 26, 246, 225, 124, 234, 214, 79, 76, 27, 225, 201, 77, 74, 28, 232, 212, 27, 76, 72, 77, 74, 248, 227, 26, 79, 76, 60, 246, 225, 201, 79, 76, 72, 259, 234, 178, 81, 77, 69, 268, 246, 165, 83, 79, 67, 26, 271, 259, 154, 268, 246, 84, 81, 27, 259, 234, 83, 79, 28, 246, 225, 27, 81, 77, 259, 234, 79, 76, 26, 65, 246, 225, 165, 81, 77, 234, 214, 79, 76, 67, 26, 223, 204, 178, 77, 74, 27, 76, 73, 26, 214, 214, 28, 27, 74, 74, 26, 26, 225, 205, 27, 27, 26, 28, 28, 178, 27, 27, 26, 26, 76, 73, 69, 69, 26, 26, 216, 216, 135, 178, 27, 27, 165, 69, 62, 28, 28, 178, 154, 27, 27, 67, 145, 26, 26, 65, 69, 74, 74, 26, 26, 259, 234, 178, 133, 64, 27, 27, 165, 145, 69, 62, 28, 28, 154, 154, 27, 27, 67, 64, 26, 26, 145, 165, 65, 65, 81, 77, 26, 26, 261, 236, 133, 178, 64, 67, 27, 27, 145, 165, 62, 69, 28, 28, 154, 178, 27, 27, 64, 67, 165, 192, 26, 26, 65, 69, 81, 77, 26, 26, 259, 234, 178, 199, 67, 71, 27, 27, 165, 69, 72, 28, 28, 178, 201, 27, 27, 67, 26, 26, 192, 69, 81, 77, 72, 26, 26, 248, 227, 199, 126, 71, 27, 27, 72, 60, 28, 28, 201, 124, 27, 27, 26, 26, 79, 76, 72, 60, 26, 26, 246, 225, 201, 124, 27, 27, 133, 60, 72, 28, 28, 201, 145, 27, 27, 62, 26, 26, 154, 64, 79, 76, 72, 26, 26, 248, 214, 194, 165, 65, 27, 27, 28, 28, 154, 27, 27, 67, 225, 145, 26, 26, 74, 65, 79, 71, 26, 26, 26, 259, 232, 214, 131, 76, 64, 27, 27, 27, 62, 28, 28, 28, 133, 27, 27, 27, 26, 26, 26, 81, 77, 74, 62, 26, 234, 216, 180, 133, 27, 246, 145, 77, 62, 69, 28, 259, 178, 154, 27, 79, 64, 246, 145, 26, 81, 65, 74, 69, 26, 26, 234, 214, 178, 133, 79, 64, 27, 27, 28, 28, 225, 124, 27, 27, 77, 62, 26, 26, 214, 118, 76, 60, 74, 69, 26, 223, 207, 180, 106, 74, 59, 27, 76, 73, 57, 28, 26, 214, 214, 97, 27, 24, 20, 16, 11, 42, 27, 37, 33, 31, 30, 74, 69, 54, 26, 28, 26, 225, 108, 27, 28, 27, 26, 27, 26, 28, 28, 204, 178, 27, 27, 74, 26, 26, 76, 57, 73, 69, 26, 26, 26, 214, 214, 180, 133, 27, 27, 27, 69, 25, 178, 28, 28, 28, 27, 27, 27, 26, 26, 26, 69, 178, 69, 74, 74, 62, 214, 234, 178, 133, 134, 74, 77, 69, 62, 62, 26, 216, 236, 180, 156, 135, 27, 74, 77, 65, 62, 28, 214, 234, 154, 134, 27, 26, 69, 26, 178, 74, 77, 65, 62, 27, 214, 234, 154, 134, 74, 77, 65, 62, 28, 214, 234, 154, 134, 27, 24, 20, 15, 10, 41, 37, 33, 30, 74, 77, 69, 65, 62, 216, 236, 156, 135, 28, 13, 74, 77, 65, 62, 13, 214, 234, 178, 154, 134, 14, 16, 18, 20, 22, 24, 26, 27, 74, 77, 65, 62, 28, 214, 234, 133, 134, 27, 26, 74, 77, 69, 62, 62, 225, 246, 201, 165, 125, 76, 79, 72, 67, 60, 25, 26, 26, 234, 259, 201, 156, 155, 27, 225, 246, 77, 81, 27, 28, 214, 234, 27, 76, 79, 26, 28, 225, 246, 26, 25, 27, 74, 77, 65, 234, 259, 154, 76, 79, 225, 246, 77, 81, 26, 65, 234, 234, 178, 76, 79, 246, 225, 77, 77, 72, 69, 65, 25, 26, 26, 26, 256, 232, 201, 180, 155, 79, 76, 27, 27, 27, 69, 28, 28, 178, 27, 27, 26, 28, 26, 26, 25, 27, 81, 77, 259, 234, 26, 69, 178, 81, 77, 72, 69, 65, 259, 234, 214, 178, 134, 81, 77, 74, 69, 62, 26, 261, 236, 216, 180, 135, 27, 81, 77, 74, 62, 28, 259, 234, 214, 134, 27, 26, 69, 26, 178, 81, 77, 74, 62, 27, 259, 234, 214, 134, 81, 77, 74, 62, 28, 259, 234, 214, 134, 27, 24, 20, 15, 10, 41, 37, 33, 30, 81, 77, 74, 69, 62, 261, 236, 216, 135, 28, 13, 81, 77, 74, 62, 13, 259, 234, 214, 154, 134, 14, 16, 18, 20, 22, 24, 26, 27, 81, 77, 74, 62, 28, 259, 234, 214, 134, 27, 26, 81, 77, 74, 65, 62, 271, 246, 225, 201, 125, 84, 79, 76, 72, 60, 25, 26, 26, 271, 259, 234, 201, 155, 268, 246, 84, 81, 27, 27, 259, 234, 83, 79, 26, 28, 28, 268, 246, 27, 27, 81, 77, 27, 271, 259, 83, 79, 268, 246, 84, 81, 26, 26, 28, 259, 234, 27, 83, 79, 246, 225, 81, 77, 77, 72, 26, 26, 259, 234, 203, 178, 79, 76, 27, 246, 225, 81, 77, 27, 28, 26, 234, 214, 27, 79, 76, 24, 20, 16, 11, 28, 246, 225, 42, 27, 77, 74, 37, 33, 31, 30, 72, 259, 234, 79, 76, 25, 246, 225, 81, 77, 26, 28, 13, 234, 214, 79, 76, 225, 201, 77, 74, 69, 65, 25, 26, 13, 234, 214, 186, 154, 115, 76, 72, 14, 15, 16, 17, 225, 225, 19, 77, 74, 20, 27, 21, 23, 26, 24, 214, 234, 25, 25, 76, 76, 26, 27, 28, 28, 223, 223, 27, 27, 74, 77, 76, 76, 58, 25, 234, 214, 114, 26, 26, 77, 70, 65, 26, 26, 26, 264, 214, 165, 25, 74, 27, 27, 214, 74, 74, 58, 25, 26, 28, 28, 227, 201, 125, 27, 27, 26, 26, 27, 82, 76, 67, 26, 259, 234, 154, 26, 28, 27, 25, 27, 81, 65, 26, 28, 26, 246, 165, 27, 27, 26, 27, 26, 28, 28, 223, 27, 27, 77, 26, 26, 79, 76, 72, 67, 60, 25, 26, 236, 236, 201, 180, 95, 27, 77, 77, 69, 259, 234, 154, 26, 28, 25, 27, 24, 26, 23, 25, 81, 77, 65, 22, 24, 26, 259, 234, 154, 21, 23, 20, 21, 18, 20, 17, 18, 15, 16, 27, 246, 225, 13, 14, 81, 77, 11, 12, 45, 10, 44, 44, 42, 43, 28, 40, 41, 234, 214, 27, 38, 39, 79, 76, 24, 37, 37, 20, 35, 35, 15, 34, 34, 10, 33, 33, 246, 225, 41, 31, 32, 77, 74, 37, 31, 33, 30, 30, 30, 72, 65, 53, 256, 232, 79, 76, 28, 28, 28, 26, 26, 26, 81, 77, 271, 259, 201, 178, 154, 84, 81, 72, 69, 65, 26, 26, 26, 271, 259, 201, 178, 154, 27, 27, 27, 268, 246, 84, 81, 28, 28, 28, 259, 234, 27, 27, 27, 83, 79, 26, 26, 26, 268, 246, 81, 77, 72, 69, 65, 26, 26, 26, 270, 256, 203, 180, 156, 83, 79, 27, 27, 27, 84, 81, 28, 28, 28, 271, 259, 27, 27, 27, 26, 26, 26, 72, 69, 65, 234, 214, 133, 84, 81, 268, 259, 83, 81, 77, 74, 62, 268, 251, 225, 192, 145, 83, 80, 76, 71, 64, 26, 26, 26, 26, 261, 261, 225, 201, 178, 27, 27, 27, 27, 81, 28, 225, 27, 28, 28, 28, 214, 26, 27, 27, 27, 76, 26, 25, 81, 24, 201, 225, 23, 74, 21, 20, 18, 16, 214, 214, 14, 72, 76, 12, 26, 10, 26, 44, 43, 41, 225, 201, 39, 74, 74, 37, 35, 34, 33, 234, 214, 32, 76, 72, 31, 30, 76, 72, 69, 26, 26, 244, 225, 192, 165, 77, 74, 232, 76, 27, 27, 77, 28, 79, 13, 214, 246, 28, 28, 201, 27, 27, 74, 79, 13, 192, 214, 214, 72, 14, 15, 16, 201, 201, 18, 71, 74, 19, 26, 26, 21, 22, 24, 214, 192, 25, 72, 72, 26, 27, 28, 28, 225, 201, 27, 74, 71, 71, 67, 26, 26, 234, 214, 180, 156, 76, 72, 27, 27, 246, 225, 77, 74, 26, 28, 28, 259, 234, 27, 27, 79, 76, 268, 246, 26, 26, 81, 77, 69, 74, 65, 26, 26, 26, 271, 259, 178, 201, 154, 83, 79, 27, 27, 27, 268, 246, 84, 81, 28, 28, 28, 271, 259, 27, 27, 27, 83, 79, 24, 20, 15, 26, 10, 26, 259, 234, 41, 84, 81, 37, 33, 30, 69, 72, 65, 26, 26, 267, 249, 225, 145, 81, 77, 28, 26, 27, 27, 83, 80, 271, 259, 178, 28, 28, 27, 27, 84, 81, 69, 26, 26, 268, 251, 192, 27, 27, 26, 26, 28, 28, 143, 27, 27, 71, 26, 26, 83, 80, 76, 64, 64, 26, 26, 26, 26, 26, 261, 261, 207, 147, 110, 27, 27, 27, 27, 27, 28, 28, 28, 28, 28, 27, 27, 27, 27, 27, 24, 24, 20, 20, 16, 16, 11, 11, 26, 26, 26, 42, 42, 37, 37, 33, 33, 31, 31, 30, 30, 81, 81, 73, 64, 57, 178, 154, 133, 69, 65, 62, 28, 28, 172, 148, 128, 69, 65, 62, 172, 148, 128, 69, 65, 62, 180, 156, 135, 69, 65, 62, 178, 154, 133, 69, 65, 62, 178, 154, 133, 69, 65, 62, 178, 154, 133, 69, 65, 62, 154, 133, 133, 65, 62, 62, 165, 145, 124, 178, 154, 108, 67, 64, 60, 154, 133, 113, 69, 65, 57, 165, 145, 100, 65, 62, 58, 145, 124, 108, 67, 64, 55, 154, 133, 94, 64, 60, 57, 133, 118, 100, 65, 62, 53, 145, 124, 91, 62, 59, 55, 124, 108, 94, 64, 60, 52, 133, 118, 60, 57, 145, 124, 91, 62, 59, 53, 152, 131, 88, 64, 60, 52, 65, 62, 50, 165, 145, 86, 67, 64, 49, 154, 133, 89, 65, 62, 50, 26, 145, 127, 108, 133, 118, 64, 61, 27, 127, 108, 62, 59, 28, 131, 116, 27, 61, 57, 62, 59, 145, 127, 26, 64, 61, 57, 154, 133, 133, 65, 62, 62, 167, 147, 126, 67, 64, 60, 178, 154, 94, 69, 65, 53, 165, 145, 124, 67, 64, 60, 154, 133, 133, 65, 62, 62, 167, 147, 126, 67, 64, 60, 165, 145, 124, 67, 64, 60, 178, 154, 148, 69, 65, 65, 186, 165, 148, 70, 67, 65, 201, 178, 154, 72, 69, 65, 188, 156, 115, 70, 65, 58, 178, 154, 113, 69, 65, 58, 26, 26, 26, 165, 145, 124, 27, 27, 27, 28, 28, 28, 27, 27, 27, 26, 26, 26, 67, 64, 60, 180, 156, 96, 69, 65, 53, 162, 141, 120, 67, 64, 60, 162, 141, 120, 67, 64, 60, 162, 141, 120, 64, 60, 26, 26, 26, 178, 145, 124, 67, 27, 27, 27, 28, 28, 28, 27, 27, 27, 26, 26, 26, 69, 64, 60, 154, 133, 133, 65, 62, 62, 26, 145, 124, 108, 133, 133, 64, 60, 27, 124, 145, 62, 62, 28, 131, 131, 27, 60, 64, 62, 62, 147, 126, 26, 64, 60, 57, 154, 133, 133, 65, 62, 62, 26, 165, 145, 124, 118, 60, 27, 124, 59, 28, 28, 159, 133, 27, 67, 60, 165, 145, 66, 62, 178, 154, 67, 64, 26, 192, 165, 69, 65, 165, 154, 71, 67, 145, 65, 26, 178, 154, 133, 67, 64, 64, 124, 62, 27, 133, 60, 28, 28, 165, 145, 27, 69, 62, 178, 154, 67, 64, 192, 165, 69, 65, 26, 201, 178, 71, 67, 178, 165, 72, 69, 154, 67, 192, 169, 145, 69, 65, 65, 201, 71, 26, 214, 178, 133, 72, 68, 64, 27, 201, 74, 26, 26, 28, 189, 143, 27, 72, 62, 27, 27, 26, 28, 28, 26, 168, 27, 27, 69, 26, 26, 71, 68, 64, 26, 26, 26, 205, 178, 108, 27, 27, 27, 28, 26, 225, 27, 24, 20, 15, 28, 28, 10, 28, 28, 41, 27, 27, 37, 33, 26, 26, 30, 25, 25, 76, 73, 24, 24, 26, 261, 23, 23, 21, 21, 20, 20, 18, 18, 16, 16, 28, 27, 26, 14, 14, 12, 12, 11, 11, 45, 45, 43, 43, 28, 41, 41, 225, 178, 27, 39, 39, 37, 37, 36, 36, 34, 34, 33, 33, 26, 32, 32, 31, 31, 30, 30, 81, 76, 69, 69, 57, 29, 29, 26, 26, 259, 234, 214, 27, 27, 81, 28, 28, 28, 28, 214, 27, 27, 26, 26, 77, 74, 74, 26, 26, 261, 236, 216, 27, 27, 81, 28, 28, 268, 27, 27, 26, 26, 83, 77, 74, 26, 271, 259, 201, 27, 268, 246, 84, 81, 259, 234, 83, 79, 28, 246, 225, 27, 81, 77, 26, 259, 234, 79, 76, 72, 26, 264, 246, 188, 81, 77, 27, 259, 234, 82, 79, 246, 225, 81, 77, 28, 234, 214, 27, 79, 76, 246, 225, 26, 77, 74, 70, 26, 259, 234, 178, 79, 76, 27, 246, 225, 81, 77, 234, 214, 79, 76, 28, 225, 201, 27, 77, 74, 26, 234, 214, 76, 72, 69, 244, 223, 167, 77, 74, 79, 76, 67, 259, 234, 165, 81, 77, 268, 246, 83, 79, 67, 26, 271, 259, 154, 27, 84, 81, 28, 259, 234, 27, 26, 81, 77, 65, 26, 248, 227, 203, 27, 79, 76, 28, 246, 225, 27, 26, 72, 201, 79, 76, 246, 225, 79, 76, 72, 234, 234, 201, 77, 77, 72, 26, 26, 26, 248, 227, 203, 27, 27, 27, 28, 28, 28, 27, 27, 27, 26, 26, 26, 79, 76, 72, 26, 26, 26, 259, 234, 234, 27, 27, 27, 28, 28, 28, 27, 27, 27, 26, 26, 26, 81, 77, 77, 259, 234, 214, 234, 214, 81, 77, 246, 225, 201, 77, 74, 74, 225, 201, 79, 76, 234, 214, 186, 76, 72, 72, 214, 186, 77, 74, 225, 201, 178, 74, 70, 70, 201, 178, 76, 72, 214, 192, 165, 72, 69, 69, 201, 178, 74, 71, 214, 192, 72, 69, 225, 201, 154, 67, 74, 71, 145, 65, 232, 212, 131, 76, 72, 64, 77, 74, 62, 228, 208, 133, 77, 74, 62, 228, 208, 145, 77, 74, 64, 236, 216, 156, 77, 74, 65, 234, 214, 165, 77, 74, 67, 225, 205, 178, 76, 73, 69, 225, 205, 178, 76, 73, 69, 236, 216, 210, 77, 74, 74, 248, 227, 209, 79, 76, 74, 259, 234, 214, 81, 77, 74, 259, 234, 214, 81, 77, 74, 234, 214, 214, 77, 74, 74, 26, 246, 225, 201, 27, 28, 234, 214, 27, 79, 76, 225, 201, 77, 74, 234, 214, 76, 72, 26, 246, 225, 77, 74, 225, 201, 79, 76, 72, 26, 234, 214, 186, 76, 72, 27, 28, 225, 201, 27, 77, 74, 214, 186, 76, 72, 225, 201, 74, 70, 26, 234, 214, 76, 72, 214, 186, 77, 74, 70, 26, 225, 201, 178, 74, 70, 27, 28, 214, 186, 27, 76, 72, 201, 178, 74, 70, 214, 186, 72, 69, 26, 225, 201, 74, 70, 201, 178, 76, 72, 69, 26, 26, 212, 185, 165, 72, 69, 27, 27, 74, 28, 165, 27, 28, 178, 26, 27, 67, 70, 186, 165, 69, 201, 178, 70, 67, 26, 214, 186, 72, 69, 225, 201, 74, 70, 67, 26, 232, 212, 154, 76, 72, 27, 77, 74, 26, 26, 259, 234, 28, 27, 27, 27, 28, 28, 27, 27, 26, 26, 26, 246, 225, 165, 81, 77, 65, 234, 214, 79, 76, 26, 223, 204, 175, 77, 74, 67, 27, 76, 73, 26, 214, 214, 28, 27, 74, 74, 26, 26, 225, 205, 27, 27, 26, 28, 28, 26, 108, 27, 27, 26, 26, 76, 73, 69, 57, 26, 26, 26, 26, 216, 214, 135, 180, 27, 27, 27, 27, 28, 28, 28, 89, 27, 27, 27, 24, 24, 20, 20, 16, 16, 11, 11, 28, 26, 42, 42, 27, 37, 37, 33, 33, 26, 31, 31, 25, 30, 30, 50, 74, 62, 69, 24, 26, 26, 178, 133, 23, 21, 20, 18, 16, 28, 28, 27, 26, 26, 14, 12, 10, 27, 44, 43, 28, 41, 234, 133, 27, 39, 24, 37, 20, 35, 15, 34, 28, 10, 28, 33, 41, 27, 32, 37, 31, 33, 30, 30, 77, 62, 69, 74, 26, 26, 259, 156, 28, 28, 27, 26, 27, 26, 28, 213, 133, 27, 28, 26, 27, 74, 65, 62, 62, 26, 26, 233, 154, 214, 133, 26, 27, 27, 81, 77, 74, 28, 28, 268, 245, 214, 27, 27, 24, 24, 20, 20, 15, 15, 10, 10, 41, 41, 37, 37, 33, 33, 30, 30, 83, 79, 65, 74, 62, 26, 271, 258, 203, 27, 268, 245, 84, 81, 259, 233, 83, 79, 28, 246, 224, 27, 81, 77, 259, 233, 26, 79, 76, 28, 28, 72, 26, 264, 245, 186, 81, 77, 27, 259, 233, 82, 79, 246, 224, 81, 77, 28, 234, 213, 27, 79, 76, 26, 246, 224, 77, 74, 70, 26, 256, 231, 233, 180, 132, 79, 76, 27, 224, 123, 77, 62, 213, 117, 76, 60, 81, 77, 28, 259, 233, 200, 107, 27, 74, 59, 213, 117, 26, 72, 57, 81, 77, 69, 224, 165, 123, 74, 59, 213, 112, 76, 60, 200, 107, 74, 58, 67, 28, 191, 165, 99, 72, 57, 200, 107, 71, 55, 67, 26, 213, 156, 112, 72, 57, 27, 200, 107, 74, 58, 191, 99, 72, 57, 28, 177, 93, 27, 71, 55, 24, 20, 16, 11, 191, 99, 42, 69, 53, 37, 33, 31, 30, 65, 258, 233, 198, 105, 71, 55, 245, 224, 81, 77, 233, 213, 79, 76, 72, 57, 28, 224, 200, 200, 107, 77, 74, 233, 213, 76, 72, 72, 57, 245, 224, 26, 77, 74, 233, 213, 79, 76, 224, 200, 77, 74, 213, 191, 76, 72, 224, 200, 74, 71, 13, 26, 231, 211, 186, 76, 72, 178, 70, 165, 69, 77, 74, 233, 213, 154, 67, 165, 65, 77, 74, 13, 201, 178, 126, 67, 14, 16, 18, 20, 192, 165, 22, 72, 69, 24, 26, 178, 154, 27, 71, 67, 28, 165, 145, 27, 69, 65, 178, 154, 26, 67, 64, 60, 26, 186, 165, 113, 69, 65, 27, 178, 154, 70, 67, 165, 145, 69, 65, 28, 154, 133, 27, 67, 64, 26, 165, 145, 65, 62, 58, 26, 178, 154, 110, 67, 64, 27, 165, 145, 69, 65, 154, 133, 67, 64, 28, 145, 124, 27, 65, 62, 154, 133, 26, 64, 60, 57, 164, 143, 100, 65, 62, 67, 64, 55, 178, 154, 100, 69, 65, 192, 165, 71, 67, 55, 26, 203, 180, 96, 27, 72, 69, 28, 178, 154, 27, 26, 69, 65, 53, 26, 165, 145, 124, 27, 67, 64, 28, 165, 145, 27, 26, 60, 126, 67, 64, 163, 142, 60, 154, 154, 133, 67, 64, 65, 65, 62, 26, 26, 26, 165, 145, 124, 27, 27, 27, 28, 28, 28, 27, 27, 27, 26, 26, 26, 67, 64, 60, 26, 26, 26, 180, 156, 96, 26, 26, 27, 27, 27, 28, 28, 28, 27, 27, 27, 24, 24, 20, 20, 16, 16, 11, 11, 42, 26, 42, 37, 37, 33, 33, 31, 31, 30, 30, 69, 65, 53, 259, 271, 178, 234, 259, 81, 84, 28, 28, 26, 26, 246, 264, 165, 77, 81, 69, 225, 246, 79, 82, 234, 259, 208, 154, 133, 76, 79, 67, 214, 234, 172, 77, 81, 74, 225, 246, 195, 145, 124, 74, 77, 69, 65, 62, 201, 225, 162, 76, 79, 72, 214, 234, 181, 133, 113, 72, 76, 67, 64, 60, 186, 214, 148, 74, 77, 70, 201, 225, 172, 124, 108, 70, 74, 65, 62, 58, 178, 201, 141, 72, 76, 69, 26, 192, 214, 164, 118, 100, 69, 72, 64, 60, 57, 27, 178, 201, 71, 74, 28, 192, 214, 27, 69, 72, 201, 225, 108, 94, 26, 59, 55, 71, 74, 100, 91, 57, 53, 67, 212, 232, 178, 131, 88, 72, 76, 55, 52, 74, 77, 69, 62, 50, 208, 228, 178, 128, 87, 74, 77, 62, 50, 208, 228, 128, 87, 74, 77, 69, 62, 216, 236, 180, 135, 96, 50, 74, 77, 69, 62, 53, 214, 234, 133, 133, 100, 74, 77, 62, 62, 55, 205, 225, 178, 108, 108, 73, 76, 69, 57, 57, 205, 225, 178, 108, 108, 73, 76, 69, 57, 57, 216, 236, 174, 151, 130, 74, 77, 69, 65, 62, 227, 248, 173, 150, 129, 76, 79, 69, 65, 62, 234, 259, 178, 154, 133, 77, 81, 69, 65, 62, 234, 259, 178, 133, 133, 77, 81, 69, 62, 62, 214, 234, 178, 133, 133, 74, 77, 69, 62, 62, 26, 26, 26, 225, 246, 203, 165, 124, 27, 27, 27, 28, 27, 24, 20, 16, 11, 28, 28, 214, 234, 42, 27, 27, 76, 79, 37, 33, 31, 30, 72, 201, 225, 74, 77, 214, 234, 72, 76, 26, 26, 28, 26, 225, 246, 74, 77, 201, 225, 76, 79, 67, 60, 26, 26, 26, 212, 232, 188, 184, 113, 72, 76, 27, 27, 70, 70, 203, 183, 27, 72, 70, 28, 28, 214, 186, 27, 27, 24, 24, 20, 20, 16, 16, 11, 11, 28, 42, 42, 27, 37, 37, 33, 33, 31, 31, 30, 30, 74, 77, 74, 70, 214, 186, 26, 74, 70, 28, 28, 186, 165, 70, 67, 58, 26, 201, 178, 108, 27, 28, 186, 165, 27, 72, 69, 26, 26, 178, 154, 70, 67, 186, 165, 69, 65, 26, 201, 178, 70, 67, 178, 154, 72, 69, 57, 26, 26, 26, 243, 248, 185, 164, 100, 69, 65, 27, 79, 79, 242, 261, 27, 27, 79, 81, 28, 246, 264, 27, 24, 20, 16, 11, 28, 28, 42, 27, 27, 37, 33, 31, 30, 79, 82, 67, 246, 264, 26, 26, 79, 82, 28, 13, 225, 246, 76, 79, 70, 55, 26, 13, 26, 234, 259, 203, 178, 94, 14, 15, 16, 27, 17, 19, 27, 20, 21, 23, 28, 24, 27, 25, 26, 27, 28, 28, 225, 246, 26, 27, 27, 77, 81, 72, 26, 214, 234, 154, 76, 79, 27, 225, 246, 74, 77, 26, 26, 28, 234, 259, 27, 76, 79, 26, 214, 234, 77, 81, 65, 69, 53, 26, 26, 225, 246, 145, 165, 124, 74, 77, 27, 27, 28, 28, 214, 234, 154, 27, 27, 76, 79, 64, 26, 201, 225, 164, 74, 77, 65, 27, 214, 234, 72, 76, 26, 26, 28, 225, 246, 27, 74, 77, 26, 201, 225, 76, 79, 67, 67, 60, 26, 26, 26, 214, 234, 133, 188, 113, 72, 76, 27, 27, 27, 28, 27, 28, 28, 201, 225, 26, 27, 27, 74, 77, 70, 26, 186, 214, 154, 72, 76, 27, 201, 225, 70, 74, 26, 26, 28, 214, 234, 27, 72, 76, 26, 186, 214, 74, 77, 62, 65, 58, 26, 26, 26, 199, 223, 180, 126, 108, 70, 74, 27, 27, 27, 76, 69, 28, 26, 28, 178, 201, 27, 27, 28, 192, 26, 27, 26, 27, 69, 72, 60, 26, 28, 178, 201, 145, 27, 71, 26, 192, 214, 69, 72, 26, 27, 72, 201, 225, 178, 71, 74, 28, 28, 214, 234, 27, 72, 76, 69, 57, 26, 26, 225, 244, 194, 100, 74, 77, 27, 234, 76, 27, 26, 79, 28, 244, 165, 27, 77, 28, 178, 26, 27, 67, 79, 71, 64, 26, 26, 165, 186, 186, 133, 69, 27, 27, 178, 201, 67, 70, 26, 28, 28, 186, 214, 27, 27, 69, 72, 26, 26, 201, 225, 70, 74, 70, 62, 55, 26, 26, 26, 214, 234, 180, 135, 94, 72, 76, 27, 27, 246, 77, 27, 28, 28, 26, 26, 234, 259, 27, 27, 74, 79, 28, 26, 26, 27, 27, 27, 69, 62, 26, 26, 28, 28, 178, 133, 27, 27, 26, 27, 27, 26, 26, 28, 28, 225, 246, 100, 27, 27, 77, 81, 53, 26, 26, 212, 234, 76, 79, 74, 69, 62, 26, 26, 26, 207, 223, 178, 108, 106, 77, 55, 27, 27, 27, 73, 76, 26, 214, 234, 28, 28, 28, 27, 27, 27, 27, 77, 26, 28, 225, 27, 27, 26, 26, 26, 26, 28, 204, 27, 74, 26, 73, 76, 69, 57, 57, 26, 26, 26, 26, 26, 214, 214, 159, 108, 89, 27, 27, 27, 27, 27, 28, 28, 28, 28, 28, 27, 27, 27, 27, 27, 26, 26, 26, 26, 26, 25, 25, 25, 25, 25, 24, 24, 24, 24, 24, 23, 23, 23, 23, 23, 21, 21, 21, 21, 21, 20, 20, 20, 20, 20, 18, 18, 18, 18, 18, 16, 16, 16, 16, 16, 14, 14, 14, 14, 14, 12, 12, 12, 12, 12, 10, 10, 10, 10, 10, 44, 44, 44, 44, 44, 43, 43, 43, 43, 43, 41, 41, 41, 41, 41, 39, 39, 39, 39, 39, 37, 37, 37, 37, 37, 35, 35, 35, 35, 35, 34, 34, 34, 34, 34, 33, 33, 33, 33, 33, 32, 32, 32, 32, 32, 31, 31, 31, 31, 31, 30, 30, 30, 30, 30, 74, 74, 66, 57, 50, 276]\n",
            "[277, 47, 275, 28, 28, 28, 28, 28, 2, 6, 5, 4, 3, 46, 7, 46, 7, 46, 7, 46, 7, 46, 7, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 9, 9, 9, 9, 9, 8, 8, 8, 8, 8, 26, 26, 26, 259, 234, 133, 81, 77, 62, 259, 234, 214, 81, 77, 74, 259, 234, 214, 81, 77, 74, 236, 216, 216, 77, 74, 74, 246, 225, 201, 79, 76, 72, 26, 259, 234, 154, 246, 225, 81, 77, 27, 259, 234, 79, 76, 28, 28, 268, 246, 27, 81, 77, 271, 259, 83, 79, 268, 246, 84, 81, 26, 271, 259, 83, 79, 268, 246, 84, 81, 65, 26, 271, 259, 154, 83, 79, 268, 246, 84, 81, 27, 259, 234, 83, 79, 28, 246, 225, 27, 81, 77, 256, 232, 79, 76, 26, 81, 77, 65, 254, 228, 154, 81, 77, 254, 228, 65, 246, 225, 201, 81, 77, 79, 76, 72, 259, 225, 201, 81, 76, 72, 26, 26, 26, 236, 216, 216, 27, 27, 27, 28, 28, 28, 27, 27, 27, 26, 26, 26, 77, 74, 74, 26, 26, 26, 225, 205, 178, 27, 27, 27, 28, 28, 28, 27, 27, 27, 24, 24, 24, 20, 20, 20, 15, 15, 15, 10, 10, 10, 41, 41, 41, 37, 37, 37, 33, 33, 33, 30, 30, 30, 76, 73, 69, 28, 28, 28, 26, 26, 234, 214, 213, 77, 74, 74, 225, 201, 224, 76, 72, 76, 214, 214, 233, 74, 74, 77, 203, 227, 227, 72, 76, 76, 214, 234, 213, 74, 77, 74, 225, 246, 200, 214, 234, 213, 76, 79, 72, 225, 246, 200, 74, 77, 74, 214, 234, 76, 79, 225, 225, 74, 77, 234, 214, 213, 76, 76, 72, 246, 225, 200, 77, 74, 74, 234, 214, 213, 79, 76, 72, 246, 225, 200, 77, 74, 74, 234, 214, 213, 79, 76, 72, 225, 225, 200, 77, 74, 74, 214, 234, 213, 76, 76, 72, 223, 244, 198, 74, 77, 74, 76, 79, 72, 225, 246, 200, 76, 79, 72, 26, 26, 261, 236, 27, 27, 26, 28, 28, 154, 27, 27, 26, 26, 81, 77, 65, 246, 225, 201, 79, 76, 72, 246, 225, 124, 79, 76, 60, 26, 26, 236, 216, 133, 27, 27, 145, 62, 28, 28, 154, 27, 27, 64, 165, 26, 26, 65, 77, 74, 26, 26, 225, 201, 178, 67, 27, 27, 192, 69, 28, 28, 201, 27, 27, 71, 26, 26, 175, 72, 69, 76, 72, 26, 26, 236, 216, 216, 27, 27, 74, 28, 28, 133, 27, 27, 26, 26, 77, 74, 62, 26, 26, 225, 205, 178, 27, 27, 69, 28, 28, 178, 27, 27, 24, 20, 15, 26, 10, 41, 37, 33, 30, 76, 73, 69, 26, 26, 236, 216, 28, 27, 27, 26, 28, 28, 234, 27, 27, 26, 26, 77, 77, 74, 271, 225, 201, 84, 76, 72, 201, 225, 201, 72, 76, 72, 26, 26, 214, 216, 188, 27, 27, 225, 74, 28, 28, 234, 27, 27, 76, 246, 26, 26, 77, 74, 70, 26, 259, 201, 178, 79, 27, 268, 81, 69, 28, 271, 178, 27, 83, 26, 259, 84, 72, 69, 264, 214, 167, 81, 259, 82, 67, 264, 165, 81, 246, 225, 74, 82, 67, 256, 234, 154, 79, 76, 246, 77, 81, 178, 259, 79, 246, 145, 81, 65, 69, 178, 234, 133, 79, 64, 192, 69, 201, 225, 178, 71, 77, 62, 214, 214, 185, 72, 76, 69, 70, 225, 205, 178, 74, 74, 234, 76, 69, 26, 246, 214, 165, 77, 73, 27, 234, 79, 67, 26, 26, 28, 223, 180, 27, 77, 27, 27, 26, 28, 28, 26, 204, 178, 27, 27, 74, 26, 26, 76, 73, 69, 69, 26, 26, 26, 214, 214, 133, 178, 27, 27, 27, 69, 178, 28, 28, 28, 28, 28, 28, 27, 27, 27, 26, 26, 26, 25, 25, 25, 69, 24, 24, 24, 180, 23, 23, 23, 21, 21, 21, 20, 20, 20, 18, 18, 18, 16, 16, 16, 14, 14, 14, 12, 12, 12, 11, 11, 11, 45, 45, 45, 43, 43, 43, 69, 41, 41, 41, 186, 39, 39, 39, 37, 37, 37, 36, 36, 36, 34, 34, 34, 33, 33, 33, 32, 32, 32, 31, 31, 31, 30, 30, 30, 70, 74, 74, 62, 29, 29, 29, 178, 28, 28, 28, 26, 26, 26, 69, 26, 259, 234, 214, 172, 133, 69, 172, 81, 77, 74, 69, 62, 261, 236, 216, 180, 135, 81, 77, 74, 69, 62, 259, 234, 214, 178, 133, 81, 77, 74, 62, 259, 234, 214, 133, 69, 178, 81, 77, 74, 69, 62, 264, 246, 214, 165, 100, 82, 79, 74, 67, 55, 26, 261, 236, 216, 180, 135, 27, 81, 77, 74, 62, 28, 254, 228, 208, 128, 27, 81, 77, 74, 62, 254, 228, 208, 128, 26, 81, 77, 74, 69, 62, 26, 259, 234, 214, 154, 133, 27, 81, 77, 74, 62, 28, 234, 214, 178, 133, 27, 26, 65, 135, 77, 74, 69, 62, 230, 209, 173, 129, 62, 246, 225, 201, 165, 124, 77, 74, 69, 62, 79, 76, 72, 67, 60, 26, 26, 26, 259, 234, 201, 154, 154, 27, 27, 27, 81, 28, 271, 27, 28, 28, 26, 28, 28, 264, 165, 27, 27, 84, 65, 77, 259, 271, 175, 82, 67, 246, 264, 81, 84, 26, 26, 69, 234, 259, 178, 79, 82, 225, 246, 77, 81, 72, 69, 65, 26, 26, 212, 234, 214, 113, 76, 79, 27, 27, 225, 77, 74, 28, 28, 28, 186, 214, 27, 27, 76, 26, 26, 201, 225, 70, 74, 74, 58, 214, 232, 216, 115, 72, 76, 225, 74, 77, 74, 58, 26, 26, 234, 264, 214, 113, 76, 214, 27, 77, 74, 58, 26, 26, 28, 223, 201, 124, 27, 74, 26, 27, 27, 76, 82, 26, 234, 259, 201, 28, 28, 27, 28, 28, 27, 27, 81, 72, 26, 28, 248, 203, 27, 27, 26, 26, 26, 77, 72, 28, 225, 201, 27, 26, 76, 79, 72, 72, 60, 26, 26, 26, 26, 234, 234, 178, 201, 94, 27, 27, 27, 27, 72, 28, 28, 28, 28, 154, 27, 27, 27, 27, 24, 24, 24, 24, 20, 20, 20, 20, 15, 15, 15, 15, 10, 10, 10, 10, 41, 41, 41, 41, 37, 37, 37, 37, 33, 33, 33, 33, 30, 30, 30, 30, 77, 77, 69, 65, 53, 203, 28, 28, 28, 28, 25, 25, 25, 25, 72, 235, 260, 202, 195, 155, 72, 195, 77, 81, 72, 72, 65, 235, 260, 202, 201, 155, 77, 81, 72, 72, 65, 235, 260, 202, 201, 155, 77, 81, 72, 65, 236, 261, 203, 156, 72, 197, 77, 81, 72, 72, 65, 226, 247, 202, 201, 125, 76, 79, 72, 72, 60, 26, 235, 260, 202, 154, 155, 27, 77, 81, 72, 65, 28, 229, 255, 196, 149, 27, 24, 20, 15, 77, 81, 72, 65, 10, 229, 255, 196, 149, 41, 37, 33, 30, 77, 81, 72, 65, 65, 236, 261, 203, 156, 77, 81, 72, 65, 260, 235, 202, 155, 28, 81, 77, 72, 65, 247, 247, 202, 155, 79, 79, 72, 65, 235, 260, 202, 155, 77, 81, 72, 65, 25, 25, 25, 227, 248, 203, 125, 76, 26, 26, 202, 25, 25, 26, 215, 25, 72, 79, 72, 25, 25, 226, 272, 166, 74, 235, 76, 26, 26, 247, 25, 25, 77, 226, 79, 84, 67, 60, 25, 25, 235, 261, 216, 134, 76, 81, 26, 215, 25, 26, 247, 226, 25, 77, 74, 74, 25, 25, 257, 235, 215, 79, 76, 247, 77, 26, 26, 260, 25, 25, 79, 238, 81, 81, 74, 62, 227, 250, 194, 147, 78, 76, 80, 71, 64, 25, 215, 260, 179, 155, 74, 69, 65, 25, 25, 26, 25, 226, 193, 146, 25, 26, 26, 26, 26, 250, 25, 25, 25, 81, 76, 80, 71, 64, 25, 25, 25, 25, 206, 260, 179, 109, 26, 26, 145, 26, 26, 27, 27, 27, 27, 64, 145, 28, 28, 27, 27, 26, 26, 25, 25, 64, 28, 28, 24, 24, 145, 27, 27, 23, 23, 22, 22, 21, 21, 20, 20, 19, 19, 26, 26, 18, 18, 17, 17, 25, 25, 15, 15, 14, 14, 24, 24, 13, 13, 12, 12, 23, 23, 10, 10, 22, 22, 45, 45, 44, 44, 21, 21, 43, 43, 64, 20, 20, 41, 41, 142, 19, 19, 40, 40, 18, 18, 39, 39, 38, 38, 17, 17, 37, 37, 64, 16, 16, 36, 36, 145, 15, 15, 35, 35, 14, 14, 34, 34, 13, 13, 33, 33, 12, 12, 11, 11, 32, 32, 10, 10, 31, 31, 45, 45, 44, 44, 43, 43, 42, 42, 73, 81, 64, 26, 154, 41, 41, 40, 40, 39, 39, 38, 38, 27, 37, 37, 36, 36, 35, 35, 34, 34, 28, 28, 28, 27, 33, 33, 32, 32, 31, 31, 26, 26, 26, 69, 65, 57, 26, 145, 28, 28, 26, 26, 27, 225, 205, 178, 108, 28, 27, 26, 25, 76, 73, 69, 57, 24, 225, 205, 178, 108, 23, 21, 20, 18, 16, 14, 12, 10, 44, 43, 76, 73, 69, 57, 41, 225, 205, 178, 108, 39, 37, 35, 34, 33, 32, 31, 30, 64, 76, 73, 69, 57, 222, 194, 147, 104, 76, 71, 64, 56, 225, 205, 145, 108, 28, 76, 73, 64, 57, 26, 26, 26, 26, 234, 214, 178, 89, 27, 27, 27, 27, 28, 28, 28, 28, 26, 27, 27, 27, 27, 26, 26, 26, 26, 77, 74, 69, 50, 26, 26, 26, 26, 227, 207, 180, 110, 27, 27, 27, 27, 28, 28, 28, 28, 145, 27, 27, 27, 27, 24, 24, 24, 24, 20, 20, 20, 20, 16, 16, 16, 16, 11, 11, 11, 11, 42, 42, 42, 42, 37, 37, 37, 37, 33, 33, 33, 33, 31, 31, 31, 31, 30, 30, 30, 30, 76, 73, 69, 64, 57, 26, 145, 28, 28, 28, 28, 27, 26, 26, 26, 26, 28, 225, 205, 178, 108, 27, 26, 64, 76, 73, 69, 57, 227, 207, 180, 147, 110, 69, 64, 57, 26, 178, 145, 108, 76, 73, 225, 205, 27, 76, 73, 69, 57, 28, 225, 205, 178, 108, 27, 26, 76, 73, 69, 64, 57, 234, 214, 178, 133, 133, 77, 74, 69, 62, 62, 26, 26, 26, 26, 26, 248, 227, 203, 167, 126, 27, 27, 27, 27, 27, 28, 28, 28, 28, 28, 27, 27, 27, 27, 27, 26, 26, 26, 26, 26, 79, 76, 72, 67, 60, 26, 26, 26, 26, 246, 225, 201, 165, 124, 27, 27, 27, 27, 72, 28, 28, 201, 27, 27, 28, 28, 26, 26, 27, 27, 79, 72, 67, 26, 26, 248, 201, 147, 26, 26, 27, 27, 79, 76, 60, 28, 26, 259, 225, 124, 27, 28, 26, 27, 27, 76, 64, 60, 28, 225, 178, 124, 27, 26, 26, 81, 76, 72, 69, 60, 234, 214, 178, 133, 133, 77, 74, 69, 62, 62, 26, 26, 26, 26, 227, 203, 178, 147, 108, 27, 27, 27, 27, 76, 28, 28, 178, 27, 27, 28, 28, 192, 26, 26, 27, 27, 69, 72, 64, 26, 201, 178, 108, 71, 27, 214, 192, 72, 69, 26, 26, 28, 225, 201, 27, 74, 71, 26, 234, 214, 76, 72, 69, 57, 57, 26, 26, 26, 244, 225, 133, 119, 100, 77, 74, 27, 234, 76, 27, 27, 79, 28, 165, 244, 27, 77, 28, 28, 178, 26, 27, 27, 67, 79, 59, 26, 192, 165, 133, 69, 201, 178, 71, 67, 26, 26, 27, 214, 192, 72, 69, 28, 28, 225, 201, 27, 74, 71, 62, 55, 26, 26, 232, 214, 180, 96, 76, 72, 27, 27, 225, 74, 26, 77, 28, 28, 154, 232, 27, 27, 76, 165, 26, 26, 65, 77, 69, 62, 53, 26, 26, 26, 178, 154, 154, 108, 94, 67, 27, 27, 27, 192, 165, 69, 65, 28, 28, 28, 201, 178, 27, 27, 27, 71, 67, 26, 26, 26, 214, 192, 72, 69, 65, 57, 53, 26, 26, 26, 223, 201, 167, 119, 92, 74, 71, 27, 27, 27, 214, 72, 76, 28, 28, 28, 26, 214, 223, 27, 27, 27, 74, 74, 201, 26, 26, 26, 27, 72, 67, 59, 52, 26, 26, 26, 28, 192, 165, 118, 91, 27, 27, 27, 26, 27, 71, 76, 28, 28, 178, 214, 27, 27, 74, 28, 26, 26, 201, 27, 69, 72, 67, 52, 26, 26, 169, 194, 145, 91, 26, 27, 27, 71, 59, 26, 178, 201, 108, 68, 28, 28, 27, 27, 27, 72, 57, 26, 26, 28, 192, 118, 27, 27, 27, 26, 26, 26, 28, 28, 168, 27, 27, 69, 26, 26, 68, 71, 64, 59, 52, 26, 26, 26, 26, 26, 180, 207, 147, 108, 108, 27, 27, 27, 27, 27, 28, 28, 28, 27, 27, 27, 24, 24, 24, 20, 20, 20, 16, 16, 16, 11, 11, 11, 28, 28, 42, 42, 42, 27, 27, 37, 37, 37, 33, 33, 33, 26, 26, 31, 31, 31, 25, 25, 30, 30, 30, 69, 73, 64, 24, 24, 23, 23, 21, 21, 20, 20, 18, 18, 16, 16, 28, 28, 28, 26, 26, 26, 14, 14, 12, 12, 10, 10, 44, 44, 43, 43, 41, 41, 201, 225, 178, 39, 39, 37, 37, 35, 35, 34, 34, 33, 33, 32, 32, 31, 31, 30, 30, 57, 57, 72, 76, 69, 203, 236, 180, 72, 77, 69, 192, 225, 165, 28, 28, 71, 76, 67, 178, 214, 154, 69, 74, 65, 165, 201, 145, 67, 72, 64, 26, 26, 26, 180, 216, 156, 27, 27, 27, 28, 28, 28, 27, 27, 27, 26, 26, 26, 69, 74, 65, 26, 26, 26, 192, 225, 145, 27, 27, 27, 28, 28, 28, 27, 27, 27, 26, 26, 26, 71, 76, 64, 203, 227, 180, 72, 76, 69, 216, 236, 216, 74, 77, 74, 225, 246, 201, 76, 79, 72, 214, 246, 186, 74, 79, 70, 225, 246, 201, 76, 79, 72, 26, 26, 26, 236, 261, 156, 27, 27, 27, 28, 28, 28, 27, 27, 27, 26, 26, 26, 77, 81, 65, 26, 225, 246, 201, 27, 214, 234, 76, 79, 28, 225, 246, 27, 74, 77, 26, 201, 225, 76, 79, 72, 26, 214, 234, 188, 72, 76, 27, 201, 225, 74, 77, 28, 214, 234, 27, 72, 76, 186, 214, 26, 74, 77, 70, 26, 201, 225, 178, 70, 74, 27, 186, 214, 72, 76, 28, 201, 225, 27, 70, 74, 26, 175, 199, 72, 76, 69, 72, 69, 26, 26, 188, 216, 165, 27, 27, 74, 28, 165, 27, 28, 178, 26, 27, 67, 70, 165, 192, 69, 178, 201, 67, 71, 26, 192, 214, 69, 72, 201, 225, 71, 74, 67, 214, 234, 156, 72, 76, 225, 246, 74, 77, 65, 26, 26, 234, 259, 133, 76, 79, 27, 27, 62, 28, 28, 154, 27, 27, 26, 26, 65, 225, 246, 165, 77, 81, 214, 234, 76, 79, 67, 26, 204, 223, 178, 74, 77, 27, 73, 76, 214, 214, 28, 27, 74, 74, 26, 26, 205, 225, 27, 27, 26, 28, 28, 27, 27, 26, 26, 73, 76, 69, 26, 26, 26, 216, 216, 135, 27, 27, 27, 28, 28, 28, 27, 27, 27, 24, 24, 24, 20, 20, 20, 16, 16, 16, 11, 11, 11, 42, 42, 42, 37, 37, 37, 33, 33, 33, 31, 31, 31, 30, 30, 30, 74, 74, 62, 28, 28, 28, 26, 26, 26, 225, 201, 178, 76, 72, 69, 236, 203, 180, 77, 72, 69, 225, 192, 165, 76, 71, 67, 214, 178, 154, 74, 69, 65, 201, 165, 145, 72, 67, 64, 26, 26, 26, 216, 180, 156, 27, 27, 27, 28, 28, 28, 26, 26, 27, 27, 27, 26, 26, 26, 74, 69, 65, 26, 225, 192, 145, 27, 76, 71, 28, 246, 225, 201, 124, 27, 24, 20, 15, 10, 41, 37, 33, 30, 79, 76, 64, 72, 60, 261, 236, 203, 156, 81, 77, 72, 65, 246, 225, 201, 124, 28, 79, 76, 72, 60, 234, 214, 178, 133, 77, 74, 69, 62, 225, 205, 178, 108, 76, 73, 69, 57, 26, 26, 26, 26, 236, 216, 180, 135, 27, 27, 27, 27, 28, 28, 28, 28, 27, 27, 27, 27, 26, 26, 26, 26, 77, 74, 69, 62, 26, 26, 26, 26, 246, 225, 201, 124, 27, 27, 27, 27, 28, 28, 28, 28, 27, 27, 27, 27, 24, 20, 15, 10, 26, 26, 26, 41, 37, 33, 30, 79, 76, 72, 60, 203, 227, 180, 72, 76, 69, 216, 236, 216, 74, 77, 74, 225, 246, 201, 28, 76, 79, 72, 214, 246, 186, 74, 79, 70, 225, 246, 201, 76, 79, 72, 26, 26, 26, 236, 261, 156, 26, 26, 27, 27, 27, 28, 28, 28, 27, 27, 27, 26, 26, 26, 77, 81, 65, 225, 201, 172, 178, 108, 76, 72, 69, 69, 57, 234, 214, 172, 133, 133, 77, 74, 62, 62, 246, 225, 201, 165, 124, 69, 79, 76, 72, 67, 60, 248, 216, 216, 167, 115, 79, 74, 74, 67, 58, 246, 225, 201, 165, 124, 79, 76, 72, 67, 60, 26, 26, 26, 26, 26, 259, 234, 201, 154, 94, 27, 27, 27, 27, 27, 28, 28, 28, 28, 28, 27, 27, 27, 27, 27, 24, 24, 20, 20, 15, 15, 26, 10, 10, 26, 26, 41, 41, 37, 37, 33, 33, 30, 30, 81, 77, 72, 65, 53, 26, 246, 225, 126, 27, 234, 214, 79, 76, 28, 246, 225, 27, 77, 74, 225, 201, 26, 79, 76, 28, 28, 60, 26, 26, 234, 214, 113, 76, 72, 27, 225, 201, 77, 74, 28, 234, 214, 27, 76, 72, 26, 214, 186, 77, 74, 58, 26, 223, 199, 201, 144, 110, 74, 70, 27, 192, 132, 72, 64, 76, 72, 28, 201, 178, 201, 144, 27, 71, 62, 178, 123, 26, 72, 64, 72, 69, 57, 26, 186, 132, 100, 69, 60, 27, 178, 123, 70, 62, 28, 28, 28, 186, 132, 27, 69, 60, 26, 165, 112, 70, 62, 55, 26, 258, 233, 178, 121, 96, 67, 58, 27, 245, 224, 192, 81, 77, 69, 60, 28, 258, 233, 201, 107, 27, 79, 76, 71, 24, 20, 16, 11, 233, 213, 214, 42, 81, 77, 72, 37, 33, 31, 30, 57, 53, 245, 224, 199, 77, 74, 74, 233, 213, 79, 76, 72, 28, 13, 245, 224, 201, 77, 74, 224, 200, 79, 76, 72, 13, 231, 211, 178, 153, 135, 76, 72, 14, 16, 18, 20, 165, 144, 22, 69, 65, 24, 26, 27, 77, 74, 28, 233, 213, 178, 153, 27, 67, 64, 154, 132, 26, 69, 65, 77, 74, 62, 26, 165, 144, 124, 65, 62, 27, 154, 132, 67, 64, 28, 26, 165, 144, 27, 65, 62, 26, 145, 123, 67, 64, 60, 26, 234, 152, 132, 115, 26, 64, 60, 27, 225, 123, 77, 62, 65, 28, 234, 154, 132, 27, 76, 60, 214, 112, 26, 77, 62, 65, 58, 26, 26, 271, 225, 201, 121, 108, 74, 58, 27, 27, 268, 214, 84, 76, 60, 28, 28, 271, 225, 107, 27, 27, 83, 74, 26, 26, 259, 201, 84, 76, 72, 57, 57, 26, 26, 264, 214, 188, 102, 81, 72, 27, 27, 259, 201, 82, 74, 28, 28, 26, 264, 214, 27, 27, 81, 72, 246, 186, 26, 26, 82, 74, 70, 55, 26, 26, 256, 201, 178, 154, 94, 79, 70, 27, 27, 145, 65, 81, 28, 28, 234, 214, 154, 27, 27, 72, 64, 26, 26, 133, 65, 77, 69, 53, 26, 26, 246, 199, 167, 145, 126, 74, 62, 27, 27, 234, 133, 79, 64, 67, 28, 28, 246, 165, 145, 27, 27, 77, 62, 225, 124, 26, 26, 79, 64, 72, 67, 60, 26, 26, 26, 234, 154, 214, 133, 113, 76, 60, 27, 27, 27, 225, 124, 77, 62, 28, 28, 28, 234, 133, 27, 27, 27, 76, 60, 26, 26, 26, 214, 113, 77, 62, 65, 74, 58, 26, 26, 26, 223, 178, 201, 122, 108, 74, 58, 27, 165, 69, 27, 27, 76, 28, 178, 154, 27, 67, 28, 28, 192, 165, 26, 27, 27, 69, 65, 60, 26, 201, 178, 108, 71, 67, 27, 214, 192, 72, 69, 26, 26, 28, 225, 201, 27, 74, 71, 26, 234, 214, 76, 72, 72, 57, 26, 26, 26, 244, 225, 186, 115, 98, 77, 74, 57, 27, 232, 76, 27, 27, 77, 79, 28, 165, 246, 27, 28, 28, 178, 26, 27, 27, 67, 79, 58, 26, 186, 165, 133, 69, 201, 178, 70, 67, 26, 26, 27, 214, 186, 72, 69, 28, 225, 201, 27, 74, 70, 70, 55, 26, 26, 232, 214, 178, 94, 76, 72, 26, 225, 74, 27, 27, 77, 62, 26, 26, 259, 234, 124, 76, 28, 28, 27, 27, 27, 27, 60, 26, 28, 28, 133, 27, 27, 26, 27, 26, 26, 26, 28, 246, 225, 100, 27, 81, 77, 53, 26, 234, 214, 79, 76, 69, 62, 26, 26, 26, 225, 201, 180, 110, 106, 77, 74, 55, 27, 27, 214, 192, 76, 72, 27, 28, 28, 201, 178, 27, 27, 74, 71, 28, 192, 192, 26, 26, 27, 72, 69, 69, 57, 26, 26, 178, 201, 178, 145, 71, 71, 27, 192, 214, 69, 72, 26, 27, 28, 201, 225, 27, 71, 74, 28, 26, 214, 234, 27, 72, 76, 69, 57, 26, 225, 246, 186, 102, 74, 77, 26, 27, 234, 259, 76, 79, 64, 28, 246, 264, 133, 27, 77, 81, 234, 259, 201, 26, 70, 79, 82, 62, 55, 26, 26, 246, 264, 212, 165, 100, 77, 81, 72, 27, 27, 234, 259, 79, 82, 74, 28, 28, 225, 246, 186, 27, 27, 77, 81, 26, 26, 214, 234, 76, 79, 70, 67, 55, 26, 26, 204, 244, 178, 147, 108, 74, 77, 27, 27, 73, 79, 64, 214, 234, 133, 28, 28, 27, 27, 74, 77, 62, 26, 26, 26, 205, 225, 145, 27, 27, 27, 26, 26, 28, 28, 28, 27, 27, 27, 26, 26, 26, 73, 76, 69, 64, 57, 26, 26, 26, 26, 26, 214, 237, 178, 133, 89, 27, 27, 27, 27, 27, 28, 28, 28, 28, 28, 27, 27, 27, 27, 27, 26, 26, 26, 26, 26, 25, 25, 25, 25, 25, 24, 24, 24, 24, 24, 23, 23, 23, 23, 23, 22, 22, 22, 22, 22, 21, 21, 21, 21, 21, 20, 20, 20, 20, 20, 18, 18, 18, 18, 18, 17, 17, 17, 17, 17, 16, 16, 16, 16, 16, 15, 15, 15, 15, 15, 13, 13, 13, 13, 13, 12, 12, 12, 12, 12, 11, 11, 11, 11, 11, 45, 45, 45, 45, 45, 44, 44, 44, 44, 44, 43, 43, 43, 43, 43, 41, 41, 41, 41, 41, 40, 40, 40, 40, 40, 39, 39, 39, 39, 39, 38, 38, 38, 38, 38, 36, 36, 36, 36, 36, 35, 35, 35, 35, 35, 34, 34, 34, 34, 34, 33, 33, 33, 33, 33, 32, 32, 32, 32, 32, 31, 31, 31, 31, 31, 30, 30, 30, 30, 30, 74, 78, 69, 62, 50, 276]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "agent = MusicDQNAgent(d_model=128, max_length=20, num_tokens=len(vocab), num_actions=len(vocab))\n",
        "num_episodes = 500\n",
        "\n",
        "for index in test_midis:\n",
        "  target_sequence_list = token_ids_data[index]\n",
        "  for episode in range(num_episodes):\n",
        "    current_seq = [vocab[\"<song_start>\"]]\n",
        "    done = False\n",
        "\n",
        "    while not done and len(current_seq) < 20:\n",
        "        state_tensor = torch.tensor([current_seq], dtype=torch.long)\n",
        "\n",
        "        # Ensure state_tensor sequence length does not exceed max_length\n",
        "        if state_tensor.shape[1] > agent.encoder.pe.pe.shape[1]:\n",
        "            break # Stop if the sequence gets too long\n",
        "\n",
        "\n",
        "        state_vector = agent.get_state(state_tensor, no_grad=False)\n",
        "\n",
        "        # Detach the state vector for action selection as we don't need gradients here\n",
        "        action = agent.get_action(state_vector.detach())\n",
        "\n",
        "\n",
        "        next_seq = current_seq + [action]\n",
        "        next_state_tensor = torch.tensor([next_seq], dtype=torch.long)\n",
        "\n",
        "        # Determine if the episode is done and calculate reward\n",
        "        done = action == vocab[\"<song_end>\"] or len(next_seq) >= 20\n",
        "\n",
        "        # Calculate reward, handling the case where we go beyond the target sequence length\n",
        "        if len(current_seq) < len(target_sequence_list):\n",
        "            reward = 1.0 if action == target_sequence_list[len(current_seq)] else -0.1\n",
        "        else:\n",
        "            # Penalize if we exceed the target length without generating the end token\n",
        "            reward = -0.5 if action != vocab[\"<song_end>\"] else 1.0\n",
        "\n",
        "\n",
        "        # Get next_state_vector only if not done to avoid potential issues with sequence length if done\n",
        "        if not done and next_state_tensor.shape[1] <= agent.encoder.pe.pe.shape[1]:\n",
        "             next_state_vector = agent.get_state(next_state_tensor, no_grad=False)\n",
        "        else:\n",
        "             next_state_vector = None # No next state to consider if done or exceeding max length\n",
        "\n",
        "\n",
        "        # Train the agent\n",
        "        # Pass next_state_vector to train_step even if it's None, the train_step will handle it\n",
        "        agent.train_step(state_vector, action, reward, next_state_vector, done)\n",
        "\n",
        "\n",
        "        current_seq = next_seq"
      ],
      "metadata": {
        "id": "3Kx0hrExduYQ",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 472
        },
        "outputId": "4a7b6d07-2a0c-4add-f5f9-4bd6178616a7"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "linear(): argument 'input' (position 1) must be Tensor, not NoneType",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-31-3580778306.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     45\u001b[0m         \u001b[0;31m# Train the agent\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m         \u001b[0;31m# Pass next_state_vector to train_step even if it's None, the train_step will handle it\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 47\u001b[0;31m         \u001b[0magent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate_vector\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maction\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnext_state_vector\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     48\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-27-3020760877.py\u001b[0m in \u001b[0;36mtrain_step\u001b[0;34m(self, state, action, reward, next_state, done)\u001b[0m\n\u001b[1;32m     80\u001b[0m         \u001b[0;31m#Detach() removes it from the computation graph — this prevents gradients from flowing through the target.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     81\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 82\u001b[0;31m       \u001b[0mnext_q\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mq_net\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnext_state\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     83\u001b[0m       \u001b[0mq_target\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mreward\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgamma\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnext_q\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     84\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1739\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1740\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1741\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1748\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1751\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1752\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-26-2922252974.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, state_vector)\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstate_vector\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate_vector\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Q-values for each action\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1739\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1740\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1741\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1748\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1751\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1752\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/linear.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    123\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    124\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 125\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    126\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    127\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mextra_repr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: linear(): argument 'input' (position 1) must be Tensor, not NoneType"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Deepseek stripped back version"
      ],
      "metadata": {
        "id": "u14CmMcnE8Te"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "test_midi = ['monteverdi_libri_dei_madrigali_1_5_(c)icking-archive.mid']"
      ],
      "metadata": {
        "id": "2zqPY4gNFCb0"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "agent = MusicDQNAgent(d_model=128, max_length=20, num_tokens=len(vocab), num_actions=len(vocab))\n",
        "num_episodes = 500\n",
        "\n",
        "for index in test_midi:\n",
        "    target_sequence = token_ids_data[index]  # Target sequence for this episode\n",
        "\n",
        "    #target_sequence = torch.tensor(token_ids_data)\n",
        "    for episode in range(num_episodes):\n",
        "        current_seq = [vocab[\"<song_start>\"]]  # Initialize with start token\n",
        "        done = False\n",
        "\n",
        "        while not done and len(current_seq) < 20:\n",
        "            # Convert current sequence to tensor (shape: [1, seq_len])\n",
        "            state_tensor = torch.tensor([current_seq], dtype=torch.long)\n",
        "\n",
        "            # Get state vector (with gradients for training)\n",
        "            state_vector = agent.get_state(state_tensor, no_grad=False)\n",
        "\n",
        "            # Select action (no gradients needed here)\n",
        "            action = agent.get_action(state_vector.detach())\n",
        "\n",
        "            # Update sequence\n",
        "            next_seq = current_seq + [action]\n",
        "            done = (action == vocab[\"<song_end>\"]) or (len(next_seq) >= 20)\n",
        "\n",
        "            # Calculate reward\n",
        "            if len(current_seq) < len(target_sequence):\n",
        "                reward = 1.0 if (action == target_sequence[len(current_seq)]) else -0.1\n",
        "            else:\n",
        "                reward = -0.5 if (action != vocab[\"<song_end>\"]) else 1.0\n",
        "\n",
        "            # Get next state vector if needed\n",
        "            if not done:\n",
        "                next_state_tensor = torch.tensor([next_seq], dtype=torch.long)\n",
        "                next_state_vector = agent.get_state(next_state_tensor, no_grad=False)\n",
        "            else:\n",
        "                next_state_vector = torch.zeros_like(state_vector)\n",
        "\n",
        "            # Train the agent\n",
        "            agent.train_step(state_vector, action, reward, next_state_vector, done)\n",
        "\n",
        "            current_seq = next_seq"
      ],
      "metadata": {
        "id": "ws3ZsVuVE7PR"
      },
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "current_seq"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BHfV62gqGYMP",
        "outputId": "5b39ffcf-d5d9-4805-a8cb-c62b1a5dda02"
      },
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[277,\n",
              " 211,\n",
              " 275,\n",
              " 173,\n",
              " 173,\n",
              " 252,\n",
              " 173,\n",
              " 206,\n",
              " 173,\n",
              " 173,\n",
              " 202,\n",
              " 173,\n",
              " 193,\n",
              " 173,\n",
              " 131,\n",
              " 173,\n",
              " 13,\n",
              " 265,\n",
              " 173,\n",
              " 173]"
            ]
          },
          "metadata": {},
          "execution_count": 45
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "type(next_state_vector)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eqYSp4T8GcIh",
        "outputId": "705575e4-59cc-4e88-9b79-7f05d79b0ced"
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "NoneType"
            ]
          },
          "metadata": {},
          "execution_count": 41
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "next_seq"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NEFL2aYGGsPj",
        "outputId": "a579ad49-4fee-4c54-fdfc-084e0925da15"
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[277,\n",
              " 243,\n",
              " 263,\n",
              " 132,\n",
              " 62,\n",
              " 164,\n",
              " 243,\n",
              " 166,\n",
              " 166,\n",
              " 166,\n",
              " 166,\n",
              " 166,\n",
              " 155,\n",
              " 123,\n",
              " 19,\n",
              " 135,\n",
              " 166,\n",
              " 166,\n",
              " 166,\n",
              " 153]"
            ]
          },
          "metadata": {},
          "execution_count": 40
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "# Assume 0 corresponds to <start> in your vocabulary\n",
        "start_token_id = 277\n",
        "\n",
        "# Begin with just the start token\n",
        "generated_sequence = [start_token_id]\n",
        "\n",
        "max_length = 20  # Set how long you want the output to be\n",
        "\n",
        "for _ in range(max_length):\n",
        "    # Convert to tensor: [1, seq_len]\n",
        "    token_tensor = torch.tensor([generated_sequence], dtype=torch.long)\n",
        "\n",
        "    # Get encoded state from transformer encoder\n",
        "    state_vector = agent.get_state(token_tensor)\n",
        "\n",
        "    # Choose next token (action)\n",
        "    next_token = agent.get_action(state_vector)\n",
        "\n",
        "    # Optionally stop if end token is produced (e.g., <end> = 1)\n",
        "    if next_token == 1:  # Assuming 1 is <end>\n",
        "        break\n",
        "\n",
        "    # Add predicted token to sequence\n",
        "    generated_sequence.append(next_token)\n",
        "\n",
        "# Decode tokens back to symbols if needed\n",
        "print(\"Generated token sequence:\", generated_sequence)\n"
      ],
      "metadata": {
        "id": "0_nsyGxUqT2o",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "012294e1-f961-4b2c-b71c-050a9df59d31"
      },
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generated token sequence: [277, 211, 275, 173, 233, 173, 173, 6, 173, 173, 49, 273, 173, 72, 173, 173, 68, 173, 173, 92, 173]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Having a look at my generated sequence"
      ],
      "metadata": {
        "id": "YigTDyRhJRg-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "named_sequence = [inv_vocab[token_id] for token_id in generated_sequence]"
      ],
      "metadata": {
        "id": "6oQQbslHJRKy"
      },
      "execution_count": 50,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "named_sequence"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EV8zfUJ5J7qs",
        "outputId": "69c7ad50-bcf0-4b5a-c016-a8bfc3dabd55"
      },
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['<song_start>',\n",
              " '<note_on_0_69_119>',\n",
              " '<program_change_52_>',\n",
              " '<note_on_0_64_116>',\n",
              " '<note_on_0_72_124>',\n",
              " '<note_on_0_64_116>',\n",
              " '<note_on_0_64_116>',\n",
              " '<control_change_0_10_84>',\n",
              " '<note_on_0_64_116>',\n",
              " '<note_on_0_64_116>',\n",
              " '<note_off_0_43_0>',\n",
              " '<note_on_0_79_127>',\n",
              " '<note_on_0_64_116>',\n",
              " '<note_off_0_67_0>',\n",
              " '<note_on_0_64_116>',\n",
              " '<note_on_0_64_116>',\n",
              " '<note_off_0_63_0>',\n",
              " '<note_on_0_64_116>',\n",
              " '<note_on_0_64_116>',\n",
              " '<note_on_0_47_127>',\n",
              " '<note_on_0_64_116>']"
            ]
          },
          "metadata": {},
          "execution_count": 51
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Agent (Snakes Game)"
      ],
      "metadata": {
        "id": "VDQsbHLKqRrv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import mido\n",
        "from mido import Message, MidiFile, MidiTrack\n",
        "import re\n",
        "from google.colab import files\n",
        "\n",
        "def tokens_to_midi(token_sequence, output_path='output.mid'):\n",
        "    # Create a new MIDI file with one track\n",
        "    midi = MidiFile()\n",
        "    track = MidiTrack()\n",
        "    midi.tracks.append(track)\n",
        "\n",
        "    # Process each token in sequence\n",
        "    for token in token_sequence:\n",
        "        # Skip song start token\n",
        "        if token == '<song_start>':\n",
        "            continue\n",
        "\n",
        "        # Extract event details using regex\n",
        "        match = re.match(r'<(.*?)_(.*?)>', token)\n",
        "        if not match:\n",
        "            continue\n",
        "\n",
        "        event_type = match.group(1)\n",
        "        params = match.group(2).split('_')\n",
        "\n",
        "        # Handle different event types\n",
        "        if event_type == 'note_on':\n",
        "            channel = int(params[0])\n",
        "            note = int(params[1])\n",
        "            velocity = int(params[2])\n",
        "            track.append(Message('note_on', channel=channel, note=note, velocity=velocity))\n",
        "\n",
        "        elif event_type == 'note_off':\n",
        "            channel = int(params[0])\n",
        "            note = int(params[1])\n",
        "            velocity = int(params[2])\n",
        "            track.append(Message('note_off', channel=channel, note=note, velocity=velocity))\n",
        "\n",
        "        elif event_type == 'control_change':\n",
        "            channel = int(params[0])\n",
        "            control = int(params[1])\n",
        "            value = int(params[2])\n",
        "            track.append(Message('control_change', channel=channel, control=control, value=value))\n",
        "\n",
        "        elif event_type == 'program_change':\n",
        "            program = int(params[0])\n",
        "            track.append(Message('program_change', program=program))\n",
        "\n",
        "        elif event_type == 'key_signature':\n",
        "            key = params[0]\n",
        "            # Key signature meta message\n",
        "            track.append(mido.MetaMessage('key_signature', key=key))\n",
        "\n",
        "    # Save the MIDI file\n",
        "    midi.save(output_path)\n",
        "    return midi\n",
        "\n",
        "# Example usage:\n",
        "\n",
        "midi = tokens_to_midi(named_sequence, 'output.mid')\n",
        "files.download('output.mid')"
      ],
      "metadata": {
        "id": "jrxxBMl1Dy_s",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "outputId": "49d67af4-23fa-48fc-8cf2-712094391be1"
      },
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_28760b9b-3a7c-446d-abd6-d105990c063b\", \"output.mid\", 26)"
            ]
          },
          "metadata": {}
        }
      ]
    }
  ]
}