{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[{"file_id":"1d6bBRDoTvLxIDyTKBjYZKPSqFuYp-wRw","timestamp":1751196007096}],"gpuType":"T4","mount_file_id":"1d6bBRDoTvLxIDyTKBjYZKPSqFuYp-wRw","authorship_tag":"ABX9TyNeUbP0NPf8vGb71RGP09zR"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU","widgets":{"application/vnd.jupyter.widget-state+json":{"b9ade97fb01f44ff8c399b3debf58bc9":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_9d03c836ffc54b72a60e1149937f41bc","IPY_MODEL_513eeb91af5748eb81772db9fb1f4657","IPY_MODEL_c696cb98b1664679aef3ff2f7363cc12"],"layout":"IPY_MODEL_962f08f334024d209124758f7c009c08"}},"9d03c836ffc54b72a60e1149937f41bc":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_b5535d13b82c42ebbadcdf18ad84255e","placeholder":"​","style":"IPY_MODEL_c9d910d738b44fc1b383dda91883c4da","value":"Epoch 9: 100%"}},"513eeb91af5748eb81772db9fb1f4657":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_51fd725399e8499295bcdc76247731ce","max":1,"min":0,"orientation":"horizontal","style":"IPY_MODEL_dae671d30b964f1790d9f72e0642dd0c","value":1}},"c696cb98b1664679aef3ff2f7363cc12":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_fd33c8ccdad44da887773ee4944d13af","placeholder":"​","style":"IPY_MODEL_d7eca84c8ea74017b14bb40ed30f5e06","value":" 1/1 [00:00&lt;00:00, 53.18it/s, v_num=7]"}},"962f08f334024d209124758f7c009c08":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":"inline-flex","flex":null,"flex_flow":"row wrap","grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":"100%"}},"b5535d13b82c42ebbadcdf18ad84255e":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"c9d910d738b44fc1b383dda91883c4da":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"51fd725399e8499295bcdc76247731ce":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":"2","flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"dae671d30b964f1790d9f72e0642dd0c":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"fd33c8ccdad44da887773ee4944d13af":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"d7eca84c8ea74017b14bb40ed30f5e06":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}}}}},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"6S8sPwt32hWe","executionInfo":{"status":"ok","timestamp":1750771711696,"user_tz":-60,"elapsed":6923,"user":{"displayName":"Naaza Zimba","userId":"04491752559292461421"}},"outputId":"526a0373-182e-4998-bbb6-8f3f1f6f6a0d"},"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: mido in /usr/local/lib/python3.11/dist-packages (1.3.3)\n","Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from mido) (24.2)\n"]}],"source":["!pip install mido"]},{"cell_type":"code","source":["# Data Exploration\n","\n","from mido import MidiFile\n","\n","# Load MIDI file\n","midi = MidiFile('/content/drive/MyDrive/Final Project Folder/Midi Files/monteverdi_libri_dei_madrigali_1_10_(c)icking-archive.mid')\n","\n","# Inspect the structure of the MIDI file\n","print(f\"Number of tracks: {len(midi.tracks)}\")\n","for i, track in enumerate(midi.tracks):\n","    print(f\"\\nTrack {i}: {track.name}\")\n","    for msg in track:\n","        print(msg)"],"metadata":{"id":"wekvtukf26gT"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import os\n","\n","folder_path = \"/content/drive/MyDrive/Final Project Folder/Midi Files\"\n","\n","# List all .mid or .midi files\n","midi_files = [f for f in os.listdir(folder_path) if f.lower().endswith(('.mid', '.midi'))]"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"p_qTef8B6F3n","executionInfo":{"status":"ok","timestamp":1750684179782,"user_tz":-60,"elapsed":30,"user":{"displayName":"Naaza Zimba","userId":"04491752559292461421"}},"outputId":"e9719faa-808f-4f6c-a3e7-40130e535d3e"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["monteverdi_libri_dei_madrigali_1_5_(c)icking-archive.mid\n","monteverdi_libri_dei_madrigali_1_10_(c)icking-archive.mid\n","monteverdi_libri_dei_madrigali_2_16_(c)icking-archive.mid\n","monteverdi_libri_dei_madrigali_3_9_(c)icking-archive.mid\n","monteverdi_libri_dei_madrigali_4_12_(c)icking-archive.mid\n","monteverdi_libri_dei_madrigali_4_13_(c)icking-archive.mid\n"]}]},{"cell_type":"code","source":["# Tracks 1, 2, 5, 6 (Same Weird Singing Instrument)\n","\n","# Tracks 4 Trumpet and Trombone\n","\n","# Track 3 (Nothing)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":53},"id":"2BxtBt1e6LtQ","executionInfo":{"status":"ok","timestamp":1750684307482,"user_tz":-60,"elapsed":7,"user":{"displayName":"Naaza Zimba","userId":"04491752559292461421"}},"outputId":"a8ab2354-29b1-4473-d9f0-110731f1e170"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["'/content/drive/MyDrive/Final Project Folder/Midi Files/monteverdi_libri_dei_madrigali_1_5_(c)icking-archive.mid'"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":11}]},{"cell_type":"code","source":["midi = MidiFile('/content/drive/MyDrive/Final Project Folder/Midi Files/' + midi_files[3])\n","\n","# Inspect the structure of the MIDI file\n","print(f\"Number of tracks: {len(midi.tracks)}\")\n","for i, track in enumerate(midi.tracks):\n","    print(f\"\\nTrack {i}: {track.name}\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"4RiJT-E45d7F","executionInfo":{"status":"ok","timestamp":1750684382553,"user_tz":-60,"elapsed":57,"user":{"displayName":"Naaza Zimba","userId":"04491752559292461421"}},"outputId":"69eaaa56-1c27-406b-a301-4446b21d8f19"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Number of tracks: 6\n","\n","Track 0: \n","\n","Track 1: Trumpet in C 1\n","\n","Track 2: Trumpet in C 2\n","\n","Track 3: Trombone\n","\n","Track 4: Bass Trombone\n","\n","Track 5: \n"]}]},{"cell_type":"code","source":["test_midis = ['monteverdi_libri_dei_madrigali_1_5_(c)icking-archive.mid',\n","              'monteverdi_libri_dei_madrigali_1_10_(c)icking-archive.mid',\n","              'monteverdi_libri_dei_madrigali_4_12_(c)icking-archive.mid',\n","              'monteverdi_libri_dei_madrigali_4_13_(c)icking-archive.mid']"],"metadata":{"id":"5rNa3gQK86uM"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import os\n","from mido import MidiFile\n","\n","input_folder = 'path/to/your/midi/folder'\n","output_folder = 'path/to/your/output/folder'\n","\n","# Make sure the output folder exists\n","os.makedirs(output_folder, exist_ok=True)\n","\n","def Midi_File_Input(mids):\n","    token = []\n","    for i, track in enumerate(mids.tracks):\n","        for msg in track:\n","            if msg.type == 'note_on':\n","                token.append(f\"<{msg.type}_{msg.channel}_{msg.note}_{msg.velocity}>\")\n","            elif msg.type == 'note_off':\n","                token.append(f\"<{msg.type}_{msg.channel}_{msg.note}_{msg.velocity}>\")\n","            elif msg.type == 'track_name':\n","                token.append(f\"<{msg.type}_{msg.name}>\")\n","            elif msg.type == 'control_change':\n","                token.append(f\"<{msg.type}_{msg.channel}_{msg.control}_{msg.value}>\")\n","            elif msg.type == 'program_change':\n","                token.append(f\"<{msg.type}_{msg.program}_>\")\n","            elif msg.type == 'key_signature':\n","                token.append(f\"<{msg.type}_{msg.key}>\")\n","    return token\n","\n","token_data = {}\n","all_tokens = []  # Flat list for training\n","\n","\n","# Process each MIDI file\n","for filename in test_midis:\n","    midi_path = '/content/drive/MyDrive/Final Project Folder/Midi Files/' + filename\n","\n","    midi = MidiFile(midi_path)\n","\n","    tokens = Midi_File_Input(midi)\n","\n","    # Add song_start and song_end markers\n","    song_tokens = ['<song_start>'] + tokens + ['<song_end>']\n","\n","    # Store in dictionary (per-song)\n","    token_data[filename] = song_tokens\n","\n","    # Append to flat list (for generative model training)\n","    all_tokens.extend(song_tokens)\n"],"metadata":{"id":"UfJo7w0N_LwE"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Token Data Length analysis"],"metadata":{"id":"Q70i7s3iwfuY"}},{"cell_type":"code","source":["token_data_length = []\n","\n","for song_names in test_midis:\n","  token_length_size = len(token_data[song_names])\n","  token_data_length.append(token_length_size)\n"],"metadata":{"id":"vWv6tJJcvRH-"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["token_data_length #[3212, 2739, 4322, 3837] These are our lengths of our token data so we are going to create a 500 cut off padding only one of the"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"XtrSaXhFw5sD","executionInfo":{"status":"ok","timestamp":1750765645814,"user_tz":-60,"elapsed":8,"user":{"displayName":"Naaza Zimba","userId":"04491752559292461421"}},"outputId":"c7ab05b7-89bd-4be2-c97b-218a37bdbb0e"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["[3212, 2739, 4322, 3837]"]},"metadata":{},"execution_count":18}]},{"cell_type":"markdown","source":["Vocab and Numerical Index Creation"],"metadata":{"id":"jjvUqz53wWT7"}},{"cell_type":"code","source":["from collections import Counter\n","\n","# Flatten all tokens across all songs\n","all_tokens_flat = [token for tokens in token_data.values() for token in tokens]\n","\n","# Count and sort tokens (optional for ordering)\n","token_freq = Counter(all_tokens_flat)\n","\n","# Assign token ID\n","vocab = {token: idx for idx, token in enumerate(sorted(token_freq))}\n","\n","# Optionally store reverse map too:\n","inv_vocab = {idx: token for token, idx in vocab.items()}\n"],"metadata":{"id":"w8P_EDwFhWWt"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["token_ids_data = {}\n","\n","for filename, tokens in token_data.items():\n","    token_ids = [vocab[token] for token in tokens]\n","    token_ids_data[filename] = token_ids"],"metadata":{"id":"9nDHtUnhksza"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["vocab[\"<pad>\"] = 279"],"metadata":{"id":"qO2MdaP6yET-"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# # vocab\n","\n","# vocab[\"<song_start>\"]\n","\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"PxBT2BSLnwfT","executionInfo":{"status":"ok","timestamp":1750696229945,"user_tz":-60,"elapsed":47,"user":{"displayName":"Naaza Zimba","userId":"04491752559292461421"}},"outputId":"17ec3be5-1bb0-4ce5-8ceb-596fa9199711"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["277"]},"metadata":{},"execution_count":36}]},{"cell_type":"markdown","source":["Transformer Building"],"metadata":{"id":"Jz4yvUsrZvfk"}},{"cell_type":"code","source":["# !pip install lightning\n","\n","#!pip install pytorch_lightning"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"AQBHA2sJAHiv","executionInfo":{"status":"ok","timestamp":1750769766831,"user_tz":-60,"elapsed":4370,"user":{"displayName":"Naaza Zimba","userId":"04491752559292461421"}},"outputId":"d2c17820-9dd9-46ab-9283-9a28dfba2d42"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: pytorch_lightning in /usr/local/lib/python3.11/dist-packages (2.5.2)\n","Requirement already satisfied: torch>=2.1.0 in /usr/local/lib/python3.11/dist-packages (from pytorch_lightning) (2.6.0+cu124)\n","Requirement already satisfied: tqdm>=4.57.0 in /usr/local/lib/python3.11/dist-packages (from pytorch_lightning) (4.67.1)\n","Requirement already satisfied: PyYAML>=5.4 in /usr/local/lib/python3.11/dist-packages (from pytorch_lightning) (6.0.2)\n","Requirement already satisfied: fsspec>=2022.5.0 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]>=2022.5.0->pytorch_lightning) (2025.3.2)\n","Requirement already satisfied: torchmetrics>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from pytorch_lightning) (1.7.3)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from pytorch_lightning) (24.2)\n","Requirement already satisfied: typing-extensions>=4.4.0 in /usr/local/lib/python3.11/dist-packages (from pytorch_lightning) (4.14.0)\n","Requirement already satisfied: lightning-utilities>=0.10.0 in /usr/local/lib/python3.11/dist-packages (from pytorch_lightning) (0.14.3)\n","Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]>=2022.5.0->pytorch_lightning) (3.11.15)\n","Requirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from lightning-utilities>=0.10.0->pytorch_lightning) (75.2.0)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch>=2.1.0->pytorch_lightning) (3.18.0)\n","Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=2.1.0->pytorch_lightning) (3.5)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=2.1.0->pytorch_lightning) (3.1.6)\n","Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.1.0->pytorch_lightning) (12.4.127)\n","Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.1.0->pytorch_lightning) (12.4.127)\n","Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.1.0->pytorch_lightning) (12.4.127)\n","Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch>=2.1.0->pytorch_lightning) (9.1.0.70)\n","Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch>=2.1.0->pytorch_lightning) (12.4.5.8)\n","Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch>=2.1.0->pytorch_lightning) (11.2.1.3)\n","Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch>=2.1.0->pytorch_lightning) (10.3.5.147)\n","Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch>=2.1.0->pytorch_lightning) (11.6.1.9)\n","Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch>=2.1.0->pytorch_lightning) (12.3.1.170)\n","Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch>=2.1.0->pytorch_lightning) (0.6.2)\n","Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>=2.1.0->pytorch_lightning) (2.21.5)\n","Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.1.0->pytorch_lightning) (12.4.127)\n","Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.1.0->pytorch_lightning) (12.4.127)\n","Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch>=2.1.0->pytorch_lightning) (3.2.0)\n","Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch>=2.1.0->pytorch_lightning) (1.13.1)\n","Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch>=2.1.0->pytorch_lightning) (1.3.0)\n","Requirement already satisfied: numpy>1.20.0 in /usr/local/lib/python3.11/dist-packages (from torchmetrics>=0.7.0->pytorch_lightning) (2.0.2)\n","Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch_lightning) (2.6.1)\n","Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch_lightning) (1.3.2)\n","Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch_lightning) (25.3.0)\n","Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch_lightning) (1.7.0)\n","Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch_lightning) (6.4.4)\n","Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch_lightning) (0.3.2)\n","Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch_lightning) (1.20.1)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=2.1.0->pytorch_lightning) (3.0.2)\n","Requirement already satisfied: idna>=2.0 in /usr/local/lib/python3.11/dist-packages (from yarl<2.0,>=1.17.0->aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch_lightning) (3.10)\n"]}]},{"cell_type":"code","source":["import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","\n","from torch.optim import Adam\n","from torch.utils.data import TensorDataset, DataLoader\n","\n","import lightning as L\n","from pytorch_lightning import Trainer\n"],"metadata":{"id":"LfT16A1AZzNg"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"IYHUb1z8AtXK"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["inputs = torch.tensor([vocab[\"<song_start>\"]])\n","\n","outputs = torch.tensor([vocab[\"<song_end>\"]])"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"uHtPmS6ekwe2","executionInfo":{"status":"ok","timestamp":1750695838746,"user_tz":-60,"elapsed":12,"user":{"displayName":"Naaza Zimba","userId":"04491752559292461421"}},"outputId":"216831ef-9ea2-4c46-93a5-a1af2a4fab6f"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["{'<control_change_0_100_0>': 0,\n"," '<control_change_0_101_0>': 1,\n"," '<control_change_0_10_104>': 2,\n"," '<control_change_0_10_24>': 3,\n"," '<control_change_0_10_44>': 4,\n"," '<control_change_0_10_64>': 5,\n"," '<control_change_0_10_84>': 6,\n"," '<control_change_0_12_3>': 7,\n"," '<control_change_0_38_0>': 8,\n"," '<control_change_0_6_12>': 9,\n"," '<control_change_0_7_100>': 10,\n"," '<control_change_0_7_101>': 11,\n"," '<control_change_0_7_102>': 12,\n"," '<control_change_0_7_103>': 13,\n"," '<control_change_0_7_104>': 14,\n"," '<control_change_0_7_105>': 15,\n"," '<control_change_0_7_106>': 16,\n"," '<control_change_0_7_107>': 17,\n"," '<control_change_0_7_108>': 18,\n"," '<control_change_0_7_109>': 19,\n"," '<control_change_0_7_110>': 20,\n"," '<control_change_0_7_111>': 21,\n"," '<control_change_0_7_112>': 22,\n"," '<control_change_0_7_113>': 23,\n"," '<control_change_0_7_114>': 24,\n"," '<control_change_0_7_115>': 25,\n"," '<control_change_0_7_116>': 26,\n"," '<control_change_0_7_117>': 27,\n"," '<control_change_0_7_118>': 28,\n"," '<control_change_0_7_83>': 29,\n"," '<control_change_0_7_84>': 30,\n"," '<control_change_0_7_85>': 31,\n"," '<control_change_0_7_86>': 32,\n"," '<control_change_0_7_87>': 33,\n"," '<control_change_0_7_88>': 34,\n"," '<control_change_0_7_89>': 35,\n"," '<control_change_0_7_90>': 36,\n"," '<control_change_0_7_91>': 37,\n"," '<control_change_0_7_92>': 38,\n"," '<control_change_0_7_93>': 39,\n"," '<control_change_0_7_94>': 40,\n"," '<control_change_0_7_95>': 41,\n"," '<control_change_0_7_96>': 42,\n"," '<control_change_0_7_97>': 43,\n"," '<control_change_0_7_98>': 44,\n"," '<control_change_0_7_99>': 45,\n"," '<control_change_0_91_76>': 46,\n"," '<key_signature_Am>': 47,\n"," '<key_signature_Dm>': 48,\n"," '<note_off_0_43_0>': 49,\n"," '<note_off_0_45_0>': 50,\n"," '<note_off_0_46_0>': 51,\n"," '<note_off_0_47_0>': 52,\n"," '<note_off_0_48_0>': 53,\n"," '<note_off_0_49_0>': 54,\n"," '<note_off_0_50_0>': 55,\n"," '<note_off_0_51_0>': 56,\n"," '<note_off_0_52_0>': 57,\n"," '<note_off_0_53_0>': 58,\n"," '<note_off_0_54_0>': 59,\n"," '<note_off_0_55_0>': 60,\n"," '<note_off_0_56_0>': 61,\n"," '<note_off_0_57_0>': 62,\n"," '<note_off_0_58_0>': 63,\n"," '<note_off_0_59_0>': 64,\n"," '<note_off_0_60_0>': 65,\n"," '<note_off_0_61_0>': 66,\n"," '<note_off_0_62_0>': 67,\n"," '<note_off_0_63_0>': 68,\n"," '<note_off_0_64_0>': 69,\n"," '<note_off_0_65_0>': 70,\n"," '<note_off_0_66_0>': 71,\n"," '<note_off_0_67_0>': 72,\n"," '<note_off_0_68_0>': 73,\n"," '<note_off_0_69_0>': 74,\n"," '<note_off_0_70_0>': 75,\n"," '<note_off_0_71_0>': 76,\n"," '<note_off_0_72_0>': 77,\n"," '<note_off_0_73_0>': 78,\n"," '<note_off_0_74_0>': 79,\n"," '<note_off_0_75_0>': 80,\n"," '<note_off_0_76_0>': 81,\n"," '<note_off_0_77_0>': 82,\n"," '<note_off_0_78_0>': 83,\n"," '<note_off_0_79_0>': 84,\n"," '<note_off_0_81_0>': 85,\n"," '<note_on_0_43_125>': 86,\n"," '<note_on_0_45_113>': 87,\n"," '<note_on_0_45_120>': 88,\n"," '<note_on_0_45_125>': 89,\n"," '<note_on_0_46_127>': 90,\n"," '<note_on_0_47_125>': 91,\n"," '<note_on_0_47_127>': 92,\n"," '<note_on_0_48_124>': 93,\n"," '<note_on_0_48_125>': 94,\n"," '<note_on_0_48_126>': 95,\n"," '<note_on_0_48_127>': 96,\n"," '<note_on_0_49_125>': 97,\n"," '<note_on_0_50_120>': 98,\n"," '<note_on_0_50_124>': 99,\n"," '<note_on_0_50_125>': 100,\n"," '<note_on_0_50_126>': 101,\n"," '<note_on_0_50_127>': 102,\n"," '<note_on_0_51_126>': 103,\n"," '<note_on_0_51_127>': 104,\n"," '<note_on_0_52_119>': 105,\n"," '<note_on_0_52_120>': 106,\n"," '<note_on_0_52_124>': 107,\n"," '<note_on_0_52_125>': 108,\n"," '<note_on_0_52_126>': 109,\n"," '<note_on_0_52_127>': 110,\n"," '<note_on_0_53_114>': 111,\n"," '<note_on_0_53_124>': 112,\n"," '<note_on_0_53_125>': 113,\n"," '<note_on_0_53_126>': 114,\n"," '<note_on_0_53_127>': 115,\n"," '<note_on_0_54_120>': 116,\n"," '<note_on_0_54_124>': 117,\n"," '<note_on_0_54_125>': 118,\n"," '<note_on_0_54_127>': 119,\n"," '<note_on_0_55_113>': 120,\n"," '<note_on_0_55_119>': 121,\n"," '<note_on_0_55_120>': 122,\n"," '<note_on_0_55_124>': 123,\n"," '<note_on_0_55_125>': 124,\n"," '<note_on_0_55_126>': 125,\n"," '<note_on_0_55_127>': 126,\n"," '<note_on_0_56_125>': 127,\n"," '<note_on_0_57_113>': 128,\n"," '<note_on_0_57_116>': 129,\n"," '<note_on_0_57_118>': 130,\n"," '<note_on_0_57_120>': 131,\n"," '<note_on_0_57_124>': 132,\n"," '<note_on_0_57_125>': 133,\n"," '<note_on_0_57_126>': 134,\n"," '<note_on_0_57_127>': 135,\n"," '<note_on_0_58_113>': 136,\n"," '<note_on_0_58_114>': 137,\n"," '<note_on_0_58_125>': 138,\n"," '<note_on_0_58_126>': 139,\n"," '<note_on_0_58_127>': 140,\n"," '<note_on_0_59_113>': 141,\n"," '<note_on_0_59_116>': 142,\n"," '<note_on_0_59_120>': 143,\n"," '<note_on_0_59_124>': 144,\n"," '<note_on_0_59_125>': 145,\n"," '<note_on_0_59_126>': 146,\n"," '<note_on_0_59_127>': 147,\n"," '<note_on_0_60_113>': 148,\n"," '<note_on_0_60_114>': 149,\n"," '<note_on_0_60_116>': 150,\n"," '<note_on_0_60_118>': 151,\n"," '<note_on_0_60_120>': 152,\n"," '<note_on_0_60_124>': 153,\n"," '<note_on_0_60_125>': 154,\n"," '<note_on_0_60_126>': 155,\n"," '<note_on_0_60_127>': 156,\n"," '<note_on_0_61_120>': 157,\n"," '<note_on_0_61_121>': 158,\n"," '<note_on_0_61_125>': 159,\n"," '<note_on_0_61_126>': 160,\n"," '<note_on_0_61_127>': 161,\n"," '<note_on_0_62_113>': 162,\n"," '<note_on_0_62_116>': 163,\n"," '<note_on_0_62_120>': 164,\n"," '<note_on_0_62_125>': 165,\n"," '<note_on_0_62_126>': 166,\n"," '<note_on_0_62_127>': 167,\n"," '<note_on_0_63_120>': 168,\n"," '<note_on_0_63_125>': 169,\n"," '<note_on_0_63_126>': 170,\n"," '<note_on_0_63_127>': 171,\n"," '<note_on_0_64_113>': 172,\n"," '<note_on_0_64_116>': 173,\n"," '<note_on_0_64_118>': 174,\n"," '<note_on_0_64_120>': 175,\n"," '<note_on_0_64_121>': 176,\n"," '<note_on_0_64_124>': 177,\n"," '<note_on_0_64_125>': 178,\n"," '<note_on_0_64_126>': 179,\n"," '<note_on_0_64_127>': 180,\n"," '<note_on_0_65_113>': 181,\n"," '<note_on_0_65_114>': 182,\n"," '<note_on_0_65_116>': 183,\n"," '<note_on_0_65_118>': 184,\n"," '<note_on_0_65_120>': 185,\n"," '<note_on_0_65_125>': 186,\n"," '<note_on_0_65_126>': 187,\n"," '<note_on_0_65_127>': 188,\n"," '<note_on_0_66_120>': 189,\n"," '<note_on_0_66_121>': 190,\n"," '<note_on_0_66_124>': 191,\n"," '<note_on_0_66_125>': 192,\n"," '<note_on_0_66_126>': 193,\n"," '<note_on_0_66_127>': 194,\n"," '<note_on_0_67_113>': 195,\n"," '<note_on_0_67_114>': 196,\n"," '<note_on_0_67_116>': 197,\n"," '<note_on_0_67_119>': 198,\n"," '<note_on_0_67_120>': 199,\n"," '<note_on_0_67_124>': 200,\n"," '<note_on_0_67_125>': 201,\n"," '<note_on_0_67_126>': 202,\n"," '<note_on_0_67_127>': 203,\n"," '<note_on_0_68_120>': 204,\n"," '<note_on_0_68_125>': 205,\n"," '<note_on_0_68_126>': 206,\n"," '<note_on_0_68_127>': 207,\n"," '<note_on_0_69_113>': 208,\n"," '<note_on_0_69_116>': 209,\n"," '<note_on_0_69_118>': 210,\n"," '<note_on_0_69_119>': 211,\n"," '<note_on_0_69_120>': 212,\n"," '<note_on_0_69_124>': 213,\n"," '<note_on_0_69_125>': 214,\n"," '<note_on_0_69_126>': 215,\n"," '<note_on_0_69_127>': 216,\n"," '<note_on_0_70_113>': 217,\n"," '<note_on_0_70_114>': 218,\n"," '<note_on_0_70_125>': 219,\n"," '<note_on_0_70_126>': 220,\n"," '<note_on_0_70_127>': 221,\n"," '<note_on_0_71_116>': 222,\n"," '<note_on_0_71_120>': 223,\n"," '<note_on_0_71_124>': 224,\n"," '<note_on_0_71_125>': 225,\n"," '<note_on_0_71_126>': 226,\n"," '<note_on_0_71_127>': 227,\n"," '<note_on_0_72_113>': 228,\n"," '<note_on_0_72_114>': 229,\n"," '<note_on_0_72_116>': 230,\n"," '<note_on_0_72_119>': 231,\n"," '<note_on_0_72_120>': 232,\n"," '<note_on_0_72_124>': 233,\n"," '<note_on_0_72_125>': 234,\n"," '<note_on_0_72_126>': 235,\n"," '<note_on_0_72_127>': 236,\n"," '<note_on_0_73_125>': 237,\n"," '<note_on_0_73_126>': 238,\n"," '<note_on_0_73_127>': 239,\n"," '<note_on_0_74_113>': 240,\n"," '<note_on_0_74_114>': 241,\n"," '<note_on_0_74_116>': 242,\n"," '<note_on_0_74_118>': 243,\n"," '<note_on_0_74_120>': 244,\n"," '<note_on_0_74_124>': 245,\n"," '<note_on_0_74_125>': 246,\n"," '<note_on_0_74_126>': 247,\n"," '<note_on_0_74_127>': 248,\n"," '<note_on_0_75_120>': 249,\n"," '<note_on_0_75_121>': 250,\n"," '<note_on_0_75_125>': 251,\n"," '<note_on_0_75_126>': 252,\n"," '<note_on_0_75_127>': 253,\n"," '<note_on_0_76_113>': 254,\n"," '<note_on_0_76_114>': 255,\n"," '<note_on_0_76_120>': 256,\n"," '<note_on_0_76_121>': 257,\n"," '<note_on_0_76_124>': 258,\n"," '<note_on_0_76_125>': 259,\n"," '<note_on_0_76_126>': 260,\n"," '<note_on_0_76_127>': 261,\n"," '<note_on_0_77_113>': 262,\n"," '<note_on_0_77_114>': 263,\n"," '<note_on_0_77_125>': 264,\n"," '<note_on_0_77_126>': 265,\n"," '<note_on_0_77_127>': 266,\n"," '<note_on_0_78_120>': 267,\n"," '<note_on_0_78_125>': 268,\n"," '<note_on_0_78_127>': 269,\n"," '<note_on_0_79_120>': 270,\n"," '<note_on_0_79_125>': 271,\n"," '<note_on_0_79_126>': 272,\n"," '<note_on_0_79_127>': 273,\n"," '<note_on_0_81_125>': 274,\n"," '<program_change_52_>': 275,\n"," '<song_end>': 276,\n"," '<song_start>': 277,\n"," '<track_name_Instrument 1>': 278}"]},"metadata":{},"execution_count":33}]},{"cell_type":"markdown","source":["Inputs"],"metadata":{"id":"IfDFBXLeqjLr"}},{"cell_type":"code","source":["# dataset = TensorDataset(inputs, outputs)\n","# dataloader = DataLoader(dataset, batch_size=1)"],"metadata":{"id":"9Cd5ksJpqFGZ"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Word Embedding"],"metadata":{"id":"xeDOYhHfqkoR"}},{"cell_type":"code","source":[],"metadata":{"id":"LIBc1YLC_fQ2"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Imports"],"metadata":{"id":"FtrIuxb9IW_P"}},{"cell_type":"code","source":["import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","\n","from torch.optim import Adam\n","from torch.utils.data import TensorDataset, DataLoader\n","\n","#import lightning as L\n","from pytorch_lightning import Trainer, LightningModule\n"],"metadata":{"id":"7fmAm1pgIWhh"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Positional Encoding"],"metadata":{"id":"KtUKqGUAqpbZ"}},{"cell_type":"code","source":["class PositionalEncoding(nn.Module):\n","  def __init__(self,d_model, max_length):\n","\n","    super().__init__()\n","\n","    pe = torch.zeros(max_length, d_model) #Here we start by creating an empty matrix which will get updated.\n","\n","  #d_model is the number of word embedding dimensions the larger this is the richer our description of a word\n","  #The actual word embedding values are updated via back propagation\n","\n","    position = torch.arange(start = 0, end = max_length, step = 1).float().unsqueeze(1)\n","\n","    embedding_index = torch.arange(start=0, end=d_model,step=2).float()\n","\n","    div_term = 1/torch.tensor(10000.0)**(embedding_index/ d_model)\n","\n","\n","    #Our Positional encoding starts with a sin and cos equation.\n","  #Have a look at the literture to check why we do this but it's not too difficult to understand\n","\n","    pe[:, 0::2] = torch.sin(position * div_term) #This updates the first column\n","    pe[:, 1::2] = torch.cos(position * div_term) # This updates the second column\n","\n","  #This happens in an alternating method where the first column and the secondary column gets updated\n","\n","    self.register_buffer('pe', pe.unsqueeze(0))\n","\n","  def forward (self, word_embeddings):\n","\n","    seq_len = word_embeddings.size(1)\n","    return word_embeddings + self.pe[:, :seq_len, :]\n","\n","    #return word_embeddings + self.pe[:word_embeddings.size(0), :] #Finally we add the positonal encoding values to the word embedding values"],"metadata":{"id":"5zBSKZ-rqqwO"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Masked Self Attention Value"],"metadata":{"id":"GssBtoxmBVFE"}},{"cell_type":"code","source":["class Attention(nn.Module):\n","  def __init__(self, d_model):\n","\n","#Here we pass in the number of word embedding values per token This is for the sake of matrix nultiplication\n","#When we do our matrix multiplication to create the queries, keys and values.\n","\n","    super().__init__()\n","\n","    self.W_q = nn.Linear(in_features = d_model, out_features = d_model, bias = False)\n","    self.W_k = nn.Linear(in_features = d_model, out_features = d_model, bias = False)\n","    self.W_v = nn.Linear(in_features = d_model, out_features = d_model, bias = False)\n","\n","    #Above we have our weight matrix which are used to calculate our Query, Keys and Values\n","    #Finally, we don't include a bias term here when calculating attention which is why we set the bias to False\n","\n","    self.row_dim = 0\n","    self.col_dim = 1 #To give us flexibility to input data sequentiallly into batches we create variables to do so\n","\n","  def forward(self, encodings_for_q, encodings_for_k, encodings_for_v, mask = None):\n","\n","      #The forward method is where we use the masked self attention values for each token to be calculated\n","      #Next we give this forward method the flexbility\n","      #Finally since we want to do masked self attention we can pass in a mask too.\n","\n","    q = self.W_q(encodings_for_q)\n","    k = self.W_k(encodings_for_k)\n","    v = self.W_v(encodings_for_v)\n","\n","    #sims = torch.matmul(q, k.transpose(dim0 = self.row_dim, dim1 = self.col_dim))\n","\n","    #scaled_sims = sims / torch.tensor(k.size(self.col_dim)**0.5)\n","\n","    sims = torch.matmul(q, k.transpose(-2, -1))  # [batch_size, seq_len, seq_len]\n","\n","    scaled_sims = sims / (k.size(-1) ** 0.5)\n","\n","    if mask is not None:\n","      scaled_sims = scaled_sims.masked_fill(mask = mask, value=-1e9) #Here we include our mask values which stops the attention from looking ahead to caluclate weights.\n","\n","    attention_percents = F.softmax(scaled_sims, dim = self.col_dim)\n","\n","    attention_scores = torch.matmul(attention_percents, v)\n","\n","    return attention_scores"],"metadata":{"id":"wu-Iq-mcBTWu"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Residual Connection"],"metadata":{"id":"qqRJU5uGZ2Aq"}},{"cell_type":"code","source":["import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","\n","from torch.optim import Adam\n","from torch.utils.data import TensorDataset, DataLoader\n","\n","#from pytorch_lightning import\n","from pytorch_lightning import Trainer, LightningModule\n","\n","\n","class DecodeOTrans(LightningModule):\n","  def __init__(self, d_model, max_length, num_tokens = 100):\n","\n","    #Here we specify the number of tokens available in the vocabulary,\n","    #Number of word embeddings per token\n","    #Max token length\n","    super().__init__()\n","\n","    self.we = nn.Embedding(num_embeddings = num_tokens, embedding_dim = d_model) #We creating a word embedding value\n","    #Embeddings needs to know how many tokens are in the vocabulary and understand the dimension size to represent the embedding\n","\n","    self.pe = PositionalEncoding(d_model = d_model, max_length = max_length)\n","\n","    #Then we create a positional encoding object using the class we created earlier\n","\n","    self.self_attention = Attention(d_model = d_model)\n","\n","    self.fc_layer = nn.Linear(in_features = d_model, out_features = num_tokens) #This is our fully connected layer also know as our Dense Layer of neurons (RNN neural networks)\n","\n","    self.loss = nn.CrossEntropyLoss()\n","\n","  def forward(self, token_ids):\n","\n","    word_embeddings = self.we(token_ids)  # [batch_size, seq_len, d_model]\n","    positional_encoding = self.pe(word_embeddings)  # add positional encoding\n","\n","    seq_len = token_ids.size(1)\n","    #mask = torch.tril(torch.ones((seq_len, seq_len), device=token_ids.device)) == 0\n","\n","    mask = torch.tril(torch.ones((seq_len, seq_len), device=token_ids.device)).unsqueeze(0) == 0\n","\n","    self_attention_values = self.self_attention(\n","        positional_encoding,\n","        positional_encoding,\n","        positional_encoding,\n","        mask=mask\n","    )\n","\n","    residual_connection_value = positional_encoding + self_attention_values\n","    fc_layer_output = self.fc_layer(residual_connection_value)\n","\n","    return fc_layer_output\n","\n","\n","  def configure_optimizers(self):\n","    return Adam(self.parameters(), lr = 1e-4) #Here we are using an Adam Optimiser to train all our parmeters with a learning rate of 0.1 (This is a fast learning rate)\n","\n","  def training_step(self, batch, batch_idx):\n","    input_tokens, labels = batch  # input: [batch_size, seq_len], labels: [batch_size, seq_len]\n","\n","    logits = self.forward(input_tokens)  # output: [batch_size, seq_len, vocab_size]\n","\n","    # Flatten for CrossEntropyLoss: expects [N, C] and [N]\n","    loss = self.loss(\n","        logits.view(-1, logits.size(-1)),  # [batch_size * seq_len, vocab_size]\n","        labels.view(-1)                    # [batch_size * seq_len]\n","    )\n","\n","    #self.log(\"train_loss\", loss)\n","    return loss\n"],"metadata":{"id":"mHVD0lIOZ3nV"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Traning Our Model From Video"],"metadata":{"id":"oQxpYQW1jDBH"}},{"cell_type":"code","source":["# def configure_optimizers(self):\n","#   return Adam(self.parameters(), lr = 0.1) #Here we are using an Adam Optimiser to train all our parmeters with a learning rate of 0.1 (This is a fast learning rate)\n","\n","# def training_step(self, batch, batch_idx):\n","\n","#   input_tokens, labels = batch #Next, we split the training data into inputs and labels\n","\n","#   output = self.forward(input_tokens[0]) #Then we pass our input tokens into the forward class to get our output\n","\n","#   loss = self.loss(output, labels[0]) #Next we compare the output to the known labels into our loss function to minimise\n","#   #Thhis does the softmax for us.\n","\n","#   return loss"],"metadata":{"id":"Zq4lO-sQjCo3"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Training Model From GPT"],"metadata":{"id":"RoEN-06T7a_n"}},{"cell_type":"code","source":["\n","\n","#We want <start_token>, .... ,<end_token>\n","\n","# <start_token> ..... <pad>, <end_token>"],"metadata":{"id":"BbVf8wC87akO"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["max_length = 100\n","d_model = 128\n","max_token_count = max_length - 2  # for <song_start> and <song_end>\n","pad_token_id = vocab[\"<pad>\"]\n","song_start_id = vocab[\"<song_start>\"]\n","song_end_id = vocab[\"<song_end>\"]\n","\n","input_sequences = []\n","target_sequences = []\n","\n","for tokens in token_data.values():\n","    # Truncate the tokens to fit within max_token_count\n","    midi_tokens = all_tokens[:max_token_count]\n","\n","    # Full token sequence with special tokens\n","    full_tokens = [song_start_id] + [vocab[t] for t in midi_tokens] + [song_end_id]\n","\n","    # Prepare input and target\n","    input_ids = full_tokens[:-1]\n","    target_ids = full_tokens[1:]\n","\n","    # Calculate padding length\n","    pad_len = max_length - len(input_ids)\n","\n","    # Pad at the end AFTER <song_end>\n","    input_ids += [song_end_id] * pad_len\n","    target_ids += [song_end_id] * pad_len\n","\n","    input_sequences.append(input_ids)\n","    target_sequences.append(target_ids)\n"],"metadata":{"id":"9KXFZBbw78S2"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Convert to tensors\n","inputs_tensor = torch.tensor(input_sequences, dtype=torch.long)\n","targets_tensor = torch.tensor(target_sequences, dtype=torch.long)\n","\n","dataset = TensorDataset(inputs_tensor, targets_tensor)\n","dataloader = DataLoader(dataset, batch_size=32, shuffle=True)"],"metadata":{"id":"3jGv4WGZ-dbW"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import torch.nn.functional as F\n","\n","# def training_step(self, batch, batch_idx):\n","#     input_tokens, target_tokens = batch  # Shape: (batch_size, seq_len)\n","\n","#     logits = self(input_tokens)  # Shape: (batch_size, seq_len, vocab_size)\n","\n","#     # Reshape for loss: flatten batch and seq dims\n","#     logits = logits.view(-1, logits.size(-1))             # (batch * seq_len, vocab_size)\n","#     target_tokens = target_tokens.view(-1)                # (batch * seq_len)\n","\n","#     loss = self.loss(logits, target_tokens)\n","#     self.log(\"train_loss\", loss)\n","#     return loss\n","\n","# def configure_optimizers(self):\n","#     return torch.optim.Adam(self.parameters(), lr=1e-4)\n","\n","model = DecodeOTrans(\n","    num_tokens=len(vocab),\n","    d_model=128,        # or 256 etc.\n","    max_length=100,\n",")\n","\n","DecodeOTrans\n","trainer = Trainer(\n","    max_epochs=10,\n","    accelerator=\"auto\",\n",")\n","\n","trainer.fit(model, dataloader)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":462,"referenced_widgets":["b9ade97fb01f44ff8c399b3debf58bc9","9d03c836ffc54b72a60e1149937f41bc","513eeb91af5748eb81772db9fb1f4657","c696cb98b1664679aef3ff2f7363cc12","962f08f334024d209124758f7c009c08","b5535d13b82c42ebbadcdf18ad84255e","c9d910d738b44fc1b383dda91883c4da","51fd725399e8499295bcdc76247731ce","dae671d30b964f1790d9f72e0642dd0c","fd33c8ccdad44da887773ee4944d13af","d7eca84c8ea74017b14bb40ed30f5e06"]},"id":"7wzrim5v-l7y","executionInfo":{"status":"ok","timestamp":1750773603777,"user_tz":-60,"elapsed":329,"user":{"displayName":"Naaza Zimba","userId":"04491752559292461421"}},"outputId":"d93abc1e-0d09-486e-f1a7-1ce43b66181f"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["INFO:pytorch_lightning.utilities.rank_zero:💡 Tip: For seamless cloud uploads and versioning, try installing [litmodels](https://pypi.org/project/litmodels/) to enable LitModelCheckpoint, which syncs automatically with the Lightning model registry.\n","INFO:pytorch_lightning.utilities.rank_zero:GPU available: True (cuda), used: True\n","INFO:pytorch_lightning.utilities.rank_zero:TPU available: False, using: 0 TPU cores\n","INFO:pytorch_lightning.utilities.rank_zero:HPU available: False, using: 0 HPUs\n","INFO:pytorch_lightning.accelerators.cuda:LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n","INFO:pytorch_lightning.callbacks.model_summary:\n","  | Name           | Type               | Params | Mode \n","--------------------------------------------------------------\n","0 | we             | Embedding          | 35.8 K | train\n","1 | pe             | PositionalEncoding | 0      | train\n","2 | self_attention | Attention          | 49.2 K | train\n","3 | fc_layer       | Linear             | 36.1 K | train\n","4 | loss           | CrossEntropyLoss   | 0      | train\n","--------------------------------------------------------------\n","121 K     Trainable params\n","0         Non-trainable params\n","121 K     Total params\n","0.484     Total estimated model params size (MB)\n","8         Modules in train mode\n","0         Modules in eval mode\n","/usr/local/lib/python3.11/dist-packages/pytorch_lightning/loops/fit_loop.py:310: The number of training batches (1) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n"]},{"output_type":"display_data","data":{"text/plain":["Training: |          | 0/? [00:00<?, ?it/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b9ade97fb01f44ff8c399b3debf58bc9"}},"metadata":{}},{"output_type":"stream","name":"stderr","text":["INFO:pytorch_lightning.utilities.rank_zero:`Trainer.fit` stopped: `max_epochs=10` reached.\n"]}]},{"cell_type":"code","source":["max_gen_len = 500\n","start_token_id = vocab[\"<song_start>\"]\n","end_token_id = vocab[\"<song_end>\"]\n","\n","import torch\n","\n","# Start with just <song_start>\n","generated = [start_token_id]\n","\n","for _ in range(max_gen_len):\n","    input_tensor = torch.tensor(generated).unsqueeze(0)  # shape: (1, seq_len)\n","    with torch.no_grad():\n","        logits = model(input_tensor)  # shape: (1, seq_len, vocab_size)\n","\n","    # Get the logits for the last token position\n","    next_token_logits = logits[0, -1, :]  # shape: (vocab_size,)\n","    next_token = torch.argmax(next_token_logits).item()\n","\n","    # Append the new token\n","    generated.append(next_token)\n","\n","    if next_token == end_token_id:\n","        break\n"],"metadata":{"id":"pslfDbDL-pf4"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["vocab['<song_end>']"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"98h1r3oDOsEQ","executionInfo":{"status":"ok","timestamp":1750773476332,"user_tz":-60,"elapsed":10,"user":{"displayName":"Naaza Zimba","userId":"04491752559292461421"}},"outputId":"fa51671e-fac4-403e-b303-1bb77aa20ede"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["276"]},"metadata":{},"execution_count":46}]},{"cell_type":"code","source":["generated"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ggv5THlROzAr","executionInfo":{"status":"ok","timestamp":1750773608577,"user_tz":-60,"elapsed":21,"user":{"displayName":"Naaza Zimba","userId":"04491752559292461421"}},"outputId":"35ca54ba-1113-4907-f935-b9df93fe176e"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["[277, 100, 55, 147, 85, 58, 247, 70, 70, 70, 70, 70, 70, 70, 70, 70, 276]"]},"metadata":{},"execution_count":51}]},{"cell_type":"markdown","source":["# Transformer Building All Inclusive"],"metadata":{"id":"wWMqVnxdhFmb"}},{"cell_type":"code","source":["class PositionalEncoding(nn.Module):\n","  def __init__(self,d_model, max_length):\n","\n","    super().__init__()\n","\n","    pe = torch.zeros(max_length, d_model) #Here we start by creating an empty matrix which will get updated.\n","\n","  #d_model is the number of word embedding dimensions the larger this is the richer our description of a word\n","  #The actual word embedding values are updated via back propagation\n","\n","    position = torch.arange(start = 0, end = max_length, step = 1).float().unsqueeze(1)\n","\n","    embedding_index = torch.arange(start=0, end=d_model,step=2).float()\n","\n","    div_term = 1/torch.tensor(10000.0)**(embedding_index/ d_model)\n","\n","\n","    #Our Positional encoding starts with a sin and cos equation.\n","  #Have a look at the literture to check why we do this but it's not too difficult to understand\n","\n","    pe[:, 0::2] = torch.sin(position * div_term) #This updates the first column\n","    pe[:, 1::2] = torch.cos(position * div_term) # This updates the second column\n","\n","  #This happens in an alternating method where the first column and the secondary column gets updated\n","\n","    self.register_buffer('pe', pe.unsqueeze(0))\n","\n","  def forward (self, word_embeddings):\n","\n","    return word_embeddings + self.pe[:word_embeddings.size(0), :] #Finally we add the positonal encoding values to the word embedding values\n","\n","class Attention(nn.Module):\n","  def __init__(self, d_model):\n","\n","#Here we pass in the number of word embedding values per token This is for the sake of matrix nultiplication\n","#When we do our matrix multiplication to create the queries, keys and values.\n","\n","    super().__init__()\n","\n","    self.W_q = nn.Linear(in_features = d_model, out_features = d_model, bias = False)\n","    self.W_k = nn.Linear(in_features = d_model, out_features = d_model, bias = False)\n","    self.W_v = nn.Linear(in_features = d_model, out_features = d_model, bias = False)\n","\n","    #Above we have our weight matrix which are used to calculate our Query, Keys and Values\n","    #Finally, we don't include a bias term here when calculating attention which is why we set the bias to False\n","\n","    self.row_dim = 0\n","    self.col_dim = 1 #To give us flexibility to input data sequentiallly into batches we create variables to do so\n","\n","  def forward(self, encodings_for_q, encodings_for_k, encodings_for_v, mask = None):\n","\n","      #The forward method is where we use the masked self attention values for each token to be calculated\n","      #Next we give this forward method the flexbility\n","      #Finally since we want to do masked self attention we can pass in a mask too.\n","\n","    q = self.W_q(encodings_for_q)\n","    k = self.W_k(encodings_for_k)\n","    v = self.W_v(encodings_for_v)\n","\n","    sims = torch.matmul(q, k.transpose(dim0 = self.row_dim, dim1 = self.col_dim))\n","\n","    scaled_sims = sims / torch.tensor(k.size(self.col_dim)**0.5)\n","\n","    if mask is not None:\n","      scaled_sims = scaled_sims.masked_fill(mask = mask, value=-1e9) #Here we include our mask values which stops the attention from looking ahead to caluclate weights.\n","\n","    attention_percents = F.softmax(scaled_sims, dim = self.col_dim)\n","\n","    attention_scores = torch.matmul(attention_percents, v)\n","\n","    return attention_scores\n","\n","import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","\n","from torch.optim import Adam\n","from torch.utils.data import TensorDataset, DataLoader\n","\n","#from pytorch_lightning import\n","from pytorch_lightning import Trainer, LightningModule\n","\n","\n","class DecodeOTrans(LightningModule):\n","  def __init__(self, d_model, max_length, num_tokens = 100):\n","\n","    #Here we specify the number of tokens available in the vocabulary,\n","    #Number of word embeddings per token\n","    #Max token length\n","    super().__init__()\n","\n","    self.we = nn.Embedding(num_embeddings = num_tokens, embedding_dim = d_model) #We creating a word embedding value\n","    #Embeddings needs to know how many tokens are in the vocabulary and understand the dimension size to represent the embedding\n","\n","    self.pe = PositionalEncoding(d_model = d_model, max_length = max_length)\n","\n","    #Then we create a positional encoding object using the class we created earlier\n","\n","    self.self_attention = Attention(d_model = d_model)\n","\n","    self.fc_layer = nn.Linear(in_features = d_model, out_features = num_tokens) #This is our fully connected layer also know as our Dense Layer of neurons (RNN neural networks)\n","\n","    self.loss = nn.CrossEntropyLoss()\n","\n","  def forward(self, token_ids):\n","\n","    word_embeddings = self.we(token_ids)  # [batch_size, seq_len, d_model]\n","    positional_encoding = self.pe(word_embeddings)  # add positional encoding\n","\n","    seq_len = token_ids.size(1)\n","    mask = torch.tril(torch.ones((seq_len, seq_len), device=token_ids.device)) == 0\n","\n","    self_attention_values = self.self_attention(\n","        positional_encoding,\n","        positional_encoding,\n","        positional_encoding,\n","        mask=mask\n","    )\n","\n","    residual_connection_value = positional_encoding + self_attention_values\n","    fc_layer_output = self.fc_layer(residual_connection_value)\n","\n","    return fc_layer_output\n","\n","\n","  def configure_optimizers(self):\n","    return Adam(self.parameters(), lr = 0.1) #Here we are using an Adam Optimiser to train all our parmeters with a learning rate of 0.1 (This is a fast learning rate)\n","\n","  def training_step(self, batch, batch_idx):\n","    input_tokens, labels = batch  # input: [batch_size, seq_len], labels: [batch_size, seq_len]\n","\n","    logits = self.forward(input_tokens)  # output: [batch_size, seq_len, vocab_size]\n","\n","    # Flatten for CrossEntropyLoss: expects [N, C] and [N]\n","    loss = self.loss(\n","        logits.view(-1, logits.size(-1)),  # [batch_size * seq_len, vocab_size]\n","        labels.view(-1)                    # [batch_size * seq_len]\n","    )\n","\n","    #self.log(\"train_loss\", loss)\n","    return loss\n","\n"],"metadata":{"id":"tKnYQhDYgh2O"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Next Stage"],"metadata":{"id":"WwI4dHh5lE7d"}},{"cell_type":"code","source":["!pip install musiclang_predict"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"id":"EdfTQv4L9cF8","executionInfo":{"status":"ok","timestamp":1750685191100,"user_tz":-60,"elapsed":142016,"user":{"displayName":"Naaza Zimba","userId":"04491752559292461421"}},"outputId":"1d805af4-3881-4a0a-f73b-94f8d700e290"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting musiclang_predict\n","  Downloading musiclang-predict-1.2.0.tar.gz (146 kB)\n","\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/146.8 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m146.8/146.8 kB\u001b[0m \u001b[31m9.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","Collecting musiclang>=0.25 (from musiclang_predict)\n","  Downloading musiclang-0.26.0-py3-none-any.whl.metadata (5.8 kB)\n","Requirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (from musiclang_predict) (2.6.0+cu124)\n","Requirement already satisfied: transformers in /usr/local/lib/python3.11/dist-packages (from musiclang_predict) (4.52.4)\n","Requirement already satisfied: tokenizers in /usr/local/lib/python3.11/dist-packages (from musiclang_predict) (0.21.1)\n","Collecting torchtoolkit (from musiclang_predict)\n","  Downloading torchtoolkit-0.0.4-py3-none-any.whl.metadata (2.1 kB)\n","Requirement already satisfied: accelerate in /usr/local/lib/python3.11/dist-packages (from musiclang_predict) (1.7.0)\n","Requirement already satisfied: lxml in /usr/local/lib/python3.11/dist-packages (from musiclang>=0.25->musiclang_predict) (5.4.0)\n","Collecting mido==1.2.10 (from musiclang>=0.25->musiclang_predict)\n","  Downloading mido-1.2.10-py2.py3-none-any.whl.metadata (3.4 kB)\n","Collecting music21==8.1.0 (from musiclang>=0.25->musiclang_predict)\n","  Downloading music21-8.1.0-py3-none-any.whl.metadata (4.7 kB)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from musiclang>=0.25->musiclang_predict) (2.0.2)\n","Collecting pandas==1.5.3 (from musiclang>=0.25->musiclang_predict)\n","  Downloading pandas-1.5.3-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (11 kB)\n","Requirement already satisfied: scipy in /usr/local/lib/python3.11/dist-packages (from musiclang>=0.25->musiclang_predict) (1.15.3)\n","Requirement already satisfied: tensorflow in /usr/local/lib/python3.11/dist-packages (from musiclang>=0.25->musiclang_predict) (2.18.0)\n","Requirement already satisfied: toml==0.10.2 in /usr/local/lib/python3.11/dist-packages (from musiclang>=0.25->musiclang_predict) (0.10.2)\n","Collecting tomli==2.0.1 (from musiclang>=0.25->musiclang_predict)\n","  Downloading tomli-2.0.1-py3-none-any.whl.metadata (8.9 kB)\n","Collecting xmlschema==2.1.1 (from musiclang>=0.25->musiclang_predict)\n","  Downloading xmlschema-2.1.1-py3-none-any.whl.metadata (8.1 kB)\n","Collecting partitura==1.3.1 (from musiclang>=0.25->musiclang_predict)\n","  Downloading partitura-1.3.1-py3-none-any.whl.metadata (9.4 kB)\n","Collecting miditok==2.1.8 (from musiclang>=0.25->musiclang_predict)\n","  Downloading miditok-2.1.8-py3-none-any.whl.metadata (8.0 kB)\n","Requirement already satisfied: huggingface-hub>=0.16.4 in /usr/local/lib/python3.11/dist-packages (from miditok==2.1.8->musiclang>=0.25->musiclang_predict) (0.33.0)\n","Collecting miditoolkit (from miditok==2.1.8->musiclang>=0.25->musiclang_predict)\n","  Downloading miditoolkit-1.0.1-py3-none-any.whl.metadata (4.9 kB)\n","Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from miditok==2.1.8->musiclang>=0.25->musiclang_predict) (4.67.1)\n","Requirement already satisfied: chardet in /usr/local/lib/python3.11/dist-packages (from music21==8.1.0->musiclang>=0.25->musiclang_predict) (5.2.0)\n","Requirement already satisfied: joblib in /usr/local/lib/python3.11/dist-packages (from music21==8.1.0->musiclang>=0.25->musiclang_predict) (1.5.1)\n","Requirement already satisfied: jsonpickle in /usr/local/lib/python3.11/dist-packages (from music21==8.1.0->musiclang>=0.25->musiclang_predict) (4.1.1)\n","Requirement already satisfied: matplotlib in /usr/local/lib/python3.11/dist-packages (from music21==8.1.0->musiclang>=0.25->musiclang_predict) (3.10.0)\n","Requirement already satisfied: more-itertools in /usr/local/lib/python3.11/dist-packages (from music21==8.1.0->musiclang>=0.25->musiclang_predict) (10.7.0)\n","Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from music21==8.1.0->musiclang>=0.25->musiclang_predict) (2.32.3)\n","Requirement already satisfied: webcolors>=1.5 in /usr/local/lib/python3.11/dist-packages (from music21==8.1.0->musiclang>=0.25->musiclang_predict) (24.11.1)\n","Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.11/dist-packages (from pandas==1.5.3->musiclang>=0.25->musiclang_predict) (2.9.0.post0)\n","Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas==1.5.3->musiclang>=0.25->musiclang_predict) (2025.2)\n","Collecting lark-parser (from partitura==1.3.1->musiclang>=0.25->musiclang_predict)\n","  Downloading lark_parser-0.12.0-py2.py3-none-any.whl.metadata (1.7 kB)\n","Collecting elementpath<4.0.0,>=3.0.0 (from xmlschema==2.1.1->musiclang>=0.25->musiclang_predict)\n","  Downloading elementpath-3.0.2-py3-none-any.whl.metadata (6.5 kB)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from accelerate->musiclang_predict) (24.2)\n","Requirement already satisfied: psutil in /usr/local/lib/python3.11/dist-packages (from accelerate->musiclang_predict) (5.9.5)\n","Requirement already satisfied: pyyaml in /usr/local/lib/python3.11/dist-packages (from accelerate->musiclang_predict) (6.0.2)\n","Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from accelerate->musiclang_predict) (0.5.3)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch->musiclang_predict) (3.18.0)\n","Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch->musiclang_predict) (4.14.0)\n","Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch->musiclang_predict) (3.5)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch->musiclang_predict) (3.1.6)\n","Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch->musiclang_predict) (2025.3.2)\n","Collecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch->musiclang_predict)\n","  Downloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n","Collecting nvidia-cuda-runtime-cu12==12.4.127 (from torch->musiclang_predict)\n","  Downloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n","Collecting nvidia-cuda-cupti-cu12==12.4.127 (from torch->musiclang_predict)\n","  Downloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n","Collecting nvidia-cudnn-cu12==9.1.0.70 (from torch->musiclang_predict)\n","  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n","Collecting nvidia-cublas-cu12==12.4.5.8 (from torch->musiclang_predict)\n","  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n","Collecting nvidia-cufft-cu12==11.2.1.3 (from torch->musiclang_predict)\n","  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n","Collecting nvidia-curand-cu12==10.3.5.147 (from torch->musiclang_predict)\n","  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n","Collecting nvidia-cusolver-cu12==11.6.1.9 (from torch->musiclang_predict)\n","  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n","Collecting nvidia-cusparse-cu12==12.3.1.170 (from torch->musiclang_predict)\n","  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n","Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch->musiclang_predict) (0.6.2)\n","Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch->musiclang_predict) (2.21.5)\n","Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch->musiclang_predict) (12.4.127)\n","Collecting nvidia-nvjitlink-cu12==12.4.127 (from torch->musiclang_predict)\n","  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n","Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch->musiclang_predict) (3.2.0)\n","Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch->musiclang_predict) (1.13.1)\n","Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch->musiclang_predict) (1.3.0)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers->musiclang_predict) (2024.11.6)\n","Requirement already satisfied: hf-xet<2.0.0,>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.16.4->miditok==2.1.8->musiclang>=0.25->musiclang_predict) (1.1.3)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch->musiclang_predict) (3.0.2)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->music21==8.1.0->musiclang>=0.25->musiclang_predict) (3.4.2)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->music21==8.1.0->musiclang>=0.25->musiclang_predict) (3.10)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->music21==8.1.0->musiclang>=0.25->musiclang_predict) (2.4.0)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->music21==8.1.0->musiclang>=0.25->musiclang_predict) (2025.6.15)\n","Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow->musiclang>=0.25->musiclang_predict) (1.4.0)\n","Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow->musiclang>=0.25->musiclang_predict) (1.6.3)\n","Requirement already satisfied: flatbuffers>=24.3.25 in /usr/local/lib/python3.11/dist-packages (from tensorflow->musiclang>=0.25->musiclang_predict) (25.2.10)\n","Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow->musiclang>=0.25->musiclang_predict) (0.6.0)\n","Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow->musiclang>=0.25->musiclang_predict) (0.2.0)\n","Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow->musiclang>=0.25->musiclang_predict) (18.1.1)\n","Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.11/dist-packages (from tensorflow->musiclang>=0.25->musiclang_predict) (3.4.0)\n","Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.3 in /usr/local/lib/python3.11/dist-packages (from tensorflow->musiclang>=0.25->musiclang_predict) (5.29.5)\n","Requirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from tensorflow->musiclang>=0.25->musiclang_predict) (75.2.0)\n","Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow->musiclang>=0.25->musiclang_predict) (1.17.0)\n","Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow->musiclang>=0.25->musiclang_predict) (3.1.0)\n","Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow->musiclang>=0.25->musiclang_predict) (1.17.2)\n","Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.11/dist-packages (from tensorflow->musiclang>=0.25->musiclang_predict) (1.73.0)\n","Requirement already satisfied: tensorboard<2.19,>=2.18 in /usr/local/lib/python3.11/dist-packages (from tensorflow->musiclang>=0.25->musiclang_predict) (2.18.0)\n","Requirement already satisfied: keras>=3.5.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow->musiclang>=0.25->musiclang_predict) (3.8.0)\n","Requirement already satisfied: h5py>=3.11.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow->musiclang>=0.25->musiclang_predict) (3.14.0)\n","Requirement already satisfied: ml-dtypes<0.5.0,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow->musiclang>=0.25->musiclang_predict) (0.4.1)\n","Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow->musiclang>=0.25->musiclang_predict) (0.37.1)\n","Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from astunparse>=1.6.0->tensorflow->musiclang>=0.25->musiclang_predict) (0.45.1)\n","Requirement already satisfied: rich in /usr/local/lib/python3.11/dist-packages (from keras>=3.5.0->tensorflow->musiclang>=0.25->musiclang_predict) (13.9.4)\n","Requirement already satisfied: namex in /usr/local/lib/python3.11/dist-packages (from keras>=3.5.0->tensorflow->musiclang>=0.25->musiclang_predict) (0.1.0)\n","Requirement already satisfied: optree in /usr/local/lib/python3.11/dist-packages (from keras>=3.5.0->tensorflow->musiclang>=0.25->musiclang_predict) (0.16.0)\n","Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.19,>=2.18->tensorflow->musiclang>=0.25->musiclang_predict) (3.8)\n","Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.19,>=2.18->tensorflow->musiclang>=0.25->musiclang_predict) (0.7.2)\n","Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.19,>=2.18->tensorflow->musiclang>=0.25->musiclang_predict) (3.1.3)\n","Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->music21==8.1.0->musiclang>=0.25->musiclang_predict) (1.3.2)\n","Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib->music21==8.1.0->musiclang>=0.25->musiclang_predict) (0.12.1)\n","Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib->music21==8.1.0->musiclang>=0.25->musiclang_predict) (4.58.4)\n","Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->music21==8.1.0->musiclang>=0.25->musiclang_predict) (1.4.8)\n","Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.11/dist-packages (from matplotlib->music21==8.1.0->musiclang>=0.25->musiclang_predict) (11.2.1)\n","Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->music21==8.1.0->musiclang>=0.25->musiclang_predict) (3.2.3)\n","Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich->keras>=3.5.0->tensorflow->musiclang>=0.25->musiclang_predict) (3.0.0)\n","Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich->keras>=3.5.0->tensorflow->musiclang>=0.25->musiclang_predict) (2.19.1)\n","Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich->keras>=3.5.0->tensorflow->musiclang>=0.25->musiclang_predict) (0.1.2)\n","Downloading musiclang-0.26.0-py3-none-any.whl (231 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m231.5/231.5 kB\u001b[0m \u001b[31m17.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading miditok-2.1.8-py3-none-any.whl (109 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m109.5/109.5 kB\u001b[0m \u001b[31m9.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading mido-1.2.10-py2.py3-none-any.whl (51 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m51.1/51.1 kB\u001b[0m \u001b[31m4.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading music21-8.1.0-py3-none-any.whl (22.8 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m22.8/22.8 MB\u001b[0m \u001b[31m33.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading pandas-1.5.3-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (12.0 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.0/12.0 MB\u001b[0m \u001b[31m63.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading partitura-1.3.1-py3-none-any.whl (318 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m318.4/318.4 kB\u001b[0m \u001b[31m22.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading tomli-2.0.1-py3-none-any.whl (12 kB)\n","Downloading xmlschema-2.1.1-py3-none-any.whl (354 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m354.3/354.3 kB\u001b[0m \u001b[31m22.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m4.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (13.8 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m75.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (24.6 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m74.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (883 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m44.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m5.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m11.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m8.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m5.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m68.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading torchtoolkit-0.0.4-py3-none-any.whl (13 kB)\n","Downloading elementpath-3.0.2-py3-none-any.whl (189 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m189.1/189.1 kB\u001b[0m \u001b[31m13.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading lark_parser-0.12.0-py2.py3-none-any.whl (103 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m103.5/103.5 kB\u001b[0m \u001b[31m7.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading miditoolkit-1.0.1-py3-none-any.whl (24 kB)\n","Building wheels for collected packages: musiclang_predict\n","  Building wheel for musiclang_predict (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for musiclang_predict: filename=musiclang_predict-1.2.0-py3-none-any.whl size=144077 sha256=6fafe082614a62a6fe08ef9d5dfcd117f40b4da11a5c7fc30d414120db5fa047\n","  Stored in directory: /root/.cache/pip/wheels/ed/13/69/e795afef76fc21778d38758f75d80823de9498b18d0c814352\n","Successfully built musiclang_predict\n","Installing collected packages: mido, lark-parser, tomli, nvidia-nvjitlink-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, elementpath, xmlschema, pandas, nvidia-cusparse-cu12, nvidia-cudnn-cu12, partitura, nvidia-cusolver-cu12, music21, miditoolkit, miditok, torchtoolkit, musiclang, musiclang_predict\n","  Attempting uninstall: mido\n","    Found existing installation: mido 1.3.3\n","    Uninstalling mido-1.3.3:\n","      Successfully uninstalled mido-1.3.3\n","  Attempting uninstall: nvidia-nvjitlink-cu12\n","    Found existing installation: nvidia-nvjitlink-cu12 12.5.82\n","    Uninstalling nvidia-nvjitlink-cu12-12.5.82:\n","      Successfully uninstalled nvidia-nvjitlink-cu12-12.5.82\n","  Attempting uninstall: nvidia-curand-cu12\n","    Found existing installation: nvidia-curand-cu12 10.3.6.82\n","    Uninstalling nvidia-curand-cu12-10.3.6.82:\n","      Successfully uninstalled nvidia-curand-cu12-10.3.6.82\n","  Attempting uninstall: nvidia-cufft-cu12\n","    Found existing installation: nvidia-cufft-cu12 11.2.3.61\n","    Uninstalling nvidia-cufft-cu12-11.2.3.61:\n","      Successfully uninstalled nvidia-cufft-cu12-11.2.3.61\n","  Attempting uninstall: nvidia-cuda-runtime-cu12\n","    Found existing installation: nvidia-cuda-runtime-cu12 12.5.82\n","    Uninstalling nvidia-cuda-runtime-cu12-12.5.82:\n","      Successfully uninstalled nvidia-cuda-runtime-cu12-12.5.82\n","  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n","    Found existing installation: nvidia-cuda-nvrtc-cu12 12.5.82\n","    Uninstalling nvidia-cuda-nvrtc-cu12-12.5.82:\n","      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.5.82\n","  Attempting uninstall: nvidia-cuda-cupti-cu12\n","    Found existing installation: nvidia-cuda-cupti-cu12 12.5.82\n","    Uninstalling nvidia-cuda-cupti-cu12-12.5.82:\n","      Successfully uninstalled nvidia-cuda-cupti-cu12-12.5.82\n","  Attempting uninstall: nvidia-cublas-cu12\n","    Found existing installation: nvidia-cublas-cu12 12.5.3.2\n","    Uninstalling nvidia-cublas-cu12-12.5.3.2:\n","      Successfully uninstalled nvidia-cublas-cu12-12.5.3.2\n","  Attempting uninstall: pandas\n","    Found existing installation: pandas 2.2.2\n","    Uninstalling pandas-2.2.2:\n","      Successfully uninstalled pandas-2.2.2\n","  Attempting uninstall: nvidia-cusparse-cu12\n","    Found existing installation: nvidia-cusparse-cu12 12.5.1.3\n","    Uninstalling nvidia-cusparse-cu12-12.5.1.3:\n","      Successfully uninstalled nvidia-cusparse-cu12-12.5.1.3\n","  Attempting uninstall: nvidia-cudnn-cu12\n","    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\n","    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\n","      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\n","  Attempting uninstall: nvidia-cusolver-cu12\n","    Found existing installation: nvidia-cusolver-cu12 11.6.3.83\n","    Uninstalling nvidia-cusolver-cu12-11.6.3.83:\n","      Successfully uninstalled nvidia-cusolver-cu12-11.6.3.83\n","  Attempting uninstall: music21\n","    Found existing installation: music21 9.3.0\n","    Uninstalling music21-9.3.0:\n","      Successfully uninstalled music21-9.3.0\n","\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n","google-colab 1.0.0 requires pandas==2.2.2, but you have pandas 1.5.3 which is incompatible.\n","cudf-cu12 25.2.1 requires pandas<2.2.4dev0,>=2.0, but you have pandas 1.5.3 which is incompatible.\n","dask-cudf-cu12 25.2.2 requires pandas<2.2.4dev0,>=2.0, but you have pandas 1.5.3 which is incompatible.\n","plotnine 0.14.5 requires pandas>=2.2.0, but you have pandas 1.5.3 which is incompatible.\n","dask-expr 1.1.21 requires pandas>=2, but you have pandas 1.5.3 which is incompatible.\n","xarray 2025.3.1 requires pandas>=2.1, but you have pandas 1.5.3 which is incompatible.\n","mizani 0.13.5 requires pandas>=2.2.0, but you have pandas 1.5.3 which is incompatible.\u001b[0m\u001b[31m\n","\u001b[0mSuccessfully installed elementpath-3.0.2 lark-parser-0.12.0 miditok-2.1.8 miditoolkit-1.0.1 mido-1.2.10 music21-8.1.0 musiclang-0.26.0 musiclang_predict-1.2.0 nvidia-cublas-cu12-12.4.5.8 nvidia-cuda-cupti-cu12-12.4.127 nvidia-cuda-nvrtc-cu12-12.4.127 nvidia-cuda-runtime-cu12-12.4.127 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-nvjitlink-cu12-12.4.127 pandas-1.5.3 partitura-1.3.1 tomli-2.0.1 torchtoolkit-0.0.4 xmlschema-2.1.1\n"]},{"output_type":"display_data","data":{"application/vnd.colab-display-data+json":{"pip_warning":{"packages":["mido","pandas"]},"id":"34a36fa5179a4e01aa44d96c0588135e"}},"metadata":{}}]},{"cell_type":"code","source":["from musiclang_predict import MusicLangTokenizer\n","from musiclang import Score\n","# Load model and tokenizer, we use the v1 of the musiclang model for this purpose\n","midi_file = 'path_to_your_midi_file.mid'\n","score = Score.from_midi(midi_file)\n","tokenizer = MusicLangTokenizer('musiclang/musiclang-4k')\n","tokens = tokenizer.tokenize(score)\n","print(tokens)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":482},"id":"RLSL3yQK3FHe","executionInfo":{"status":"error","timestamp":1750685033523,"user_tz":-60,"elapsed":29,"user":{"displayName":"Naaza Zimba","userId":"04491752559292461421"}},"outputId":"241f0168-f9bd-4700-db44-ccb6bdaf738e"},"execution_count":null,"outputs":[{"output_type":"error","ename":"ModuleNotFoundError","evalue":"No module named 'musiclang_predict'","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)","\u001b[0;32m/tmp/ipython-input-20-3306715955.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mmusiclang_predict\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mMusicLangTokenizer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mmusiclang\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mScore\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m# Load model and tokenizer, we use the v1 of the musiclang model for this purpose\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mmidi_file\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'path_to_your_midi_file.mid'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mscore\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mScore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_midi\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmidi_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'musiclang_predict'","","\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"],"errorDetails":{"actions":[{"action":"open_url","actionText":"Open Examples","url":"/notebooks/snippets/importing_libraries.ipynb"}]}}]}]}